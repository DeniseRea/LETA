Scopus
EXPORT DATE: 16 December 2025

@ARTICLE{Ali2025,
	author = {Ali, Hashim and Tanveer, Umer and Saeed, Amir and Khalid Alkahtani, Hend Khalid and Alzahrani, Khalid J. and Akbayan, Bekarystankzy},
	title = {Cloud-based machine learning for scalable classification of software requirements: Insights from the PROMISE dataset},
	year = {2025},
	journal = {Systems and Soft Computing},
	volume = {7},
	pages = {},
	doi = {10.1016/j.sasc.2025.200405},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019075113&doi=10.1016%2Fj.sasc.2025.200405&partnerID=40&md5=318eec4050643f8dbea4f6f50b0abed1},
	abstract = {Software requirement classification (SRC) is a critical yet challenging task in large-scale software development, where manual classification is time-consuming, error-prone, and unscalable, consuming significant project effort as reported by industry surveys. The urgent need for automated, scalable solutions motivates this research, which proposes a novel integration of advanced machine learning (ML) techniques and a cloud-based architecture to enhance SRC using the PROMISE dataset. Our approach leverages a hybrid cloud–edge deployment strategy, combining the precision of ML models, such as BERT, with dynamic resource allocation to achieve an F1-score of 89.2%, outperforming traditional methods. Key contributions include: (1) a comprehensive evaluation of five ML models for SRC, (2) a novel hybrid cloud–edge architecture balancing performance, latency, and privacy, and (3) a cost–benefit analysis demonstrating cost-effectiveness for enterprise applications. These advancements address scalability and accuracy challenges in requirement engineering, enabling more efficient, consistent, and automated SRC processes, with significant potential for widespread industry adoption. © 2025 The Authors},
	author_keywords = {Cloud computing; Machine learning; Natural language processing; PROMISE dataset; Requirement engineering; Scalable architecture; Software requirement classification},
	keywords = {Architecture; Automation; Balancing; Classification (of information); Cloud computing architecture; Computer architecture; Cost effectiveness; Enterprise software; Learning algorithms; Learning systems; Machine learning; Software design; Cloud-computing; Language processing; Machine-learning; Natural language processing; Natural languages; PROMISE dataset; Requirement engineering; Requirements classifications; Scalable architectures; Software requirement classification; Software requirements; Requirements engineering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Robredo2025,
	author = {Robredo, Mikel and Saarimäki, Nyyti and Esposito, Matteo and Taibi, Davide and Peñaloza, Rafael and Lenarduzzi, Valentina},
	title = {Evaluating time-dependent methods and seasonal effects in code technical debt prediction},
	year = {2025},
	journal = {Journal of Systems and Software},
	volume = {230},
	pages = {},
	doi = {10.1016/j.jss.2025.112545},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012260533&doi=10.1016%2Fj.jss.2025.112545&partnerID=40&md5=8d806219321cb1750d1c8a016f9e7afb},
	abstract = {Background: Code Technical Debt (Code TD) prediction has gained significant attention in recent software engineering research. However, no standardized approach to Code TD prediction fully captures the factors influencing its evolution. Objective: Our study aims to assess the impact of time-dependent models and seasonal effects on Code TD prediction. It evaluates such models against widely used Machine Learning models also considering the influence of seasonality on prediction performance. Methods: We trained 11 prediction models with 31 Java open-source projects. To assess their performance, we predicted future observations of the SQALE index. To evaluate the practical usability of our TD forecasting model and their impact on practitioners, we surveyed 23 software engineering professionals. Results: Our study confirms the benefits of time-dependent techniques, with the ARIMAX model outperforming the others. Seasonal effects improved predictive performance, though the impact remained modest. ARIMAX/SARIMAX models demonstrated to provide well-balanced long-term forecasts. The survey highlighted strong industry interest in short- to medium-term TD forecasts. Conclusions: Our findings support using techniques that capture time dependence in historical software metric data, particularly for Code TD. Effectively addressing this evidence requires adopting methods that account for temporal patterns. © 2025 The Authors},
	author_keywords = {Empirical software engineering; Software quality mining software repositories; Technical debt; Time series analysis},
	keywords = {Artificial intelligence; Autoregressive moving average model; Codes (symbols); Computer software selection and evaluation; Forecasting; Learning systems; Open source software; Open systems; Prediction models; Quality control; Software quality; Empirical Software Engineering; Mining software; Seasonal effects; Software engineering research; Software Quality; Software quality mining software repository; Software repositories; Technical debts; Time dependent; Time-series analysis; Time series analysis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Aghilar2025,
	author = {Aghilar, Potito and Anelli, Vito Walter and Trizio, Michelantonio and Di Sciascio, Eugenio and Di Noia, Tommaso},
	title = {Training-free, Identity-preserving Image Editing for Fashion Pose Alignment and Normalization},
	year = {2025},
	journal = {Expert Systems with Applications},
	volume = {293},
	pages = {},
	doi = {10.1016/j.eswa.2025.128579},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008992813&doi=10.1016%2Fj.eswa.2025.128579&partnerID=40&md5=5d27cb5733ad560e6512fc95c15a980f},
	abstract = {Diffusion models have recently unlocked new possibilities in editing images of real-world objects. Yet, transforming objects in non-rigid ways, such as modifying poses or applying image-based conditioning, continues to present significant challenges. Retaining the unique identity of objects during these edits is a complex task, and current techniques often fall short of delivering the precision needed for industrial settings, where consistency is non-negotiable. Additionally, adapting diffusion models demands custom training data, which is often unavailable in real-world scenarios. To address these gaps, we present FASHIONREPOSE, a novel, training-free pipeline designed to handle non-rigid pose adjustments specifically for the fashion industry. This approach combines pretrained off-the-shelf models to modify the poses of long-sleeve garments while safeguarding their identity and branding characteristics. By adopting a zero-shot methodology, FASHIONREPOSE enables near real-time edits, entirely eliminating the requirement for specialized training data. FASHIONREPOSE has been deployed for a global fashion firm, OVS, handling more than 30,000 long-sleeve garments. © 2025 The Author(s)},
	author_keywords = {Computer vision; Diffusion models; Large language models; Software engineering},
	keywords = {Diffusion; Human computer interaction; Software engineering; Diffusion model; Image editing; Image-based; Language model; Large language model; Non-rigid; Pose alignments; Pose normalization; Real-world objects; Training data; Computer vision},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{De Martino2025,
	author = {De Martino, Vincenzo and Voria, Gianmario and Troiano, Ciro and Catolino, Gemma and Palomba, Fabio},
	title = {Examining the impact of bias mitigation algorithms on the sustainability of ML-enabled systems: A benchmark study},
	year = {2025},
	journal = {Journal of Systems and Software},
	volume = {230},
	pages = {},
	doi = {10.1016/j.jss.2025.112458},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006881113&doi=10.1016%2Fj.jss.2025.112458&partnerID=40&md5=283c4958cb15ebafc24a70572b14f23c},
	abstract = {Context: As machine learning (ML) systems become increasingly prevalent across various industries, concerns regarding fairness have intensified. Bias mitigation algorithms—that aim to reduce bias in ML models—serve as solutions to mitigate this issue. However, these techniques can affect more than just social sustainability. They may alter the computational overhead and energy usage of ML systems, affecting their environmental sustainability. Similarly, they can influence businesses’ economic sustainability by shaping resource allocation and consumer trust. Goal: This work aims to provide a benchmark study of the implications of applying bias mitigation algorithms on the sustainability of ML solutions. We first corroborate previous findings by examining their effect on social sustainability metrics. Additionally, we complement existing studies by offering a comprehensive analysis of how bias mitigation affects environmental and economic sustainability, aiming to highlight trade-offs for practitioners designing ML solutions. Method: We evaluate six bias mitigation algorithms by conducting 3,360 experiments across multiple configurations of four ML algorithms and datasets. From these experiments, we compute metrics for social, environmental, and economic sustainability, evaluating them using statistical analysis. Results: Our quantitative findings show that all bias mitigation algorithms affect the three sustainability dimensions differently, indicating that applying these algorithms involves complex trade-offs. Furthermore, we expand our discussion with qualitative insights that arise from our results, also providing implications for both research and practice. Conclusions: Our study emphasizes the need for a deeper investigation into the trade-offs bias mitigation algorithms introduce and how they impact various non-functional requirements of ML systems. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2025 The Authors},
	author_keywords = {Machine learning-enabled systems; Software engineering for artificial intelligence; Software sustainability},
	keywords = {Computer operating systems; Computer software maintenance; Economic analysis; Environmental management systems; Green development; Software packages; Software quality; Sustainable development; Verification; Benchmark study; Economic sustainability; Environmental sustainability; Machine learning systems; Machine learning-enabled system; Machine-learning; Social sustainability; Software engineering for artificial intelligence; Software sustainability; Trade off; Software design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Stradowski2025,
	author = {Stradowski, Szymon and Madeyski, Lech},
	title = {“Your AI is impressive, but my code does not have any bugs” managing false positives in industrial contexts},
	year = {2025},
	journal = {Science of Computer Programming},
	volume = {246},
	pages = {},
	doi = {10.1016/j.scico.2025.103320},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004412682&doi=10.1016%2Fj.scico.2025.103320&partnerID=40&md5=d892299f6b14e144e34c84903f65152c},
	abstract = {Context: “Your AI is impressive, but my code does not contain any bugs”— such a statement from a software developer is the antithesis of a quality mindset and open communication. What makes it worse is that it is oftentimes true. Objective: This paper analyses false positives' impact and related challenges in machine learning software defect prediction and describes the mitigation possibilities. Methods: We propose a broad-picture perspective on dealing with false positive predictions based on what we learned from our industrial implementation study in Nokia 5G. Results: Accordingly, we draw a new direction in transitioning defect prediction into a well-established industry practice, as well as highlight potential emerging topics in predictive software engineering. Conclusion: Increasing human buy-in and the business impact of predictions significantly improves the chances of future software defect prediction industry adoptions to succeed. © 2025},
	author_keywords = {Industry; Machine learning; Real-world; Software defect prediction},
	keywords = {False positive; Industrial context; Machine learning software; Machine-learning; Open communication; Paper analysis; Prediction-based; Real-world; Software defect prediction; Software developer},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access}
}

@ARTICLE{Kalinowski2025,
	author = {Kalinowski, M. and Mendez, Daniel and Giray, Görkem and Santos Alves, Antonio Pedro and Azevedo, Kelly and Escovedo, Tatiana and Villamizar, Hugo and Lopes, Hélio Côrtes Vieira and Baldassarre, Maria Teresa and Wagner, Stefan and Biffl, Stefan and Musil, Juergen and Felderer, Michael and Lavesson, Niklas and Gorschek, Tony},
	title = {Naming the Pain in machine learning-enabled systems engineering},
	year = {2025},
	journal = {Information and Software Technology},
	volume = {187},
	pages = {},
	doi = {10.1016/j.infsof.2025.107866},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012821054&doi=10.1016%2Fj.infsof.2025.107866&partnerID=40&md5=6867725f616b2c091d7965eb831feb90},
	abstract = {Context: Machine learning (ML)-enabled systems are being increasingly adopted by companies aiming to enhance their products and operational processes. Objective: This paper aims to deliver a comprehensive overview of the current status quo of engineering ML-enabled systems and lay the foundation to steer practically relevant and problem-driven academic research. Method: We conducted an international survey to collect insights from practitioners on the current practices and problems in engineering ML-enabled systems. We received 188 complete responses from 25 countries. We conducted quantitative statistical analyses on contemporary practices using bootstrapping with confidence intervals and qualitative analyses on the reported problems using open and axial coding procedures. Results: Our survey results reinforce and extend existing empirical evidence on engineering ML-enabled systems, providing additional insights into typical ML-enabled systems project contexts, the perceived relevance and complexity of ML life cycle phases, and current practices related to problem understanding, model deployment, and model monitoring. Furthermore, the qualitative analysis provides a detailed map of the problems practitioners face within each ML life cycle phase and the problems causing overall project failure. Conclusions: The results contribute to a better understanding of the status quo and problems in practical environments. We advocate for the further adaptation and dissemination of software engineering practices to enhance the engineering of ML-enabled systems. © 2025},
	author_keywords = {Machine learning-enabled system; Survey; Systems engineering},
	keywords = {Computer programming; Learning systems; Machine learning; Software engineering; Academic research; Current practices; Current status; Engineering machines; Machine learning-enabled system; Machine-learning; Operational process; Product process; Qualitative analysis; Status quo; Life cycle},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Alami2025,
	author = {Alami, Adam and Jensen, Victor Vadmand and Ernst, Neil A.},
	title = {Accountability in Code Review: The Role of Intrinsic Drivers and the Impact of LLMs},
	year = {2025},
	journal = {ACM Transactions on Software Engineering and Methodology},
	volume = {34},
	number = {8},
	pages = {},
	doi = {10.1145/3721127},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019645084&doi=10.1145%2F3721127&partnerID=40&md5=bf23168dc532faa1b03b5931289350f2},
	abstract = {Accountability is an innate part of social systems. It maintains stability and ensures positive pressure on individuals’ decision-making. As actors in a social system, software developers are accountable to their team and organization for their decisions. However, the drivers of accountability and how it changes behavior in software development are less understood. In this study, we look at how the social aspects of code review affect software engineers’ sense of accountability for code quality. Since Software Engineering (SE) is increasingly involving Large Language Models (LLM) assistance, we also evaluate the impact on accountability when introducing LLM-assisted code reviews. We carried out a two-phased sequential qualitative study (interviews → focus groups). In Phase I (16 interviews), we sought to investigate the intrinsic drivers of software engineers influencing their sense of accountability for code quality, relying on self-reported claims. In Phase II, we tested these traits in a more natural setting by simulating traditional peer-led reviews with focus groups and then LLM-assisted review sessions. We found that there are four key intrinsic drivers of accountability for code quality: personal standards, professional integrity, pride in code quality, and maintaining one’s reputation. In a traditional peer-led review, we observed a transition from individual to collective accountability when code reviews are initiated. We also found that the introduction of LLM-assisted reviews disrupts this accountability process, challenging the reciprocity of accountability taking place in peer-led evaluations, i.e., one cannot be accountable to an LLM. Our findings imply that the introduction of AI into SE must preserve social integrity and collective accountability mechanisms. © 2025 Copyright held by the owner/author(s).},
	author_keywords = {Accountability; Artificial Intelligence; Code quality; Code Review; Human; Large Language Models; LLM; Social Aspects of Software Engineering},
	keywords = {Behavioral research; Codes (symbols); Computer aided software engineering; Computer programming languages; Computer software selection and evaluation; Decision making; Distributed computer systems; Engineers; Human engineering; Professional aspects; Social aspects; Social software; Software design; Software quality; Accountability; Code quality; Code review; Human; Language model; Large language model; Social aspect of software engineering; Social systems; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Bronze Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Jalali2025,
	author = {Jalali, Nasir Ahmad and Hongsong, Chen},
	title = {Securing social network user data in large language model deployments: challenges and best practices},
	year = {2025},
	journal = {Cluster Computing},
	volume = {28},
	number = {11},
	pages = {},
	doi = {10.1007/s10586-025-05482-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016088492&doi=10.1007%2Fs10586-025-05482-y&partnerID=40&md5=1e972078850fef17cdf503dfd84029b7},
	abstract = {AI technology’s rapid advancements have led to its widespread adoption in diverse industries, such as support agents, healthcare, education, software development and finance because such AI technologies has the ability to generated human-like text to solve a complex user’s problem. Nevertheless, the effectiveness and success of AI models greatly depends on large language model that contain extensive, varied, and top-notch datasets and social network platforms for both training and evaluation purposes that is sourced from web and directly form user interaction that will also contain sensitive information. However, this power capabilities and progress have raised significant concerns related data security and privacy, particularly regarding protection of user data including social network during training and deployment phase. Large language models process and generate huge amount of data which have the potential to unintentionally memorise and repeat personal sensitive information like location data, social security number, and phone numbers which are shared by users that can lead to pose a risk of data leaking and degrade the accuracy of model training and information. This paper will perform a comprehensive and systematic review to examine the fundamentals principle of large language model with particularly concentration on data privacy concern associated with large language model during training data and potential biases. Furthermore, we analysis and evaluated the current level of vulnerabilities, privacy challenges, and examine new security and privacy attacks targeting LLMs that cause of misuse and information leakage. Additionally, we proposed a set of best practices aimed to protect user data. Recognising the significance of conducting such research on protecting privacy and improving security is crucial because it ensures the continuous progress and public trust in LLM technology. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.},
	author_keywords = {Defence mechanism; Large language model; LLM attacks; Privacy; Security},
	keywords = {Artificial intelligence; Information leakage; Information systems; Network security; Personnel training; Sensitive data; Social networking (online); AI Technologies; Best practices; Defence mechanisms; Language model; Large language model; LLM attack; Privacy; Security; Sensitive informations; User data; Large datasets},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Kemell2025,
	author = {Kemell, Kai Kristian and Saarikallio, Matti and Nguyen-Duc, Anh and Abrahamsson, Pekka},
	title = {Still just personal assistants? – A multiple case study of generative AI adoption in software organizations},
	year = {2025},
	journal = {Information and Software Technology},
	volume = {186},
	pages = {},
	doi = {10.1016/j.infsof.2025.107805},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008513590&doi=10.1016%2Fj.infsof.2025.107805&partnerID=40&md5=e850fbf0f6207c5f0cc26d0727b4914f},
	abstract = {Context: Generative AI (GenAI) is argued to transform software engineering (SE) in various ways, and GenAI tools show promise for various SE tasks. Software organizations across the globe are currently exploring the use of GenAI for SE. Objective: While numerous studies have recently been published on GenAI, few studies have looked at the adoption of these tools and their usage from an organizational point of view, focusing instead on individual users. Our objective is to understand how organizations adopt these tools and what their impacts are in industrial contexts, with a focus on the European perspective. Method: We conducted a multiple case study of seven European companies. We collected data through semi-structured interviews (n=15), as well as through longitudinal observation in one case company. All data were analyzed using thematic analysis. Results: We analyzed 28 transcripts, resulting in 456 quotations and 557 code occurrences split between 66 individual codes that were categorized under 6 high-level themes. We identified 25 types of tasks GenAI was currently being used for in our case organizations. We identified 12 benefits for GenAI in SE and 10 adoption and use challenges. Key adoption challenges for organizations include data privacy and legislative concerns, the emerging and fast-moving market of GenAI tools, difficulty of measuring the positive impact of the tools, and potential change resistance. For individuals, the key challenges are related to prompting, such as understanding what a good prompt is, and how to write prompts for specific tasks. Conclusion: GenAI adoption is becoming widespread in SE, but good practices and use cases are still emerging. While GenAI can potentially produce various benefits in SE, companies and individual users are facing various challenges in making the most of GenAI in SE. Overall, GenAI is still primarily used as a personal assistant. © 2025 The Authors},
	author_keywords = {Artificial intelligence; Case study; Empirical research; Generative AI; Large language model; Software engineering; Technology adoption},
	keywords = {Data privacy; Case-studies; Empirical research; Engineering tasks; Generative AI; Language model; Large language model; Multiple-case study; Personal assistants; Software organization; Technology adoption; Artificial intelligence; Software engineering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Kelmendi2025,
	author = {Kelmendi, Ardian and Pappas, George},
	title = {AI-Driven Optimization of Functional Feature Placement in Automotive CAD},
	year = {2025},
	journal = {Algorithms},
	volume = {18},
	number = {9},
	pages = {},
	doi = {10.3390/a18090553},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017374797&doi=10.3390%2Fa18090553&partnerID=40&md5=6737559786bbe8ab7be9a8120b76a36a},
	abstract = {The automotive industry increasingly relies on 3D modeling technologies to design and manufacture vehicle components with high precision. One critical challenge is optimizing the placement of latches that secure the dashboard side paneling, as these placements vary between models and must maintain minimal tolerance for movement to ensure durability. While generative artificial intelligence (AI) has advanced rapidly in generating text, images, and video, its application to creating accurate 3D CAD models remains limited. This paper proposes a novel framework that integrates a PointNet deep learning model with Python-based CAD automation to predict optimal clip placements and surface thickness for dashboard side panels. Unlike prior studies that focus on general-purpose CAD generation, this work specifically targets automotive interior components and demonstrates a practical method for automating part design. The approach involves generating placement data—potentially via generative AI—and importing it into the CAD environment to produce fully parameterized 3D models. Experimental results show that the prototype achieved a 75% success rate across six of eight test surfaces, indicating strong potential despite the limited sample size. This research highlights a clear pathway for applying generative AI to part design automation in the automotive sector and offers a foundation for scaling to broader design applications. © 2025 by the authors.},
	author_keywords = {algorithm; artificial intelligence; computer-aided design; python},
	keywords = {Automotive industry; Computer aided software engineering; Computer software; Deep learning; Machine design; Optimization; 3d modeling technologies; Automotives; Computer-aided design; Critical challenges; Design and manufactures; Functional features; High-precision; Optimisations; Part design; Vehicle components; Computer aided design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Marzouk2025,
	author = {Marzouk, Mohamed M. and Bin Mahmoud, Abdulrahman A. and Al-Gahtani, Khalid S. and Adel, Kareem},
	title = {Automation in Construction (2000–2023): Science Mapping and Visualization of Journal Publications},
	year = {2025},
	journal = {Buildings},
	volume = {15},
	number = {15},
	pages = {},
	doi = {10.3390/buildings15152789},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013227088&doi=10.3390%2Fbuildings15152789&partnerID=40&md5=b877517c6e469d626b994ddcf46d856f},
	abstract = {This paper presents a scientometric review that provides a quantitative perspective on the evolution of Automation in Construction Journal (AICJ) research, emphasizing its developmental paths and emerging trends. The study aims to analyze the journal’s growth and citation impact over time. It also seeks to identify the most influential publications and the cooperation patterns among key contributors. Furthermore, the study explores the journal’s primary research themes and their evolution. Accordingly, 4084 articles were identified using the Web of Science (WoS) database and subjected to a multistep analysis using VOsviewer version 1.6.18 and Biblioshiny as software tools. First, the growth and citation of the publications over time are inspected and evaluated, in addition to ranking the most influential documents. Second, the co-authorship analysis method is applied to visualize the cooperation patterns between countries, organizations, and authors. Finally, the publications are analyzed using keyword co-occurrence and keyword thematic evolution analyses, revealing five major research clusters: (i) foundational optimization, (ii) deep learning and computer vision, (iii) building information modeling, (iv) 3D printing and robotics, and (v) machine learning. Additionally, the analysis reveals significant growth in publications (54.5%) and citations (78.0%) from 2018 to 2023, indicating the journal’s increasing global influence. This period also highlights the accelerated adoption of digitalization (e.g., BIM, computational design), increased integration of AI and machine learning for automation and predictive analytics, and rapid growth of robotics and 3D printing, driving sustainable and innovative construction practices. The paper’s findings can help readers and researchers gain a thorough understanding of the AICJ’s published work, aid research groups in planning and optimizing their research efforts, and inform editorial boards on the most promising areas in the existing body of knowledge for further investigation and development. © 2025 by the authors.},
	author_keywords = {Automation in Construction; bibliometrics; Biblioshiny; knowledge structure; VOSviewer},
	keywords = {Architectural design; Computer aided software engineering; Deep learning; Engineering research; Learning systems; Publishing; Robotics; 3-D printing; 3D-printing; Automation in construction; Bibliometric; Biblioshiny; Journal publication; Knowledge structures; Machine-learning; Scientometrics; Vosviewer; Predictive analytics},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Yang2025,
	author = {Yang, Rui and Fu, Michael and Tantithamthavorn, Kla (kla) and Arora, Chetan and Vandenhurk, Lisa and Chua, Joey},
	title = {RAGVA: Engineering retrieval augmented generation-based virtual assistants in practice},
	year = {2025},
	journal = {Journal of Systems and Software},
	volume = {226},
	pages = {},
	doi = {10.1016/j.jss.2025.112436},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000995469&doi=10.1016%2Fj.jss.2025.112436&partnerID=40&md5=746b302ec1edcbbd9ff29458f959707c},
	abstract = {Retrieval-augmented generation (RAG)-based applications are gaining prominence due to their ability to leverage large language models (LLMs). These systems excel at combining retrieval mechanisms with generative capabilities, resulting in contextually relevant responses that enhance user experience. In particular, Transurban, a road operation company, replaced its rule-based virtual assistant (VA) with a RAG-based VA (RAGVA) to offer flexible customer interactions and support a wider range of scenarios. This paper presents an experience report from Transurban's engineering team on building and deploying a RAGVA, offering a step-by-step guide for creating a conversational application and engineering a RAGVA. The report serves as a reference for future researchers and practitioners. While the engineering processes for traditional software applications are well-established, the development and evaluation of RAG-based applications are still in their early stages, with numerous emerging challenges remaining uncharted. To address this gap, we conduct a focus group study with Transurban practitioners regarding developing and evaluating their RAGVA. We identified eight challenges encountered by the engineering team and proposed eight future directions that should be explored to advance the development of RAG-based applications. This study contributes to the foundational understanding of a RAG-based conversational application and the emerging AI software engineering challenges it presents. © 2025 The Authors},
	author_keywords = {AI engineering; LLMOps; Responsible AI; Retrieval augmented generation; SE4AI; Software engineering; Virtual assistants},
	keywords = {Computer software selection and evaluation; Human engineering; Search engines; Virtual reality; AI engineering; Engineering teams; Excel; Language model; LLMOp; Responsible AI; Retrieval augmented generation; Retrieval mechanisms; SE4AI; Virtual assistants; Application programs},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Hasan2025639,
	author = {Hasan, Raghda Azad and Saleh, Ibrahim Ahmed},
	title = {PREDICTION OF SOFTWARE ANOMALIES METHODS BASED ON ENSEMBLE LEARNING METHODS},
	year = {2025},
	journal = {Kufa Journal of Engineering},
	volume = {16},
	number = {3},
	pages = {639 - 657},
	doi = {10.30572/2018/KJE/160336},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012836930&doi=10.30572%2F2018%2FKJE%2F160336&partnerID=40&md5=81747b482780e2c3534e7d12b8ca9fd6},
	abstract = {Software plays a vital role in all aspects of our daily lives, specifically in the fields of medicine and industry. In order to design high-quality and reliable software and avoid risks resulting from software errors, including physical and human errors, this is considered a major challenge due to the limited time and budget specified. Therefore, most software development companies tend to use machine learning for prediction. With the presence of software defects that contribute to improving the quality and safety of the software produced, this is done by relying on and using records, previous projects, and available data. this paper proposed machine learning and ensemble learning suite to predict software anomalies. The evaluated approach is for models in the PROMISE real-word dataset repository containing 5 projects (Turkish company SOFTLAB). The model applies the basic algorithms (Random Forest (RF), Decision Tree (DT), Extra Tree) and the learning model ensemble (Adaboost, xgboost,Stack, Voting, bagging) and metrics (accuracy, recall, F1 score, accuracy) to measure the prediction performance of the models and a comparison was made between the proposed model algorithms. Both adaboost, stack achieved prediction accuracy about 99.2% when implemented on the ar5 dataset. © 2025, University of Kufa. All rights reserved.},
	author_keywords = {Boosting; Decision Tree; Ensemble Learning; Random Forest; Software Defect Prediction; Software Engineering; Stacking},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Banh2025,
	author = {Banh, Leonardo and Holldack, Florian and Strobel, Gero},
	title = {Copiloting the future: How generative AI transforms Software Engineering},
	year = {2025},
	journal = {Information and Software Technology},
	volume = {183},
	pages = {},
	doi = {10.1016/j.infsof.2025.107751},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002314681&doi=10.1016%2Fj.infsof.2025.107751&partnerID=40&md5=ec957fd7183021433113edc8f25894b6},
	abstract = {Context: With rapid technological advancements, artificial intelligence (AI) has become integral to various sectors. Generative AI (GenAI) tools like ChatGPT or GitHub Copilot, with their unique content creation capabilities, pose transformative potential in Software Engineering by offering new ways to optimize software development processes. However, the integration into current processes also presents challenges that require a sociotechnical analysis to effectively realize GenAI's potential. Objective: This study investigates how GenAI can be leveraged in the domain of Software Engineering, exploring its action potentials and challenges to help businesses and developers optimize the adoption of this technology in their workflows. Method: We performed a qualitative study and collected data from expert interviews with eighteen professionals working in Software Engineering-related roles. Data analysis followed the principles of Grounded Theory to analyze how GenAI supports developers' goals, aligns with organizational practices, and facilitates integration into existing routines. Results: The findings demonstrate several opportunities of GenAI in Software Engineering to increase productivity in development teams. However, several key barriers were also identified, that should be accounted for in successful integrations. We synthesize the results in a grounded conceptual framework for GenAI adoption in Software Engineering. Conclusions: This study contributes to the discourse on GenAI in Software Engineering by providing a conceptual framework that aids in understanding the opportunities and challenges of GenAI. It offers practical guidelines for businesses and developers to enhance GenAI integration and lays the groundwork for future research on its impact in software development. © 2025 The Author(s)},
	author_keywords = {Generative AI; Grounded Theory; Information system development; Software Engineering},
	keywords = {Software design; 'current; Artificial intelligence tools; Conceptual frameworks; Content creation; Generative artificial intelligence; Grounded theory; Information system development; Sociotechnical; Software development process; Technological advancement; Information use},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Kozub2025463,
	author = {Kozub, Vladyslav and Druzhynin, Valentyn and Trufanova, Diana and Ihnatenko, Pavlo and Kolos, Kateryna},
	title = {Using artificial intelligence in software development processes: Achievements and challenges},
	year = {2025},
	journal = {Sustainable Engineering and Innovation},
	volume = {7},
	number = {2},
	pages = {463 - 476},
	doi = {10.37868/sei.v7i2.id526},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019982668&doi=10.37868%2Fsei.v7i2.id526&partnerID=40&md5=63361c9cb7a2d521e7292c0c954e43d6},
	abstract = {This study consolidates contemporary methodologies for applying artificial intelligence in software engineering. Using the PRISMA protocol, an analysis of 60 peer-reviewed publications was conducted. Findings indicate that the use of generative tools (such as GitHub Copilot), AI-based testing platforms (like Testim.io and Diffblue), and DevOps automation systems (e.g., Harness.io) can lead to a 20–40% reduction in development time, while also enhancing code quality and minimizing errors. A key academic contribution of the research is the introduction of a three-tier classification of integration barriers – technical, organizational, and legal – that hinder the seamless adoption of AI technologies within the Software Development Life Cycle (SDLC), as well as the lack of standardized methodologies. The recommendations provided in this work are particularly relevant to software engineers, IT project leaders, and academic researchers, as they address crucial concerns related to model interpretability, system instability, the absence of unified standards, and regulatory ambiguity. The practical relevance of the study lies in presenting actionable strategies for the responsible, scalable, and ethically grounded deployment of AI-driven tools in industrial, academic, and research settings. © The Author 2025.},
	author_keywords = {Artificial intelligence; Code generation; Development automation; Ethical challenges; Software development},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Anderson2025,
	author = {Anderson, Stuart and Falconer, Ruth E.},
	title = {Emergence of Player Tactics by expert-guided Machine Learning: An industry tower defence case study},
	year = {2025},
	journal = {Entertainment Computing},
	volume = {54},
	pages = {},
	doi = {10.1016/j.entcom.2025.100963},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005765427&doi=10.1016%2Fj.entcom.2025.100963&partnerID=40&md5=1289214fe232e5bdae06d613640ca858},
	abstract = {Modern games generate a large amount of player data that can enhance the game design and development process. Game developers can potentially utilise data science methods to extract information to inform decision making as they strive to improve the user experience and meet business goals. However, this practice is far from widespread due to the specialist expertise needed. Additionally, games are often complex, where the large number of possible player actions creates datasets with a vast state space and high dimensionality. Additionally, these player actions often require context to fully interpret and analyse. In this emerging research field, a further challenge is in ensuring proposed methods are suitable for commercial game development environments, where genres, available data sources and the production process must be considered. This work presents the results of an industry–academic collaboration, applying the less common player tactic classification method, individual sequence mining, on a fast paced, commercially available tower defence mobile game. It proposes and evaluates a novel pipeline for validating and discovering player tactics to facilitate game balancing. Rather than being applied to data captured from an analytics framework, the analysis was conducted on data captured from network messages generated by game clients. The real-time nature of these network messages creates potential for this data source to have value beyond tactics classification, with opportunities to integrate into AI pipelines for purposes such as automation. The resulting mixed methods process demonstrates the ability of using this data source to generate insight on player tactics to game development teams, and it being feasible within the commercial game development process. The pipeline can be applied by other games companies seeking to extract value from data that is collected to make better games for their player base. © 2025 The Authors},
	author_keywords = {Commercial case study; Data science; Player behaviour; Sequence mining; Tower defence},
	keywords = {Behavioral research; Information retrieval; Information use; Risk perception; Case-studies; Commercial case study; Data-source; Game development; Machine-learning; Network messages; Player action; Player behavior; Sequence mining; Tower defense; Game design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Eshraghian20251659,
	author = {Eshraghian, Farjam and Hafezieh, Najmeh and Farivar, Farveh and de Cesare, Sergio},
	title = {AI in software programming: understanding emotional responses to GitHub Copilot},
	year = {2025},
	journal = {Information Technology and People},
	volume = {38},
	number = {4},
	pages = {1659 - 1685},
	doi = {10.1108/ITP-01-2023-0084},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189068574&doi=10.1108%2FITP-01-2023-0084&partnerID=40&md5=8731559f543a974dd3a514955bc1ecfb},
	abstract = {Purpose: The applications of Artificial Intelligence (AI) in various areas of professional and knowledge work are growing. Emotions play an important role in how users incorporate a technology into their work practices. The current study draws on work in the areas of AI-powered technologies adaptation, emotions, and the future of work, to investigate how knowledge workers feel about adopting AI in their work. Design/methodology/approach: We gathered 107,111 tweets about the new AI programmer, GitHub Copilot, launched by GitHub and analysed the data in three stages. First, after cleaning and filtering the data, we applied the topic modelling method to analyse 16,130 tweets posted by 10,301 software programmers to identify the emotions they expressed. Then, we analysed the outcome topics qualitatively to understand the stimulus characteristics driving those emotions. Finally, we analysed a sample of tweets to explore how emotional responses changed over time. Findings: We found six categories of emotions among software programmers: challenge, achievement, loss, deterrence, scepticism, and apathy. In addition, we found these emotions were driven by four stimulus characteristics: AI development, AI functionality, identity work, and AI engagement. We also examined the change in emotions over time. The results indicate that negative emotions changed to more positive emotions once software programmers redirected their attention to the AI programmer's capabilities and functionalities, and related that to their identity work. Practical implications: Overall, as organisations start adopting AI-powered technologies in their software development practices, our research offers practical guidance to managers by identifying factors that can change negative emotions to positive emotions. Originality/value: Our study makes a timely contribution to the discussions on AI and the future of work through the lens of emotions. In contrast to nascent discussions on the role of AI in high-skilled jobs that show knowledge workers' general ambivalence towards AI, we find knowledge workers show more positive emotions over time and as they engage more with AI. In addition, this study unveils the role of professional identity in leading to more positive emotions towards AI, as knowledge workers view such technology as a means of expanding their identity rather than as a threat to it. © 2024, Emerald Publishing Limited.},
	author_keywords = {AI programmer; Artificial intelligence; Emotion; GitHub Copilot; Twitter},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access}
}

@ARTICLE{Johnstone2025,
	author = {Johnstone, Michael N. and Yang, Wencheng and Ahmed, Mohiuddin},
	title = {Using Machine Learning to Detect Vault (Anti-Forensic) Apps},
	year = {2025},
	journal = {Future Internet},
	volume = {17},
	number = {5},
	pages = {},
	doi = {10.3390/fi17050186},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006446980&doi=10.3390%2Ffi17050186&partnerID=40&md5=c8d169ab317d28c2bf78436e4f604aa1},
	abstract = {Content hiding, or vault applications (apps), are designed with a secondary, often concealed purpose, such as encrypting and storing files. While these apps may serve legitimate functions, they unequivocally present significant challenges for law enforcement. Conventional methods for tackling this issue, whether static or dynamic, prove inadequate when devices—typically smartphones—cannot be modified. Additionally, these methods frequently require prior knowledge of which apps are classified as vault apps. This research decisively demonstrates that a non-invasive method of app analysis, combined with machine learning, can effectively identify vault apps. Our findings reveal that it is entirely possible to detect an Android vault app with 98% accuracy using a random forest classifier. This clearly indicates that our approach can be instrumental for law enforcement in their efforts to address this critical issue. © 2025 by the authors.},
	author_keywords = {Android; content hiding; machine learning; malware detection; software development; vault apps},
	keywords = {Computer operating systems; Enterprise software; Malware; Mobile applications; Software prototyping; Terrorism; Android; Anti-Forensics; Classifieds; Content hiding; Conventional methods; Machine-learning; Malware detection; Prior-knowledge; Smart phones; Vault application; Software design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Ramachandran2025,
	author = {Ramachandran, Muthu and Fouracre, Steven},
	title = {Rich Data Versus Quantity of Data in Code Generation AI: A Paradigm Shift for Healthcare},
	year = {2025},
	journal = {Blockchain in Healthcare Today},
	volume = {8},
	number = {1},
	pages = {},
	doi = {10.30953/bhty.v8.396},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009707457&doi=10.30953%2Fbhty.v8.396&partnerID=40&md5=6d0df714b6ecc7851ce052778d69534a},
	abstract = {In the context of Code Generation AI (Code Gen AI), “rich” and “quality” data refer to datasets that are not only syntactically and structurally sound but also context-aware, domain-specific, and semantically aligned with the target application. Unlike large-scale, general-purpose code corpora scraped from open repositories, rich datasets are curated to reflect regulatory requirements, architectural patterns, and problem-solving con­ventions within a given field. This distinction is critically important when deploying Code Gen AI in the healthcare sector, where software must meet rigorous standards for safety, auditability, and compliance. Blindly scaling models with low-quality or irrelevant data may lead to brittle, error-prone systems—posing risks not only to patients and providers but also to the integrity of digital healthcare infrastructure. This issue has not been fully addressed in the Code Gen AI research to date. This article evaluates the critical trade-offs between “rich data” and “data quantity” strategies in Code Gen AI and autonomous code agents, focusing on high-integrity sectors such as healthcare. While Code Gen AI can enhance productivity by up to 55% in controlled environments, models trained on unfiltered, large-scale datasets often increase code dupli­cation, churn, and error rates. The central challenge is balancing performance gains with reliability, maintainability, and ethical account­ability. In healthcare, codebases must embody accuracy, traceability, and data privacy—attributes often diluted in large but uncurated training sets. Using Self-Evolving Software as a case study, this article con­trasts the outcomes of both approaches and introduces a weighted data selection matrix tailored to Code Gen AI systems. The findings demonstrate that rich, curated, domain-specific datasets consistently pro­duce more robust, compliant, and sustainable code, especially in sectors where quality and governance are non-negotiable. Plain Language Summary The authors compare two different approaches to training AI systems that write computer code: One that uses massive amounts of general code data (“quantity approach”) versus one that uses smaller but higher-quality, specialized data (“rich data approach”). The research reveals that while the quantity approach might be faster to set up and works for general coding tasks, it often creates serious problems in sensitive areas like healthcare software. These problems include du­plicate code, security vulnerabilities, and code that does not comply with regulations like the Health Insurance Portability and Accountability Act of 1996. The researchers studied a system called Self-Evolving Software (SES) that uses the rich data approach. In healthcare settings, SES produced better results by generating code that results in less rework time and elimination of repeated documentation errors, all fully compliant with healthcare privacy laws. The authors conclude that for important software in regulated industries like healthcare, the quality of data used to train artificial intelligence coding tools matters much more than the quantity. Using carefully selected, well-documented code examples rather than scraping massive amounts of code from public repositories is recommended. This approach creates more reliable, secure, and maintainable software, especially when health or privacy is at stake. © 2025, The Author(s).},
	author_keywords = {Code Gen AI in healthcare; code generative AI; large language models; LLM; rich data; self-evolving software; SES; software engineering},
	keywords = {Codes (symbols); Computer software selection and evaluation; Data integrity; Data privacy; Data reliability; Ethical aspects; Health care; Large datasets; Medical computing; Regulatory compliance; Code generation AI in healthcare; Code generative AI; Codegeneration; Domain specific; Language model; Large language model; LLM; Rich data; Self-evolving software; Economic and social effects},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Gaitantzi2025,
	author = {Gaitantzi, Alkmini and Kazanidis, Ioannis K.},
	title = {The Role of Artificial Intelligence in Computer Science Education: A Systematic Review with a Focus on Database Instruction},
	year = {2025},
	journal = {Applied Sciences (Switzerland)},
	volume = {15},
	number = {7},
	pages = {},
	doi = {10.3390/app15073960},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002282378&doi=10.3390%2Fapp15073960&partnerID=40&md5=6169881ea17da278e8fce16bc250b3c2},
	abstract = {The integration of artificial intelligence (AI) into computer science (CS) education is evolving, yet its specific application in database instruction remains underexplored. This systematic review analyzes 31 empirical studies published between 2020 and 2025, examining how AI applications support teaching and learning in CS, with an emphasis on database education. Following the PRISMA methodology, the review categorizes AI applications according to instructional design models, roles, actions, benefits, and challenges. Findings indicate that AI tools, particularly chatbots, intelligent tutoring systems, and code generators, effectively support personalized instruction, immediate feedback, and interactive problem-solving across CS and database-specific contexts. However, challenges persist, including AI inaccuracies, biases, student dependency in AI, and academic integrity risks. The review also identifies a shift in programming education as AI reshapes software development practices, prompting a need to align curricula with evolving industry expectations. Despite growing attention to AI applications in programming education, database-related research remains limited. This review highlights the necessity for further empirical investigations specifically in database instruction, including more extensive studies addressing AI-driven pedagogical strategies and their long-term impacts. The results suggest that careful integration of AI tools can complement traditional instruction, emphasizing the critical role of human educators in achieving meaningful and effective learning outcomes. © 2025 by the authors.},
	author_keywords = {AI in education; artificial intelligence; ChatGPT; computer science education; database instruction; generative AI; programming education; systematic review},
	keywords = {Adversarial machine learning; Contrastive Learning; Artificial intelligence in education; Artificial intelligence tools; ChatGPT; Computer Science Education; Database instruction; Empirical studies; Generative artificial intelligence; Programming education; Systematic Review; Teaching and learning},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access}
}

@ARTICLE{Haque2025,
	author = {Haque, Md Asraful},
	title = {LLMs: A game-changer for software engineers?},
	year = {2025},
	journal = {BenchCouncil Transactions on Benchmarks, Standards and Evaluations},
	volume = {5},
	number = {1},
	pages = {},
	doi = {10.1016/j.tbench.2025.100204},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006809602&doi=10.1016%2Fj.tbench.2025.100204&partnerID=40&md5=a0f0660deab18ec9f0e970c193521112},
	abstract = {Large Language Models (LLMs) like GPT-3 and GPT-4 have emerged as groundbreaking innovations with capabilities that extend far beyond traditional AI applications. These sophisticated models, trained on massive datasets, can generate human-like text, respond to complex queries, and even write and interpret code. Their potential to revolutionize software development has captivated the software engineering (SE) community, sparking debates about their transformative impact. Through a critical analysis of technical strengths, limitations, real-world case studies, and future research directions, this paper argues that LLMs are not just reshaping how software is developed but are redefining the role of developers. While challenges persist, LLMs offer unprecedented opportunities for innovation and collaboration. Early adoption of LLMs in software engineering is crucial to stay competitive in this rapidly evolving landscape. This paper serves as a guide, helping developers, organizations, and researchers understand how to harness the power of LLMs to streamline workflows and acquire the necessary skills. © 2025},
	author_keywords = {AI tools; Coding; Debugging; Large language model; Software engineering; Testing},
	keywords = {Application programs; Computer aided software engineering; Computer operating systems; Computer program listings; Computer software maintenance; Computer software selection and evaluation; Groupware; Program processors; Search engines; Software agents; Software packages; Software prototyping; Software quality; Software testing; Utility programs; Verification; AI applications; AI tool; Coding; Complex queries; Debugging; Engineering community; Human like; Language model; Large language model; Massive data sets; Software design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access}
}

@ARTICLE{Di Francesco2025,
	author = {Di Francesco, Domenic},
	title = {Risk management in the era of data-centric engineering},
	year = {2025},
	journal = {Data-Centric Engineering},
	volume = {6},
	pages = {},
	doi = {10.1017/dce.2025.10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219124598&doi=10.1017%2Fdce.2025.10&partnerID=40&md5=3084939faeb2ad6716754429f9f69474},
	abstract = {Novel methods of data collection and analysis can enhance traditional risk management practices that rely on expert engineering judgment and established safety records, specifically when key conditions are met: Analysis is linked to the decisions it is intended to support, standards and competencies remain up to date, and assurance and verification activities are performed. This article elaborates on these conditions. The reason engineers are required to perform calculations is to support decision-making. Since humans are famously weak natural statisticians, rather than ask stakeholders to implicitly assimilate data, and arrive at a decision, we can instead rely on subject matter experts to explicitly define risk management decision problems. The results of engineering calculation can then also communicate which interventions (if any) are considered to be risk-optimal. It is also proposed that the next generation of engineering standards should learn from the success of open source software development in community building. Interacting with open datasets and code can promote engagement, identification (and resolution) of errors, training and ultimately competence. Finally, the profession's tradition of independent verification should also be applied to the complex models that will increasingly contribute to the safety of the built environment. Model assurance will be required to keep pace with model development to identify suitable use cases as adequately safe. These are considered to be increasingly important components in ensuring that methods of data-centric engineering can be safely and appropriately adopted in industry. © The Author(s), 2025.},
	author_keywords = {assurance; computational statistics; machine learning; risk management},
	keywords = {Decision making; Open source software; Records management; Risk assessment; Risk management; Assurance; Computational statistics; Condition; Data centric; Machine-learning; Methods of data analysis; Methods of data collections; Novel methods; Risk management practices; Risks management; Software design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Ramos2025,
	author = {Ramos, Tafline and Dean, Amanda and McGregor, David},
	title = {AI-Augmented Software Engineering: Revolutionizing or Challenging Software Quality and Testing?},
	year = {2025},
	journal = {Journal of Software: Evolution and Process},
	volume = {37},
	number = {2},
	pages = {},
	doi = {10.1002/smr.2741},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209802239&doi=10.1002%2Fsmr.2741&partnerID=40&md5=a0abcd95f10994d07d89fad632247e38},
	abstract = {With organizations seeking faster, cheaper, and smarter ways of delivering higher quality software, many are looking towards generative artificial intelligence (AI) to drive efficiencies and innovation throughout the software development lifecycle. However, generative AI can suffer from several fundamental issues, including a lack of traceability in concept generation and decision-making, the potential for making incorrect inferences (hallucinations), shortcomings in response quality, and bias. Quality engineering (QE) has long been utilized to enable more efficient and effective delivery of higher quality software. A core aspect of QE is adopting quality models to support various lifecycle practices, including requirements definition, quality risk assessments, and testing. In this position paper, we introduce the application of QE to AI systems, consider shortcomings in existing AI quality models from the International Organization for Standardization (ISO), and propose extensions to ISO models based on the results of a survey. We also reflect on skills that IT graduates may need in the future, to support delivery of better-quality AI. © 2024 John Wiley & Sons Ltd.},
	author_keywords = {AI; artificial intelligence; generative AI; ISO; quality; quality engineering; quality models; testing},
	keywords = {Generative adversarial networks; Generative artificial intelligence; High-quality software; International organization for standardizations; Looking toward; Quality; Quality engineering; Quality modeling; Software development life-cycle; Software Quality; Software testings; Software quality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Bronze Open Access}
}

@ARTICLE{Quaranta2025,
	author = {Quaranta, Luigi and Azevedo, Kelly and Calefato, Fabio and Kalinowski, M.},
	title = {A multivocal literature review on the benefits and limitations of industry-leading AutoML tools},
	year = {2025},
	journal = {Information and Software Technology},
	volume = {178},
	pages = {},
	doi = {10.1016/j.infsof.2024.107608},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207749797&doi=10.1016%2Fj.infsof.2024.107608&partnerID=40&md5=50a8fcd587298a03f2dea29d8f11209d},
	abstract = {Context: Rapid advancements in Artificial Intelligence (AI) and Machine Learning (ML) are revolutionizing software engineering in every application domain, driving unprecedented transformations and fostering innovation. However, despite these advances, several organizations are experiencing friction in the adoption of ML-based technologies, mainly due to the current shortage of ML professionals. In this context, Automated Machine Learning (AutoML) techniques have been presented as a promising solution to democratize ML adoption, even in the absence of specialized people. Objective: Our research aims to provide an overview of the evidence on the benefits and limitations of AutoML tools being adopted in industry. Methods: We conducted a Multivocal Literature Review, which allowed us to identify 54 sources from the academic literature and 108 sources from the grey literature reporting on AutoML benefits and limitations. We extracted explicitly reported benefits and limitations from the papers and applied the thematic analysis method for synthesis. Results: In general, we identified 18 reported benefits and 25 limitations. Concerning the benefits, we highlight that AutoML tools can help streamline the core steps of ML workflows, namely data preparation, feature engineering, model construction, and hyperparameter tuning—with concrete benefits on model performance, efficiency, and scalability. In addition, AutoML empowers both novice and experienced data scientists, promoting ML accessibility. However, we highlight several limitations that may represent obstacles to the widespread adoption of AutoML. For instance, AutoML tools may introduce barriers to transparency and interoperability, exhibit limited flexibility for complex scenarios, and offer inconsistent coverage of the ML workflow. Conclusion: The effectiveness of AutoML in facilitating the adoption of machine learning by users may vary depending on the specific tool and the context in which it is used. Today, AutoML tools are used to increase human expertise rather than replace it and, as such, require skilled users. © 2024 Elsevier B.V.},
	author_keywords = {Autoai; Automl; Benefits; Limitations; Multivocal literature review},
	keywords = {Application programs; Autoai; Automated machines; Automl; Benefit; Learning tool; Limitation; Literature reviews; Machine-learning; Multivocal literature review; Work-flows; Adversarial machine learning},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Zhang2025,
	author = {Zhang, Hongyi and Bosch, Jan and Olsson, Helena Holmström},
	title = {Enabling efficient and low-effort decentralized federated learning with the EdgeFL framework},
	year = {2025},
	journal = {Information and Software Technology},
	volume = {178},
	pages = {},
	doi = {10.1016/j.infsof.2024.107600},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207074131&doi=10.1016%2Fj.infsof.2024.107600&partnerID=40&md5=09559e09cc3fc2eae11ac11fecb759ba},
	abstract = {Context: Federated Learning (FL) has gained prominence as a solution for preserving data privacy in machine learning applications. However, existing FL frameworks pose challenges for software engineers due to implementation complexity, limited customization options, and scalability issues. These limitations prevent the practical deployment of FL, especially in dynamic and resource-constrained edge environments, preventing its widespread adoption. Objective: To address these challenges, we propose EdgeFL, an efficient and low-effort FL framework designed to overcome centralized aggregation, implementation complexity and scalability limitations. EdgeFL applies a decentralized architecture that eliminates reliance on a central server by enabling direct model training and aggregation among edge nodes, which enhances fault tolerance and adaptability to diverse edge environments. Methods: We conducted experiments and a case study to demonstrate the effectiveness of EdgeFL. Our approach focuses on reducing weight update latency and facilitating faster model evolution on edge devices. Results: Our findings indicate that EdgeFL outperforms existing FL frameworks in terms of learning efficiency and performance. By enabling quicker model evolution on edge devices, EdgeFL enhances overall efficiency and responsiveness to changing data patterns. Conclusion: EdgeFL offers a solution for software engineers and companies seeking the benefits of FL, while effectively overcoming the challenges and privacy concerns associated with traditional FL frameworks. Its decentralized approach, simplified implementation, combined with enhanced customization and fault tolerance, make it suitable for diverse applications and industries. © 2024 The Authors},
	author_keywords = {Decentralized architecture; Federated learning; Information privacy; Machine learning; Software engineering},
	keywords = {Adversarial machine learning; Contrastive Learning; Differential privacy; Customisation; Decentralised; Decentralized architecture; Implementation complexity; Information privacy; Learning frameworks; Machine learning applications; Machine-learning; Model evolution; Scalability issue; Federated learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Protschky202534518,
	author = {Protschky, Dominik and Lammermann, Luis and Hofmann, Peter and Urbach, Nils},
	title = {What Gets Measured Gets Improved: Monitoring Machine Learning Applications in Their Production Environments},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {34518 - 34538},
	doi = {10.1109/ACCESS.2025.3534628},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217885336&doi=10.1109%2FACCESS.2025.3534628&partnerID=40&md5=30b73348b767d75869f273c88feb93ee},
	abstract = {Machine learning (ML) applications face many new, hardly predictable aspects in their production environments. Detecting new aspects in an ML production environment and understanding their impacts on the ML application is crucial if organizations are to ensure ML applications functionality. A monitoring entity is essential if one is to monitor ML applications in their production environments, to both continually minimize risks and improve ML application's performance. But existing monitoring approaches are struggling to deal with specifics that arise from ML applications. We aim at deriving monitoring practices and providing a holistic view over required steps in successful ML applications monitoring. Since there has been little research on this topic, we followed a qualitative research approach, i.e., we conducted an interview study combined with a multivocal literature review. Thus, we provide a theoretical framework of an ML-enabled agent in its production environment, five characteristics of ML applications' production environments and 17 monitoring practices - 14 practices arranged sequentially on a typical quality management cycle and three cross-sectional practices. To outline the ML specifics that arise in monitoring ML applications, we investigate the five ML production environment characteristics' influences on the ML monitoring practices. © 2013 IEEE.},
	author_keywords = {Machine learning; MLOps; monitoring; software engineering},
	keywords = {Application monitoring; Holistic view; Machine learning applications; Machine-learning; MLOp; Monitoring approach; Performance; Production environments; Qualitative research; Research approach; Quality management},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Mustaqeem2025866,
	author = {Mustaqeem, Mohd and Alam, Mahfooz and Mustajab, Suhel and Alshanketi, Faisal and Alam, Shadab and Shuaib, Mohammed},
	title = {Comprehensive Bibliographic Survey and Forward-Looking Recommendations for Software Defect Prediction: Datasets, Validation Methodologies, Prediction Approaches, and Tools},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {866 - 903},
	doi = {10.1109/ACCESS.2024.3517419},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212392066&doi=10.1109%2FACCESS.2024.3517419&partnerID=40&md5=7fce1fe7946fdca2772ff4a36413d6ad},
	abstract = {The development of reliable software depends heavily on the effective collaboration between teams responsible for development and testing. Despite ongoing efforts, many software programs still contain bugs that can lead to financial losses and business risks. Therefore, detecting and fixing software defects after release is crucial. While binary classification methods have been commonly used for this purpose, recent Artificial Intelligence (AI) advancements offer new opportunities for software teams to create more robust software. To address challenges in Software Defect Prediction (SDP), we conducted a thorough bibliographic survey of 79 research articles from the year 2011 to 2023 that examined previous models, datasets, data validation techniques, defect detection, prediction methods, and SDP tools. The survey revealed that previous research often lacked appropriate datasets with the necessary characteristics and data validation methods. Additionally, many standard datasets suffer from a lack of labels, which hinders effective defect detection. Systematic literature reviews on SDP are scarce, further emphasizing the importance of this study. Based on the findings, we provide crucial recommendations for designing effective SDP models and tools. The proposed survey outlines an architecture for constructing SDP datasets with the appropriate characteristics, as well as multi-label classification and data validation methodologies for software defects. This approach aims to enhance SDP research and contribute to the development of high-quality software products by improving defect prediction accuracy. © 2024 The Authors.},
	author_keywords = {artificial intelligence; bibliographic survey; classification; machine learning; search-based techniques; Software defect prediction; statistical validation},
	keywords = {Bibliographic survey; Data validation; Defect detection; Machine-learning; Search-based; Search-based technique; Software defect prediction; Software defects; Statistical validation; Validation methodologies},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Gold Open Access}
}

@ARTICLE{Faraji2025183296,
	author = {Faraji, Aazaade and Pombo, Nuno},
	title = {AI-Driven Software Test Automation: An AI4SE-Oriented Survey of Techniques, Tools, and Challenges},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {183296 - 183313},
	doi = {10.1109/ACCESS.2025.3623944},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019668292&doi=10.1109%2FACCESS.2025.3623944&partnerID=40&md5=3e874c6ed353a9cd2f02cf257ea3d645},
	abstract = {The growing complexity of software systems and the need for more rapid, high-quality software releases have created the need for intelligent and automated testing mechanisms. Drawing on AI-driven test automation of software in the larger context of Artificial Intelligence for Software Engineering, this survey discusses how artificial intelligence methods are used to augment and automate multiple stages of the software test cycle, ranging from test planning, generation, execution, repair, and maintenance. We offer a systematic overview of 76 industrial tools and peer-reviewed research and examine the contribution of Machine Learning, Deep Learning, Large Language Models, Reinforcement Learning, and other paradigms of artificial intelligence to modern test automation pipelines. The paper suggests a lifecycle-oriented taxonomy, aligning industry with research with industrial practices, and offers structured visual analyses to map current trends, limitations, and emerging research opportunities. Special focus is awarded to untapped areas such as Explainable Artificial Intelligence, Natural Language Processing-based test scripting, and hybrid AI strategies. This survey also acts as a ready reference guide for researchers and practitioners towards AI-driven software testing under the framework of Artificial Intelligence for Software Engineering. © 2013 IEEE.},
	author_keywords = {Artificial intelligence; automated software testing; deep learning; intelligent system; large language models; machine learning; reinforcement learning; software testing; test automation; test case generation},
	keywords = {Automation; Computer software maintenance; Computer software selection and evaluation; Deep learning; Industrial research; Learning algorithms; Learning systems; Life cycle; Machine tools; Software design; Software quality; Automated software testing; Language model; Large language model; Machine-learning; Reinforcement learnings; Software test automation; Software testings; Test Automation; Test case generation; Software testing},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Burachynskyi202559,
	author = {Burachynskyi, Andrii and Shantyr, Anton},
	title = {Overview of Artificial Intelligence Application Methods in Software Development},
	year = {2025},
	journal = {Informatica (Slovenia)},
	volume = {49},
	number = {28},
	pages = {59 - 72},
	doi = {10.31449/inf.v49i28.8694},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017760009&doi=10.31449%2Finf.v49i28.8694&partnerID=40&md5=f1bb2868622e9007102d528de17c00e2},
	abstract = {The study aimed to analyse modern approaches to the integration of artificial intelligence into the software development process to optimise workflows and improve the quality of end products. The study analysed existing research and practical examples of artificial intelligence applications at different stages of the software life cycle. The study covered the automation of key tasks such as requirements analysis, design, code creation and testing, as well as project management and support of software systems. The study results demonstrated that the use of artificial intelligence, in particular machine learning models and deep neural networks, can significantly reduce development time and costs by automating routine tasks such as code generation and test scenarios. It also helps to improve product quality by automatically detecting defects and predicting potential points of failure, which ensures more stable software operation. In addition, the use of artificial intelligence improves project management, including more accurate timeline planning, resource allocation, and risk management, which improves the efficiency of the organisation of development teams. The study also analysed the optimisation of communication between developers and stakeholders by applying natural language processing techniques to analyse requirements, which reduces the probability of errors in specifications and helps to create better products. In addition, the study addressed the prospects of using artificial intelligence in the processes of continuous integration and delivery, as well as in real-time monitoring of software performance, which contributes to the proactive detection of possible failures and rapid response to them. Recommendations on the effective use of artificial intelligence to automate and optimise the software development process were provided. This will help minimise risks, improve the cost-effectiveness of projects and support the development of intelligent systems that can adapt to changes. © (2024), (). All Rights Reserved.},
	author_keywords = {automation; machine learning; natural language; productivity; project management; testing},
	keywords = {Application programs; Cost effectiveness; Human resource management; Intelligent systems; Learning algorithms; Learning systems; Life cycle; Machine learning; Natural language processing systems; Risk management; Software design; Software testing; Analysis/design; Application method; Different stages; End-products; Machine-learning; Natural languages; Requirement analysis; Software development process; Software life cycles; Work-flows; Automation; Project management},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{De La Cruz20253569,
	author = {De La Cruz, Elyson and Le, Hanh and Meduri, Karthik and Nadella, Geeta Sandeep and Gonaygunta, Hari},
	title = {Redefining the Programmer: Human-AI Collaboration, LLMs, and Security in Modern Software Engineering},
	year = {2025},
	journal = {Computers, Materials and Continua},
	volume = {85},
	number = {2},
	pages = {3569 - 3582},
	doi = {10.32604/cmc.2025.068137},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017256288&doi=10.32604%2Fcmc.2025.068137&partnerID=40&md5=f1ffa78f298e6a1e5ec09e33ff6a867b},
	abstract = {The rapid integration of artificial intelligence (AI) into software development, driven by large language models (LLMs), is reshaping the role of programmers from traditional coders into strategic collaborators within Industry 4.0 ecosystems. This qualitative study employs a hermeneutic phenomenological approach to explore the lived experiences of Information Technology (IT) professionals as they navigate a dynamic technological landscape marked by intelligent automation, shifting professional identities, and emerging ethical concerns. Findings indicate that developers are actively adapting to AI-augmented environments by engaging in continuous upskilling, prompt engineering, interdisciplinary collaboration, and heightened ethical awareness. However, participants also voiced growing concerns about the reliability and security of AI-generated code, noting that these tools can introduce hidden vulnerabilities and reduce critical engagement due to automation bias. Many described instances of flawed logic, insecure patterns, or syntactically correct but contextually inappropriate suggestions, underscoring the need for rigorous human oversight. Additionally, the study reveals anxieties around job displacement and the gradual erosion of fundamental coding skills, particularly in environments where AI tools dominate routine development tasks. These findings highlight an urgent need for educational reforms, industry standards, and organizational policies that prioritize both technical robustness and the preservation of human expertise. As AI becomes increasingly embedded in software engineering workflows, this research offers timely insights into how developers and organizations can responsibly integrate intelligent systems to promote accountability, resilience, and innovation across the software development lifecycle. © © 2025 The Authors.},
	author_keywords = {AI security; AI-assisted programming; developer identity; ethical AI in software development; Human-AI collaboration; large language models},
	keywords = {Artificial intelligence; Automation; Codes (symbols); Computer aided software engineering; Computer programming; Engineering education; Engineering research; Ethical technology; Life cycle; Artificial intelligence security; Artificial intelligence-assisted programming; Developer identity; Ethical artificial intelligence in software development; Human-artificial intelligence collaboration; Language model; Language security; Large language model; Phenomenological approach; Qualitative study; Software design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Novkin2025,
	author = {Novkin, Rodion and Amrouch, Hussam},
	title = {Kolmogorov–Arnold Network for Transistor Compact Modeling},
	year = {2025},
	journal = {Advanced Intelligent Systems},
	pages = {},
	doi = {10.1002/aisy.202500325},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013148578&doi=10.1002%2Faisy.202500325&partnerID=40&md5=fca7feaf5e6f5857d1637d3426272033},
	abstract = {Neural network (NN)-based transistor compact modeling is a transformative solution for accelerating device modeling and SPICE circuit simulations. However, conventional NN architectures primarily function as black-box problem solvers. This lack of interpretability significantly limits their capacity to extract and convey meaningful insights into learned data patterns, posing a major barrier to their broader adoption in critical modeling tasks. This work introduces Kolmogorov–Arnold network (KAN) for the transistor—a groundbreaking NN architecture that seamlessly integrates interpretability with high precision in physics-based function modeling. The performance of KAN and Fourier KAN (FKAN) is systematically evaluated for fin field-effect transistor (FinFET) compact modeling, benchmarking them against the golden industry-standard compact model and the widely used multi-layer perceptron architecture. The results reveal that KAN and FKAN consistently achieve superior prediction accuracy for critical figures of merit, including gate current (I<inf>D</inf>), drain charge (Q<inf>D</inf>), and source charge (Q<inf>S</inf>). Furthermore, the unique ability of KAN to derive symbolic formulas from learned data patterns is demonstrated and improved—a capability that not only enhances interpretability but also facilitates in-depth transistor analysis and optimization. Additionally, we identify challenges inherent to KAN and FKAN architectures, which limit their ability to achieve a general-purpose SPICE simulation suitability. © 2025 The Author(s). Advanced Intelligent Systems published by Wiley-VCH GmbH.},
	author_keywords = {circuit simulation; Kolmogorov–Arnold network; machine learning; symbolic regression; transistor compact model},
	keywords = {Computer aided software engineering; Drain current; Intelligent systems; Neural networks; SPICE; Compact model; Data patterns; Fourier; Interpretability; Kolmogorov; Kolmogorov–arnold network; Machine-learning; Neural network architecture; Symbolic regression; Transistor compact model; FinFET; Learning systems},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Izhar202583591,
	author = {Izhar, Rahat and Cosh, Kenneth J. and Ramingwon, Lachana and Ramingwong, Sakgasit and Bhatti, Shahid N.},
	title = {An Efficient Methodology for the Categorization of Software Requirements Using Natural Language Processing and Similarity Analysis},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {83591 - 83606},
	doi = {10.1109/ACCESS.2025.3568504},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004911348&doi=10.1109%2FACCESS.2025.3568504&partnerID=40&md5=d60da510fe0931f0e7540659668aa487},
	abstract = {The classification of software requirements into functional (FRs) and non-functional requirements (NFRs) is indispensable for the efficacious implementation of software systems. Traditionally, this endeavor reliant on manual exertion, this process has proven to be both protracted and inherently susceptible to errors, and inaccuracies. Recent advancements in machine learning (ML) have begun to offer promising avenues for automation, enhancing both the efficiency and accuracy of requirement classification. The following research study proposes “NLPReqClassifier,” a lightweight automated classification model that integrates Term Frequency-Inverse Document Frequency (TF-IDF) and Cosine Similarity. Leveraging the PROMISE dataset, which is enriched continually by academic and professional input, this model addresses the existing shortcomings of traditional classification methods. The proposed model utilizes a dual-evaluation approach, ensuring relevance and precision across varied software development contexts. By incorporating iterative feedback into the model’s training process, this research not only aligns with academic standards but also meets the practical demands of the industry. The model’s performance was tested in both academic settings dataset and real-world industry dataset, particularly focusing on its application in Enterprise Resource Planning (ERP) systems. The proposed model demonstrated superior capability to categorize a broad spectrum of software requirements accurately, outperforming existing traditional methodologies in terms of adaptability and efficiency. It showed significant improvements over traditional classification methods, particularly in its ability to dynamically adapt to new and evolving requirements. The dual-evaluation process verified the model’s effectiveness, showcasing high precision and recall rates in both controlled and practical environments. © 2013 IEEE.},
	author_keywords = {machine learning (ML); natural language processing (NLP); requirements classification; Software requirements engineering},
	keywords = {Computer aided software engineering; Computer operating systems; Computer software maintenance; Computer software selection and evaluation; Engineering research; Enterprise software; Program processors; Requirements engineering; Search engines; Software packages; Software prototyping; Software quality; Software testing; Utility programs; Verification; Classification methods; Language processing; Machine learning; Machine-learning; Natural language processing; Natural languages; Requirement engineering; Requirements classifications; Similarity analysis; Software requirements; Enterprise resource planning; Software design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Jensen2025,
	author = {Jensen, Victor Vadmand and Alami, Adam and Bruun, Anders Rysholt and Persson, John Stouby},
	title = {Managing expectations towards AI tools for software development: a multiple-case study},
	year = {2025},
	journal = {Information Systems and e-Business Management},
	pages = {},
	doi = {10.1007/s10257-025-00704-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004759633&doi=10.1007%2Fs10257-025-00704-7&partnerID=40&md5=cae8b42277f6eb2301879b35615f873f},
	abstract = {Software development organizations (SDOs) are increasingly working to adopt artificial intelligence (AI) tools, like GitHub Copilot, to meet varied expectations. Nevertheless, we know little about how SDOs manage these expectations. This paper investigates how different SDOs expect AI tools to impact software development, and how these expectations change after a period of considering and evaluating AI tools. We conducted a multiple-case study involving three SDOs. To elicit initial expectations towards AI tools, we collected data using semi-structured interviews and field visits. To assess the persistence of expectations towards AI tools, we collected data from meetings, a debriefing, and retrospectives on AI tools. We found three expectations particular to one SDO; four shared between two SDOs; and six pervasive across all SDOs. Five expectations did not persist after experiential learning with AI tools, due to platform- and SDO-related factors. SDOs must carefully manage their expectations towards AI tools due to the variety and complexity of expectations. Some expectations are niche-specific based on their compatibility with the unique SDOs' people- and structure-related aspects, while others are becoming mainstream for a broader array of SDOs. Recognizing factors that affect the persistence of expectations and how they manifest in the individual SDO will enable SDOs to form their initial expectations and understand how these might change during adoption of AI tools, supporting expectation management. © The Author(s) 2025.},
	author_keywords = {Artificial intelligence; Case studies; Expectation management; Software development; Technology adoption},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Ginde202560955,
	author = {Ginde, Gouri and Ruhe, Guenther and Chad Saunders, W. W.},
	title = {Rethinking Technological Investment and Cost-Benefit: A Software Requirements Dependency Extraction Case Study},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {60955 - 60968},
	doi = {10.1109/ACCESS.2025.3556313},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003088481&doi=10.1109%2FACCESS.2025.3556313&partnerID=40&md5=d03dee9ee63959df32b2c2775ab9bc2c},
	abstract = {Machine Learning (ML) is widely used for different purposes within Software Engineering. It can substantially improve the efficiency and effectiveness of organizations. While various methods and techniques exist, all of them have strengths and weaknesses under varying scenarios and contexts. Thus far, the selection and implementation of ML techniques rely almost exclusively on accuracy criteria. This narrow perspective ignores crucial considerations of anticipated costs of the ML activities versus the projected benefits gained from applying the results. Thus, in this study we introduce a return-on-investment (ROI) perspective to evaluate ML techniques in Software Engineering, offering a novel lens to assess their true value beyond traditional benchmarks. We present findings for an approach that addresses this gap by enhancing the accuracy criterion with return on investment (ROI) considerations. Specifically, we extract dependencies from textual descriptions of software requirements and analyze the performance of two state-of-the-art ML techniques: Random Forest and Bidirectional Encoder Representations from Transformers (BERT), a encoder only Large Language Model. Drawing upon two publicly available data sets, we compare decision-making based on 1) exclusively on accuracy and 2) on ROI analysis to provide decision support for the selection and usage of ML classification methods. As such, our results showed that, 1) chasing model accuracy improvisation through increased annotated data does not generate expected returns in traditional ML methods. 2) For complex ML algorithms, the need for larger annotated dataset investment cost is justified by the higher returns, however, the trade-offs between accuracy and ROI become evident. © 2013 IEEE.},
	author_keywords = {BERT; Machine learning; management decision-making; random forest; requirements dependency extraction; return on investment},
	keywords = {Cost benefit analysis; Bidirectional encoder representation from transformer; Investment benefit; Machine learning techniques; Machine-learning; Management decision-making; Random forests; Requirement dependencies; Requirement dependency extraction; Returns on investment; Software requirements},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Ogrizović2024,
	author = {Ogrizović, Mihajlo and Drašković, Dražen D. and Bojić, Dragan M.},
	title = {Quality assurance strategies for machine learning applications in big data analytics: an overview},
	year = {2024},
	journal = {Journal of Big Data},
	volume = {11},
	number = {1},
	pages = {},
	doi = {10.1186/s40537-024-01028-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208507234&doi=10.1186%2Fs40537-024-01028-y&partnerID=40&md5=f2b7b54f0641598dac1308e01c53b8f6},
	abstract = {Machine learning (ML) models have gained significant attention in a variety of applications, from computer vision to natural language processing, and are almost always based on big data. There are a growing number of applications and products with built-in machine learning models, and this is the area where software engineering, artificial intelligence and data science meet. The requirement for a system to operate in a real-world environment poses many challenges, such as how to design for wrong predictions the model may make; How to assure safety and security despite possible mistakes; which qualities matter beyond a model’s prediction accuracy; How can we identify and measure important quality requirements, including learning and inference latency, scalability, explainability, fairness, privacy, robustness, and safety. It has become crucial to test thoroughly these models to assess their capabilities and potential errors. Existing software testing methods have been adapted and refined to discover faults in machine learning and deep learning models. This paper covers a taxonomy, a methodologically uniform presentation of all presented solutions to the aforementioned issues, as well as conclusions about possible future development trends. The main contributions of this paper are a classification that closely follows the structure of the ML-pipeline, a precisely defined role of each team member within that pipeline, an overview of trends and challenges in the combination of ML and big data analytics, with uses in the domains of industry and education. © The Author(s) 2024.},
	author_keywords = {Big data; Data quality; Integration and system testing; Machine learning; ML pipeline quality; Model quality; Quality assurance; Software in production},
	keywords = {Contrastive Learning; Data accuracy; Integration testing; Data analytics; Data quality; Integration and system testing; Machine learning models; Machine learning pipeline quality; Machine-learning; Modeling quality; Software in production; System testing; Adversarial machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access; Gold Open Access}
}

@ARTICLE{Poddar2024,
	author = {Poddar, Mukund and Marwaha, Jayson S. and Yuan, William and Romero-Brufau, Santiago and Brat, Gabriel A.},
	title = {An operational guide to translational clinical machine learning in academic medical centers},
	year = {2024},
	journal = {npj Digital Medicine},
	volume = {7},
	number = {1},
	pages = {},
	doi = {10.1038/s41746-024-01094-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193702911&doi=10.1038%2Fs41746-024-01094-9&partnerID=40&md5=94a7e7e697fd79a5f6fb2863bf1964e1},
	abstract = {Few published data science tools are ever translated from academia to real-world clinical settings for which they were intended. One dimension of this problem is the software engineering task of turning published academic projects into tools that are usable at the bedside. Given the complexity of the data ecosystem in large health systems, this task often represents a significant barrier to the real-world deployment of data science tools for prospective piloting and evaluation. Many information technology companies have created Machine Learning Operations (MLOps) teams to help with such tasks at scale, but the low penetration of home-grown data science tools in regular clinical practice precludes the formation of such teams in healthcare organizations. Based on experiences deploying data science tools at two large academic medical centers (Beth Israel Deaconess Medical Center, Boston, MA; Mayo Clinic, Rochester, MN), we propose a strategy to facilitate this transition from academic product to operational tool, defining the responsibilities of the principal investigator, data scientist, machine learning engineer, health system IT administrator, and clinician end-user throughout the process. We first enumerate the technical resources and stakeholders needed to prepare for model deployment. We then propose an approach to planning how the final product will work from data extraction and analysis to visualization of model outputs. Finally, we describe how the team should execute on this plan. We hope to guide health systems aiming to deploy minimum viable data science tools and realize their value in clinical practice. © The Author(s) 2024.},
	keywords = {Data Science; Data visualization; Hospitals; Software engineering; Academic medical centers; Academic program; Clinical practices; Clinical settings; Engineering tasks; Health systems; Machine-learning; One dimension; Real-world; Science tools; Machine learning; academia; administrative personnel; article; clinical practice; clinician; data extraction; digital technology; health care organization; human; information technology; machine learning; university hospital},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Jiang2024,
	author = {Jiang, Wenxin and Banna, Vishnu and Vivek, Naveen and Goel, Abhinav and Synovic, Nicholas M. and Thiruvathukal, George K. and Davis, James C.},
	title = {Challenges and practices of deep learning model reengineering: A case study on computer vision},
	year = {2024},
	journal = {Empirical Software Engineering},
	volume = {29},
	number = {6},
	pages = {},
	doi = {10.1007/s10664-024-10521-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201627563&doi=10.1007%2Fs10664-024-10521-0&partnerID=40&md5=321cbc5a000abbd67fb7ce2cb08d5549},
	abstract = {Context: Many engineering organizations are reimplementing and extending deep neural networks from the research community. We describe this process as deep learning model reengineering. Deep learning model reengineering — reusing, replicating, adapting, and enhancing state-of-the-art deep learning approaches — is challenging for reasons including under-documented reference models, changing requirements, and the cost of implementation and testing. Objective: Prior work has characterized the challenges of deep learning model development, but as yet we know little about the deep learning model reengineering process and its common challenges. Prior work has examined DL systems from a “product” view, examining defects from projects regardless of the engineers’ purpose. Our study is focused on reengineering activities from a “process” view, and focuses on engineers specifically engaged in the reengineering process. Method: Our goal is to understand the characteristics and challenges of deep learning model reengineering. We conducted a mixed-methods case study of this phenomenon, focusing on the context of computer vision. Our results draw from two data sources: defects reported in open-source reeengineering projects, and interviews conducted with practitioners and the leaders of a reengineering team. From the defect data source, we analyzed 348 defects from 27 open-source deep learning projects. Meanwhile, our reengineering team replicated 7 deep learning models over two years; we interviewed 2 open-source contributors, 4 practitioners, and 6 reengineering team leaders to understand their experiences. Results: Our results describe how deep learning-based computer vision techniques are reengineered, quantitatively analyze the distribution of defects in this process, and qualitatively discuss challenges and practices. We found that most defects (58%) are reported by re-users, and that reproducibility-related defects tend to be discovered during training (68% of them are). Our analysis shows that most environment defects (88%) are interface defects, and most environment defects (46%) are caused by API defects. We found that training defects have diverse symptoms and root causes. We identified four main challenges in the DL reengineering process: model operationalization, performance debugging, portability of DL operations, and customized data pipeline. Integrating our quantitative and qualitative data, we propose a novel reengineering workflow. Conclusions: Our findings inform several conclusion, including: standardizing model reengineering practices, developing validation tools to support model reengineering, automated support beyond manual model reengineering, and measuring additional unknown aspects of model reengineering. © The Author(s) 2024.},
	author_keywords = {Bug study; Case study; Computer vision; Deep learning; Deep neural networks; Empirical software engineering; Failure analysis; Machine learning; Mixed methods; Software reliability},
	keywords = {Computer debugging; Deep neural networks; Error correction; Failure analysis; Open source software; Program debugging; Reengineering; Reliability analysis; Bug study; Case-studies; Deep learning; Empirical Software Engineering; Learning models; Machine-learning; Mixed method; Neural-networks; Open-source; Software-Reliability; Software reliability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Eramo2024,
	author = {Eramo, Romina and Said, Bilal and Oriol, Marc and Bruneliere, Hugo and Morales, Sergio},
	title = {An architecture for model-based and intelligent automation in DevOps},
	year = {2024},
	journal = {Journal of Systems and Software},
	volume = {217},
	pages = {},
	doi = {10.1016/j.jss.2024.112180},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200822665&doi=10.1016%2Fj.jss.2024.112180&partnerID=40&md5=b2804db641189dc5fea6975035bf44db},
	abstract = {The increasing complexity of modern systems poses numerous challenges at all stages of system development and operation. Continuous software and system engineering processes, e.g., DevOps, are increasingly adopted and spread across organizations. In parallel, many leading companies have begun to apply artificial intelligence (AI) principles and techniques, including Machine Learning (ML), to improve their products. However, there is no holistic approach that can support and enhance the growing challenges of DevOps. In this paper, we propose a software architecture that provides the foundations of a model-based framework for the development of AI-augmented solutions incorporating methods and tools for continuous software and system engineering and validation. The key characteristic of the proposed architecture is that it allows leveraging the advantages of both AI/ML and Model Driven Engineering (MDE) approaches and techniques in a DevOps context. This architecture has been designed, developed and applied in the context of the European large collaborative project named AIDOaRt. In this paper, we also report on the practical evaluation of this architecture. This evaluation is based on a significant set of technical solutions implemented and applied in the context of different real industrial case studies coming from the AIDOaRt project. Moreover, we analyze the collected results and discuss them according to both architectural and technical challenges we intend to tackle with the proposed architecture. © 2024 The Authors},
	author_keywords = {Artificial Intelligence; Continuous software engineering; DevOps; Mode-driven engineering; Software architecture},
	keywords = {Artificial intelligence; Continuous software engineerings; Development and operations; Intelligent automation; Machine-learning; Mode-driven engineering; Model-based OPC; Proposed architectures; Software and systems engineerings; System development; Systems operation; Software architecture},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Maier2024,
	author = {Maier, Robert and Schlattl, Andreas and Guess, Thomas and Mottok, Jǘrgen Horst},
	title = {CausalOps — Towards an industrial lifecycle for causal probabilistic graphical models},
	year = {2024},
	journal = {Information and Software Technology},
	volume = {174},
	pages = {},
	doi = {10.1016/j.infsof.2024.107520},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197423566&doi=10.1016%2Fj.infsof.2024.107520&partnerID=40&md5=eced47ca9ee7751b4a7aa836636ce0e8},
	abstract = {Context: Causal probabilistic graph-based models have gained widespread utility, enabling the modeling of cause-and-effect relationships across diverse domains. With their rising adoption in new areas, such as safety analysis of complex systems, software engineering, and machine learning, the need for an integrated lifecycle framework akin to DevOps and MLOps has emerged. Currently, such a reference for organizations interested in employing causal engineering is missing. This lack of guidance hinders the incorporation and maturation of causal methods in the context of real-life applications. Objective: This work contextualizes causal model usage across different stages and stakeholders and outlines a holistic view of creating and maintaining them within the process landscape of an organization. Methods: A novel lifecycle framework for causal model development and application called CausalOps is proposed. By defining key entities, dependencies, and intermediate artifacts generated during causal engineering, a consistent vocabulary and workflow model to guide organizations in adopting causal methods are established. Results: Based on the early adoption of the discussed methodology to a real-life problem within the automotive domain, an experience report underlining the practicability and challenges of the proposed approach is discussed. Conclusion: It is concluded that besides current technical advancements in various aspects of causal engineering, an overarching lifecycle framework that integrates these methods into organizational practices is missing. Although diverse skills from adjacent disciplines are widely available, guidance on how to transfer these assets into causality-driven practices still need to be addressed in the published literature. CausalOps’ aim is to set a baseline for the adoption of causal methods in practical applications within interested organizations and the causality community. © 2024 The Author(s)},
	author_keywords = {Causal engineering; Causal graphical models; MLOps; Model lifecycle},
	keywords = {Life cycle; Software engineering; Causal engineering; Causal graphical model; Causal modeling; Cause-and-effect relationships; Graph-based models; GraphicaL model; MLOp; Model lifecycle; Probabilistic graphical models; Probabilistic graphs; Graphic methods},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Mohammad2024555,
	author = {Mohammad, Abdulghafour and Chirchir, Brian},
	title = {Challenges of Integrating Artificial Intelligence in Software Project Planning: A Systematic Literature Review},
	year = {2024},
	journal = {Digital},
	volume = {4},
	number = {3},
	pages = {555 - 571},
	doi = {10.3390/digital4030028},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205075376&doi=10.3390%2Fdigital4030028&partnerID=40&md5=da7d66fb43077e05694cc76bbc67d144},
	abstract = {Artificial intelligence (AI) has helped enhance the management of software development projects through automation, improving efficiency and enabling project professionals to focus on strategic aspects. Despite its advantages, applying AI in software development project management still faces several challenges. Thus, this study investigates key obstacles to applying artificial intelligence in project management, specifically in the project planning phase. This research systematically reviews the existing literature. The review comprises scientific articles published from 2019 to 2024 and, from the inspected records, 17 papers were analyzed in full-text form. In this review, 10 key barriers were reported and categorized based on the Technology–Organization–Environment (TOE) framework. This review showed that eleven articles reported technological challenges, twelve articles identified organizational challenges, and six articles reported environmental challenges. In addition, this review found that there was relatively little interest in the literature on environmental challenges, compared to organizational and technological barriers. © 2024 by the authors.},
	author_keywords = {artificial intelligence; information technology; machine learning; project management; project planning; software development projects; TOE framework},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Schuszter2024,
	author = {Schuszter, Ioan Cristian and Marius, Cioca},
	title = {Increasing the Reliability of Software Systems Using a Large-Language-Model-Based Solution for Onboarding},
	year = {2024},
	journal = {Inventions},
	volume = {9},
	number = {4},
	pages = {},
	doi = {10.3390/inventions9040079},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202471122&doi=10.3390%2Finventions9040079&partnerID=40&md5=2638d7636439385148176ba8d70ae61a},
	abstract = {Software systems are often maintained by a group of experienced software developers in order to ensure that faults that may bring the system down are less likely. Large turnover in organizations such as CERN makes it important to think of ways of onboarding newcomers on a technical project rapidly. This paper focuses on optimizing the way that people get up-to-speed on the business logic and technologies used on the project by using a knowledge-imbued large language model that is enhanced using domain-specific knowledge from the group or team’s internal documentation. The novelty of this approach is the gathering of all of these different open-source methods for developing a chatbot and using it in an industrial use-case. © 2024 by the authors.},
	author_keywords = {GPT; large language models; LLM; onboarding; software engineering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access}
}

@ARTICLE{Russo2024,
	author = {Russo, Daniel},
	title = {Navigating the Complexity of Generative AI Adoption in Software Engineering},
	year = {2024},
	journal = {ACM Transactions on Software Engineering and Methodology},
	volume = {33},
	number = {5},
	pages = {},
	doi = {10.1145/3652154},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195509905&doi=10.1145%2F3652154&partnerID=40&md5=3f9dd19eded63f29fa76b8dd70eda135},
	abstract = {This article explores the adoption of Generative Artificial Intelligence (AI) tools within the domain of software engineering, focusing on the influencing factors at the individual, technological, and social levels. We applied a convergent mixed-methods approach to offer a comprehensive understanding of AI adoption dynamics. We initially conducted a questionnaire survey with 100 software engineers, drawing upon the Technology Acceptance Model, the Diffusion of Innovation Theory, and the Social Cognitive Theory as guiding theoretical frameworks. Employing the Gioia methodology, we derived a theoretical model of AI adoption in software engineering: the Human-AI Collaboration and Adaptation Framework. This model was then validated using Partial Least Squares-Structural Equation Modeling based on data from 183 software engineers. Findings indicate that at this early stage of AI integration, the compatibility of AI tools within existing development workflows predominantly drives their adoption, challenging conventional technology acceptance theories. The impact of perceived usefulness, social factors, and personal innovativeness seems less pronounced than expected. The study provides crucial insights for future AI tool design and offers a framework for developing effective organizational implementation strategies. © 2024 Copyright held by the owner/author(s).},
	author_keywords = {empirical software engineering; Generative AI; large language models; technology adaption},
	keywords = {Least squares approximations; Social aspects; Software engineering; Artificial intelligence tools; Diffusions of innovation theories; Empirical Software Engineering; Generative artificial intelligence; Language model; Large language model; Mixed method; Questionnaire surveys; Technology acceptance model; Technology adaption; Digital storage},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 95; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Wei2024187,
	author = {Wei, Hanyu and Xu, Wen and Kang, Byeong and Eisner, Rowan and Muleke, Albert and Rodriguez, Daniel and DeVoil, Peter G. and Sadras, Víctor Oscar and Monjardino, Marta and Harrison, M. T.},
	title = {Irrigation with Artificial Intelligence: Problems, Premises, Promises},
	year = {2024},
	journal = {Human-Centric Intelligent Systems},
	volume = {4},
	number = {2},
	pages = {187 - 205},
	doi = {10.1007/s44230-024-00072-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018882898&doi=10.1007%2Fs44230-024-00072-4&partnerID=40&md5=37c9cbd5ed373bcc80192d4bf524f65f},
	abstract = {Protagonists allege that artificial intelligence (AI) is revolutionising contemporaneous mindscapes. Here, we authoritatively review the status quo of AI and machine learning application in irrigated agriculture, evaluating the potential of, and challenges associated with, a wide range of existential AI approaches. We contend that aspiring developers of AI irrigation systems may benefit from human-centred AI, a nascent algorithm that captures diverse end-user views, behaviours and actions, potentially facilitating refinement of proposed systems through iterative stakeholder feedback. AI-guided human–machine collaboration can streamline integration of user needs, allowing customisation towards situational farm management adaptation. Presentation of big data in intuitive, legible and actionable forms for specialists and laypeople also urgently requires attention: here, AI-explainable interpretability may help harness human expertise, enabling end-users to contribute their experience within an AI pipeline for bespoke outputs. Transfer learning holds promise in contextualising place-based AI to agroecological regions, production systems or enterprise mixes, even with limited data inputs. We find that the rate of AI scientific and software development in recent times has outpaced the evolution of adequate legal and institutional regulations, and often social, moral and ethical license to operate, revealing consumer issues associated with data ownership, legitimacy and trust. We opine that AI has great potential to elicit sustainable outcomes in food security, social innovation and environmental stewardship, albeit such potential is more likely to be realised through concurrent development of appropriate ethical, moral and legal dimensions. © The Author(s) 2024.},
	author_keywords = {AI; Industry 5.0 human centric; Irrigation management; Machine Learning; Smart irrigation},
	keywords = {Agricultural machinery; Behavioral research; Environmental regulations; Ethical aspects; Irrigation; Software design; Transfer learning; Artificial intelligence learning; Human-centric; Industry 5.0 human centric; Irrigated agriculture; Irrigation management; Irrigation systems; Machine learning applications; Machine-learning; Smart irrigation; Status quo; Learning systems},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 39; All Open Access; Gold Open Access}
}

@ARTICLE{Olivares2024,
	author = {Olivares, Rodrigo and Noel, Rene Alejandro and Guzmán, Sebastián M. and Miranda, Diego and Munoz-Soto, Roberto},
	title = {Intelligent Learning-Based Methods for Determining the Ideal Team Size in Agile Practices},
	year = {2024},
	journal = {Biomimetics},
	volume = {9},
	number = {5},
	pages = {},
	doi = {10.3390/biomimetics9050292},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194286016&doi=10.3390%2Fbiomimetics9050292&partnerID=40&md5=2d4fe87c6946d6bf97a8b1736b77a266},
	abstract = {One of the significant challenges in scaling agile software development is organizing software development teams to ensure effective communication among members while equipping them with the capabilities to deliver business value independently. A formal approach to address this challenge involves modeling it as an optimization problem: given a professional staff, how can they be organized to optimize the number of communication channels, considering both intra-team and inter-team channels? In this article, we propose applying a set of bio-inspired algorithms to solve this problem. We introduce an enhancement that incorporates ensemble learning into the resolution process to achieve nearly optimal results. Ensemble learning integrates multiple machine-learning strategies with diverse characteristics to boost optimizer performance. Furthermore, the studied metaheuristics offer an excellent opportunity to explore their linear convergence, contingent on the exploration and exploitation phases. The results produce more precise definitions for team sizes, aligning with industry standards. Our approach demonstrates superior performance compared to the traditional versions of these algorithms. © 2024 by the authors.},
	author_keywords = {agile practices; ensemble learning; machine learning; metaheuristics; software engineering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Naveed2024,
	author = {Naveed, Hira and Arora, Chetan and Khalajzadeh, Hourieh and Grundy, John C. and Haggag, Omar},
	title = {Model driven engineering for machine learning components: A systematic literature review},
	year = {2024},
	journal = {Information and Software Technology},
	volume = {169},
	pages = {},
	doi = {10.1016/j.infsof.2024.107423},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185399331&doi=10.1016%2Fj.infsof.2024.107423&partnerID=40&md5=0a9134d6c45f1eb8b34fc2deb07cf4f4},
	abstract = {Context: Machine Learning (ML) has become widely adopted as a component in many modern software applications. Due to the large volumes of data available, organizations want to increasingly leverage their data to extract meaningful insights and enhance business profitability. ML components enable predictive capabilities, anomaly detection, recommendation, accurate image and text processing, and informed decision-making. However, developing systems with ML components is not trivial; it requires time, effort, knowledge, and expertise in ML, data processing, and software engineering. There have been several studies on the use of model-driven engineering (MDE) techniques to address these challenges when developing traditional software and cyber–physical systems. Recently, there has been a growing interest in applying MDE for systems with ML components. Objective: The goal of this study is to further explore the promising intersection of MDE with ML (MDE4ML) through a systematic literature review (SLR). Through this SLR, we wanted to analyze existing studies, including their motivations, MDE solutions, evaluation techniques, key benefits and limitations. Method: Our SLR is conducted following the well-established guidelines by Kitchenham. We started by devising a protocol and systematically searching seven databases, which resulted in 3934 papers. After iterative filtering, we selected 46 highly relevant primary studies for data extraction, synthesis, and reporting. Results: We analyzed selected studies with respect to several areas of interest and identified the following: (1) the key motivations behind using MDE4ML; (2) a variety of MDE solutions applied, such as modeling languages, model transformations, tool support, targeted ML aspects, contributions and more; (3) the evaluation techniques and metrics used; and (4) the limitations and directions for future work. We also discuss the gaps in existing literature and provide recommendations for future research. Conclusion: This SLR highlights current trends, gaps and future research directions in the field of MDE4ML, benefiting both researchers and practitioners. © 2024 The Author(s)},
	author_keywords = {Artificial intelligence; Machine learning; Model driven engineering; Software engineering; Systematic literature review},
	keywords = {Anomaly detection; Application programs; Data handling; Data mining; Decision making; Engineering research; Iterative methods; Modeling languages; Motivation; Statistics; Text processing; Engineering solutions; Images processing; Large volumes; Machine-learning; Model-driven Engineering; Predictive capabilities; Software applications; Systematic literature review; Text-processing; Machine learning},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 24; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Skuridin2024,
	author = {Skuridin, Alexander and Wynn, Martin George},
	title = {Chatbot Design and Implementation: Towards an Operational Model for Chatbots},
	year = {2024},
	journal = {Information (Switzerland)},
	volume = {15},
	number = {4},
	pages = {},
	doi = {10.3390/info15040226},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191612592&doi=10.3390%2Finfo15040226&partnerID=40&md5=d097b08805d9a32413dcb9f4c5d05cb6},
	abstract = {The recent past has witnessed a growing interest in technologies for creating chatbots. Advances in Large Language Models for natural language processing are underpinning rapid progress in chatbot development, and experts predict revolutionary changes in the labour market as many manual tasks are replaced by virtual assistants in a range of business functions. As the new technology becomes more accessible and advanced, more companies are exploring the possibilities of implementing virtual assistants to automate routine tasks and improve service. This article reports on qualitative inductive research undertaken within a chatbot development team operating in a major international enterprise. The findings identify critical success factors for chatbot projects, and a model is developed and validated to support the planning and implementation of chatbot projects. The presented model can serve as an exemplary guide for researchers and practitioners working in this field. It is flexible and applicable in a wide range of business contexts, linking strategic business goals with execution steps. It is particularly applicable for teams with no experience in chatbot implementation, reducing uncertainty and managing decisions and risks throughout the project lifecycle, thereby increasing the likelihood of project success. © 2024 by the authors.},
	author_keywords = {agile; artificial intelligence; chatbots; customer service; digital transformation; large language models; machine learning; minimum viable product; project management; TOE framework},
	keywords = {Computational linguistics; E-learning; Employment; Life cycle; Machine learning; Natural language processing systems; Product design; Agile; Chatbots; Customer-service; Digital transformation; Language model; Large language model; Machine-learning; Minimum viable product; TOE framework; Virtual assistants; Project management},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access; Gold Open Access}
}

@ARTICLE{Garikapati2024,
	author = {Garikapati, Divya and Shetiya, Sneha Sudhir},
	title = {Autonomous Vehicles: Evolution of Artificial Intelligence and the Current Industry Landscape},
	year = {2024},
	journal = {Big Data and Cognitive Computing},
	volume = {8},
	number = {4},
	pages = {},
	doi = {10.3390/bdcc8040042},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191455647&doi=10.3390%2Fbdcc8040042&partnerID=40&md5=985ef91d1349e8bcc408c8dc9de3995c},
	abstract = {The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. Central to this evolution is the integration of artificial intelligence (AI), propelling vehicles into realms of unprecedented autonomy. Commencing with an overview of the current industry landscape with respect to Operational Design Domain (ODD), this paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles. It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing various challenges such as safety, security, privacy, and ethical considerations in AI-driven software development for autonomous vehicles. The study presents statistical insights into the usage and types of AI algorithms over the years, showcasing the evolving research landscape within the automotive industry. Furthermore, the paper highlights the pivotal role of parameters in refining algorithms for both trucks and cars, facilitating vehicles to adapt, learn, and improve performance over time. It concludes by outlining different levels of autonomy, elucidating the nuanced usage of AI algorithms, and discussing the automation of key tasks and the software package size at each level. Overall, the paper provides a comprehensive analysis of the current industry landscape, focusing on several critical aspects. © 2024 by the authors.},
	author_keywords = {artificial intelligence (AI); autonomous vehicles (AVs); autonomy levels; connected and automated vehicles (CAVs); deep learning (DL); deep neural networks (DNNs); emerging trends; ethics; generative AI (GenAI); in-vehicle AI assistant; internet of things (IoT); Machine learning (ML); natural language processing (NLP); operational design domain (ODD); safety; security; software-defined vehicles (SDVs); trucks vs. cars},
	keywords = {Automobiles; Automotive industry; Autonomous vehicles; Decision making; Deep neural networks; Learning algorithms; Learning systems; Life cycle; Natural language processing systems; Software design; Trucks; Artificial intelligence; Automated vehicles; Autonomous vehicle; Autonomous Vehicles; Autonomy levels; Connected and automated vehicle; Deep learning; Deep neural network; Design domains; Emerging trends; Generative artificial intelligence; In-vehicle artificial intelligence assistant; Internet of thing; Language processing; Machine learning; Machine-learning; Natural language processing; Natural languages; Operational design; Operational design domain; Security; Software-defined vehicle; Truck vs.; Accident prevention},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 106; All Open Access; Gold Open Access}
}

@ARTICLE{Abdullahi2024,
	author = {Abdullahi, Ibrahim and Longo, Stefano and Samie, Mohammad},
	title = {Towards a Distributed Digital Twin Framework for Predictive Maintenance in Industrial Internet of Things (IIoT)},
	year = {2024},
	journal = {Sensors},
	volume = {24},
	number = {8},
	pages = {},
	doi = {10.3390/s24082663},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191410655&doi=10.3390%2Fs24082663&partnerID=40&md5=9d217ddeae91f9195d6ceb1042e898c2},
	abstract = {This study uses a wind turbine case study as a subdomain of Industrial Internet of Things (IIoT) to showcase an architecture for implementing a distributed digital twin in which all important aspects of a predictive maintenance solution in a DT use a fog computing paradigm, and the typical predictive maintenance DT is improved to offer better asset utilization and management through real-time condition monitoring, predictive analytics, and health management of selected components of wind turbines in a wind farm. Digital twin (DT) is a technology that sits at the intersection of Internet of Things, Cloud Computing, and Software Engineering to provide a suitable tool for replicating physical objects in the digital space. This can facilitate the implementation of asset management in manufacturing systems through predictive maintenance solutions leveraged by machine learning (ML). With DTs, a solution architecture can easily use data and software to implement asset management solutions such as condition monitoring and predictive maintenance using acquired sensor data from physical objects and computing capabilities in the digital space. While DT offers a good solution, it is an emerging technology that could be improved with better standards, architectural framework, and implementation methodologies. Researchers in both academia and industry have showcased DT implementations with different levels of success. However, DTs remain limited in standards and architectures that offer efficient predictive maintenance solutions with real-time sensor data and intelligent DT capabilities. An appropriate feedback mechanism is also needed to improve asset management operations. © 2024 by the authors.},
	author_keywords = {digital twins; fog computing; machine learning; predictive maintenance; wind turbines},
	keywords = {Asset management; Condition based maintenance; Condition monitoring; Data Analytics; E-learning; Engineering education; Fog computing; Internet of things; Machine learning; Predictive analytics; Software engineering; Wind power; Asset utilization; Assets management; Case-studies; Computing paradigm; Digital space; Machine-learning; Physical objects; Predictive maintenance; Sensors data; Subdomain; Wind turbines; article; cloud computing; digital twin; feedback system; health care management; human; internet of things; machine learning; sensor; software; wind; wind farm},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 37; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Duda2024203,
	author = {Duda, Sebastian and Hofmann, Peter and Urbach, Nils and Völter, Fabiane and Zwickel, Amelie},
	title = {The Impact of Resource Allocation on the Machine Learning Lifecycle: Bridging the Gap between Software Engineering and Management},
	year = {2024},
	journal = {Business and Information Systems Engineering},
	volume = {66},
	number = {2},
	pages = {203 - 219},
	doi = {10.1007/s12599-023-00842-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176433267&doi=10.1007%2Fs12599-023-00842-7&partnerID=40&md5=3100579bc8a9c873753be09b1822167a},
	abstract = {An organization’s ability to develop Machine Learning (ML) applications depends on its available resource base. Without awareness and understanding of all relevant resources as well as their impact on the ML lifecycle, we risk inefficient allocations as well as missing monopolization tendencies. To counteract these risks, the study develops a framework that interweaves the relevant resources with the procedural and technical dependencies within the ML lifecycle. To rigorously develop and evaluate this framework the paper follows the Design Science Research paradigm and builds on a literature review and an interview study. In doing so, it bridges the gap between the software engineering and management perspective to advance the ML management discourse. The results extend the literature by introducing not yet discussed but relevant resources, describing six direct and indirect effects of resources on the ML lifecycle, and revealing the resources’ contextual properties. Furthermore, the framework is useful in practice to support organizational decision-making and contextualize monopolization tendencies. © The Author(s) 2023.},
	author_keywords = {Artificial intelligence; Design science research; Machine learning lifecycle; ML management; Resource-based view},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Kanbach20241189,
	author = {Kanbach, Dominik K. and Heiduk, Louisa and Blueher, Georg and Schreiter, Maximilian and Lahmann, Alexander D.F.},
	title = {The GenAI is out of the bottle: generative artificial intelligence from a business model innovation perspective},
	year = {2024},
	journal = {Review of Managerial Science},
	volume = {18},
	number = {4},
	pages = {1189 - 1220},
	doi = {10.1007/s11846-023-00696-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171188382&doi=10.1007%2Fs11846-023-00696-z&partnerID=40&md5=ac9d2cd87288fbff1b71eb97c400dcf7},
	abstract = {The introduction of ChatGPT in November 2022 by OpenAI has stimulated substantial discourse on the implementation of artificial intelligence (AI) in various domains such as academia, business, and society at large. Although AI has been utilized in numerous areas for several years, the emergence of generative AI (GAI) applications such as ChatGPT, Jasper, or DALL-E are considered a breakthrough for the acceleration of AI technology due to their ease of use, intuitive interface, and performance. With GAI, it is possible to create a variety of content such as texts, images, audio, code, and even videos. This creates a variety of implications for businesses requiring a deeper examination, including an influence on business model innovation (BMI). Therefore, this study provides a BMI perspective on GAI with two primary contributions: (1) The development of six comprehensive propositions outlining the impact of GAI on businesses, and (2) the discussion of three industry examples, specifically software engineering, healthcare, and financial services. This study employs a qualitative content analysis using a scoping review methodology, drawing from a wide-ranging sample of 513 data points. These include academic publications, company reports, and public information such as press releases, news articles, interviews, and podcasts. The study thus contributes to the growing academic discourse in management research concerning AI's potential impact and offers practical insights into how to utilize this technology to develop new or improve existing business models. © The Author(s) 2023.},
	author_keywords = {Artificial intelligence; Business model innovation; ChatGPT; Generative AI; Large language models; M10; M13; M15; M19},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 257; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Schrijver2024,
	author = {Schrijver, Gilian and Sarmah, Dipti Kapoor and el-Hajj, Mohammed Ibrahim},
	title = {Automobile insurance fraud detection using data mining: A systematic literature review},
	year = {2024},
	journal = {Intelligent Systems with Applications},
	volume = {21},
	pages = {},
	doi = {10.1016/j.iswa.2024.200340},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185202250&doi=10.1016%2Fj.iswa.2024.200340&partnerID=40&md5=c5388ff962cbaab38b599298d4c72943},
	abstract = {Insurance is a pivotal element in modern society, but insurers face a persistent challenge from fraudulent behaviour performed by policyholders. This behaviour could be detrimental to both insurance companies and their honest customers, but the intricate nature of insurance fraud severely complicates its efficient, automated detection. This study surveys fifty recent publications on automobile insurance fraud detection, published between January 2019 and March 2023, and presents both the most commonly used data sets and methods for resampling and detection, as well as interesting, novel approaches. The study adopts the highly-cited Systematic Literature Review (SLR) methodology for software engineering research proposed by Kitchenham and Charters and collected studies from four online databases. The findings indicate limited public availability of automobile insurance fraud data sets. In terms of detection methods, the prevailing approach involves supervised machine learning methods that utilise structured, intrinsic features of claims or policies and that lack consideration of an example-dependent cost of misclassification. However, alternative techniques are also explored, including the use of graph-based methods, unstructured textual data, and cost-sensitive classifiers. The most common resampling approach was found to be oversampling. This SLR has identified commonly used methods in recent automobile insurance fraud detection research, and interesting directions for future research. It adds value over a related review by also including studies published from 2021 onward, and by detailing the used methodology. Limitations of this SLR include its restriction to a small number of considered publication years and limited validation of choices made during the process. © 2024 The Author(s)},
	author_keywords = {Automobile insurance; Data mining; Fraud detection; Insurance fraud; Machine learning; Systematic literature review},
	keywords = {Automobiles; Crime; Data mining; Graphic methods; Learning systems; Publishing; Supervised learning; Automated detection; Automobile insurance; Data set; Fraud detection; Insurance companies; Insurance frauds; Machine-learning; Resampling; Software engineering research; Systematic literature review; Insurance},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access; Gold Open Access}
}

@ARTICLE{Ferrara2024,
	author = {Ferrara, Carmine and Sellitto, Giulia and Ferrucci, Filomena and Palomba, Fabio and De Lucia, Andrea},
	title = {Fairness-aware machine learning engineering: how far are we?},
	year = {2024},
	journal = {Empirical Software Engineering},
	volume = {29},
	number = {1},
	pages = {},
	doi = {10.1007/s10664-023-10402-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178151949&doi=10.1007%2Fs10664-023-10402-y&partnerID=40&md5=930e401228771fd910fd906df4a1056a},
	abstract = {Machine learning is part of the daily life of people and companies worldwide. Unfortunately, bias in machine learning algorithms risks unfairly influencing the decision-making process and reiterating possible discrimination. While the interest of the software engineering community in software fairness is rapidly increasing, there is still a lack of understanding of various aspects connected to fair machine learning engineering, i.e., the software engineering process involved in developing fairness-critical machine learning systems. Questions connected to the practitioners’ awareness and maturity about fairness, the skills required to deal with the matter, and the best development phase(s) where fairness should be faced more are just some examples of the knowledge gaps currently open. In this paper, we provide insights into how fairness is perceived and managed in practice, to shed light on the instruments and approaches that practitioners might employ to properly handle fairness. We conducted a survey with 117 professionals who shared their knowledge and experience highlighting the relevance of fairness in practice, and the skills and tools required to handle it. The key results of our study show that fairness is still considered a second-class quality aspect in the development of artificial intelligence systems. The building of specific methods and development environments, other than automated validation tools, might help developers to treat fairness throughout the software lifecycle and revert this trend. © 2023, The Author(s).},
	author_keywords = {Empirical software engineering; Machine learning; Practitioners’ perspective; Software fairness; Survey study},
	keywords = {Behavioral research; Decision making; Engineering education; Learning algorithms; Life cycle; Software engineering; Daily lives; Decision-making process; Empirical Software Engineering; Engineering community; Machine learning algorithms; Machine-learning; Practitioner’ perspective; Software engineering process; Software fairness; Survey study; Machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 34; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Hong2024193717,
	author = {Hong, Hyun-taek and Wang, Daesung and Kim, Sejin and Sung, Hoon and Park, Chang-won and Park, Ho-hyun and Lee, Chan Gun},
	title = {Implementing and Evaluating Automated Bug Triage in Industrial Projects},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {193717 - 193730},
	doi = {10.1109/ACCESS.2024.3519418},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212639875&doi=10.1109%2FACCESS.2024.3519418&partnerID=40&md5=b1f9a969a648aba6e9488155574a7f07},
	abstract = {Resolving bugs on time is essential for software development and is critical in industrial projects because it directly affects businesses. Automatic bug triage has been investigated to increase software productivity, and research has become more active as machine learning techniques have improved. However, most research has focused on open-source projects, whereas studies on industrial projects remain limited. The research gap in previous studies is that the research has directly triaged developers, reducing accuracy in industrial projects where organizational structures frequently change. Moreover, developers often move between teams, making this approach less effective. The research in this article applies automatic bug triage to industrial projects by adapting the characteristics of industrial projects. Addressing these limitations establishes an approach that is better suited to industrial projects and has enhanced accuracy. Based on this background, we propose a novel approach to triage developers associated with component-based developer lists. Each component has an associated list of developers, and the triage results of the model are limited to selecting from among the listed developers, enhancing triage accuracy. The proposed approach reflects the characteristics of industrial projects and addresses the dynamic workload adjustments in a component-based team structure. The proposed approach improves the results by 6.2 percentage points over human triage for top-1 results, suggesting that this approach could be further expanded for broader application in industrial contexts. Future research should focus on refining the proposed method with real-time feedback and experiment with a broader dataset for generalizability and scalability. © 2013 IEEE.},
	author_keywords = {Bug triage; component; industrial project; pretrained language model; software engineering},
	keywords = {Bug triage; Component; Component based; Industrial programs; Language model; Machine learning techniques; Open source projects; Pretrained language model; Research gaps; Software productivity},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Millam2024,
	author = {Millam, Andrew and Bakke, Christine},
	title = {CODING WITH AI AS AN ASSISTANT: CAN AI GENERATE CONCISE COMPUTER CODE?},
	year = {2024},
	journal = {Journal of Information Technology Education: Innovations in Practice},
	volume = {23},
	pages = {},
	doi = {10.28945/5362},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203665181&doi=10.28945%2F5362&partnerID=40&md5=4738d9e141e51cd713b476de93fea8ef},
	abstract = {Aim/Purpose This paper is part of a multi-case study that aims to test whether generative AI makes an effective coding assistant. Particularly, this work evaluates the ability of two AI chatbots (ChatGPT and Bing Chat) to generate concise computer code, considers ethical issues related to generative AI, and offers suggestions for how to improve the technology. Background Since the release of ChatGPT in 2022, generative artificial intelligence has steadily gained wide use in software development. However, there is conflicting information on the extent to which AI helps developers be more productive in the long term. Also, whether using generated code violates copyright restrictions is a matter of debate. Methodology ChatGPT and Bing Chat were asked the same question, their responses were recorded, and the percentage of each chatbot’s code that was extraneous was calculated. Also examined were qualitative factors, such as how often the generated code required modifications before it would run. Contribution This paper adds to the limited body of research on how effective generative AI is at aiding software developers and how to practically address its shortcomings. Findings Results of AI testing observed that 0.7% of lines and 1.4% of characters in ChatGPT’s responses were extraneous, while 0.7% of lines and 1.1% of characters in Bing Chat’s responses were extraneous. This was well below the 2% threshold, meaning both chatbots can generate concise code. However, code from both chatbots frequently had to be modified before it would work; ChatGPT’s code needed major modifications 30% of the time and minor ones 50% of the time, while Bing Chat’s code needed major modifications 10% of the time and minor ones 70% of the time. Recommendations Companies building generative AI solutions are encouraged to use this study’s for Practitioners findings to improve their models, specifically by decreasing error rates, adding more training data for programming languages with less public documentation, and implementing a mechanism that checks code for syntactical errors. Developers can use the findings to increase their productivity, learning how to reap generative AI’s full potential while being aware of its limitations. Recommendations Researchers are encouraged to continue where this paper left off, exploring for Researchers more programming languages and prompting styles than the scope of this study allowed. Impact on Society As artificial intelligence touches more areas of society than ever, it is crucial to make AI models as accurate and dependable as possible. If practitioners and researchers use the findings of this paper to improve coders’ experience with generative AI, it will make millions of developers more productive, saving their companies money and time. Future Research The results of this study can be strengthened (or refuted) by a future study with a large, diverse dataset that more fully represents the programming languages and prompting styles developers tend to use. Moreover, further research can examine the reasons generative AI fails to deliver working code, which will yield valuable insights into improving these models. © (2024), (Informing Science Institute). All rights reserved.},
	author_keywords = {artificial intelligence; Bing Chat; ChatGPT; code modifications; concise code; ethics; programming assistants},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{Mahida20241,
	author = {Mahida, Ankur Harendrasinh},
	title = {Integrating Observability with DevOps Practices in Financial Services Technologies: A Study on Enhancing Software Development and Operational Resilience},
	year = {2024},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {15},
	number = {7},
	pages = {1 - 8},
	doi = {10.14569/IJACSA.2024.0150701},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200395357&doi=10.14569%2FIJACSA.2024.0150701&partnerID=40&md5=6f1eda9db6296170209b83c68fb41c35},
	abstract = {The finance market closely depends on translation and high-quality software solutions when performing crucial transactions and processing important information and customer services. Thus, systems’ reliability and good performance become crucial when these systems become complicated. This paper aims to focus on the implementation of the observability concept with the DevOps approach in financial services technologies, where its strengths, weaknesses, opportunities, and threats are also discussed with regard to the future. The concept of observability is intertwined with DevOps since, with its help, it is possible to gain deep insights into the system’s inner state and further enhance status monitoring, detect problems in less time, and optimize performance constantly. When organized and analyzed properly, observability data can, therefore, play a critical role in increasing software quality in financial institutions, aligning with regulatory standards, and decreasing development and operations teams’ silos. However, the implementation of observability within an organization using DevOps best practices in the financial services industry has some challenges, which include The issue of security, especially when it comes to data, the Challenge of data overload, the challenging task of encouraging the right organizational culture for continuous and consistent observability. The article presents a guide that discusses how to incorporate observability with DevOps: the step-by-step process of defining observability needs, choosing the most suitable tools, integrating with other tools in the existing DevOps frameworks, laboratory of alarms, and constant enhancement. Furthermore, it considers examples of how some financial organizations have applied observability to reduce risks, improve efficacy, and enrich customers’ interactions. In addition, the article also deliberates on the future perspectives of observability, for instance, artificial intelligence and machine learning are quickly emerging as means through which different tasks of observability can be automated, and there are increasing concerns with security when it comes to the implementation of observability in the financial services industry. By adopting observability and aligning it with DevOps, financial institutions can develop and sustain sound, reliable and high-quality infrastructure and maintain the industry’s leadership. © (2024), (Science and Information Organization). All rights reserved.},
	author_keywords = {DevOPs; integrated analysis; integration; monitoring; Observability; operational resilience},
	keywords = {Artificial intelligence; Computer software selection and evaluation; Finance; Observability; Devops; Financial institution; Financial services industries; Financial services technologies; High-quality software; Integrated analysis; Operational resilience; Performance; Software solution; Translation quality; Software design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@ARTICLE{Iqbal20245031,
	author = {Iqbal, Marya and Hafeez, Yasir and Almashfi, Nabil and Alsirhani, Amjad and Alserhani, Faeiz Mohammed and Ali, Sadia and Humayun, Mamoona and Jamal, Muhammad},
	title = {Enhancing Secure Development in Globally Distributed Software Product Lines: A Machine Learning-Powered Framework for Cyber-Resilient Ecosystems},
	year = {2024},
	journal = {Computers, Materials and Continua},
	volume = {79},
	number = {3},
	pages = {5031 - 5049},
	doi = {10.32604/cmc.2024.051371},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199196045&doi=10.32604%2Fcmc.2024.051371&partnerID=40&md5=c0143d1d6431ccf6b4345cd8879e8352},
	abstract = {Embracing software product lines (SPLs) is pivotal in the dynamic landscape of contemporary software development. However, the flexibility and global distribution inherent in modern systems pose significant challenges to managing SPL variability, underscoring the critical importance of robust cybersecurity measures. This paper advocates for leveraging machine learning (ML) to address variability management issues and fortify the security of SPL. In the context of the broader special issue theme on innovative cybersecurity approaches, our proposed ML-based framework offers an interdisciplinary perspective, blending insights from computing, social sciences, and business. Specifically, it employs ML for demand analysis, dynamic feature extraction, and enhanced feature selection in distributed settings, contributing to cyber-resilient ecosystems. Our experiments demonstrate the framework’s superiority, emphasizing its potential to boost productivity and security in SPLs. As digital threats evolve, this research catalyzes interdisciplinary collaborations, aligning with the special issue’s goal of breaking down academic barriers to strengthen digital ecosystems against sophisticated attacks while upholding ethics, privacy, and human values. © 2024 Tech Science Press. All rights reserved.},
	author_keywords = {cyber-resilience; cybersecurity; digital ecosystems; Machine Learning; variability management},
	keywords = {Blending; E-learning; Ecosystems; Feature extraction; Social sciences computing; Software design; Cybe-resilience; Cyber security; Digital ecosystem; Distributed software; Flexibility distribution; Global distribution; Machine-learning; Management issues; Software Product Line; Variability management; Cybersecurity},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Malik202499382,
	author = {Malik, Jasmita and Raja, M. M. and Pawar, Pranav M.},
	title = {A Systematic Review of Adversarial Machine Learning Attacks, Defensive Controls, and Technologies},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {99382 - 99421},
	doi = {10.1109/ACCESS.2024.3423323},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197531321&doi=10.1109%2FACCESS.2024.3423323&partnerID=40&md5=2f215088a3abe9e70f68cfbdf316767f},
	abstract = {Adversarial machine learning (AML) attacks have become a major concern for organizations in recent years, as AI has become the industry’s focal point and GenAI applications have grown in popularity around the world. Organizations are eager to invest in GenAI applications and develop their own large language models, but they face numerous security and data privacy issues, particularly AML attacks. AML attacks have jeopardized numerous large-scale machine learning models. If carried out successfully, AML attacks can significantly reduce the efficiency and precision of machine learning models. They have far-reaching negative consequences in the context of critical healthcare and autonomous transportation systems. In this paper, AML attacks are identified, analyzed, and classified using adversarial tactics and techniques. This research also recommends open-source tools for testing AI and ML models against AML attacks. Furthermore, this research suggests specific mitigating measures against each attack. It aims to serve as a guidance for organizations to defend against AML attacks and gain assurance in the security of ML models. © 2024 The Authors.},
	author_keywords = {Adversarial machine learning; AI assurance; cybersecurity; data privacy; secure software development lifecycle},
	keywords = {Application programs; Artificial intelligence; Cybersecurity; Learning systems; Life cycle; Open source software; Software design; Software testing; Adversarial machine learning; AI assurance; Computational modelling; Cyber security; Life cycle assessment; Machine-learning; Secure software development; Secure software development lifecycle; Security; Software development life-cycle; Software development management; Data privacy},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 30; All Open Access; Gold Open Access}
}

@ARTICLE{Vänskä2024,
	author = {Vänskä, Sini and Kemell, Kai Kristian and Mikkonen, Tommi and Abrahamsson, Pekka},
	title = {Continuous Software Engineering Practices in AI/ML Development Past the Narrow Lens of MLOps: Adoption Challenges},
	year = {2024},
	journal = {E-Informatica Software Engineering Journal},
	volume = {18},
	number = {1},
	pages = {},
	doi = {10.37190/e-Inf240102},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190453085&doi=10.37190%2Fe-Inf240102&partnerID=40&md5=4dcdd82634498512fe91c4bf0fc03a7c},
	abstract = {Background: Continuous software engineering practices are currently considered state of the art in software engineering (SE). Recently, this interest in continuous SE has extended to ML system development as well, primarily through MLOps. However, little is known about continuous SE in ML development outside the specific continuous practices present in MLOps. Aim: In this paper, we explored continuous SE in ML development more generally, outside the specific scope of MLOps. We sought to understand what challenges organizations face in adopting all the 13 continuous SE practices identified in existing literature. Method: We conducted a multiple case study of organizations developing ML systems. Data from the cases was collected through thematic interviews. The interview instrument focused on different aspects of continuous SE, as well as the use of relevant tools and methods. Results: We interviewed 8 ML experts from different organizations. Based on the data, we identified various challenges associated with the adoption of continuous SE practices in ML development. Our results are summarized through 7 key findings. Conclusion: The largest challenges we identified seem to stem from communication issues. ML experts seem to continue to work in silos, detached from both the rest of the project and the customers. © 2024 The Authors. Published by Wrocław University of Science and Technology Publishing House.},
	author_keywords = {artificial intelligence; continuous software engineering; continuous star; machine learning; multiple case study},
	keywords = {Software engineering; Continuous software engineerings; Continuous star; Machine-learning; Multiple-case study; Software engineering practices; State of the art; System development; Tools and methods; Machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Bahi202454,
	author = {Bahi, Anas and Gharib, Jihane and Gahi, Youssef},
	title = {Integrating Generative AI for Advancing Agile Software Development and Mitigating Project Management Challenges},
	year = {2024},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {15},
	number = {3},
	pages = {54 - 61},
	doi = {10.14569/IJACSA.2024.0150306},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189940766&doi=10.14569%2FIJACSA.2024.0150306&partnerID=40&md5=6792660b2b844bca456111559d129429},
	abstract = {Agile software development emphasizes iterative progress, adaptability, and stakeholder collaboration. It champions flexible planning, continuous improvement, and rapid delivery, aiming to respond swiftly to change and deliver value efficiently. Integrating Generative Artificial Intelligence (AI) into Agile software development processes presents a promising avenue for overcoming project management challenges and enhancing the efficiency and effectiveness of software development endeavors. This paper explores the potential benefits of leveraging Generative AI in Agile methodologies, aiming to streamline development workflows, foster innovation, and mitigate common project management challenges. By harnessing the capabilities of Generative AI for tasks such as code generation, automated testing, and predictive analytics, Agile teams can augment their productivity, accelerate delivery cycles, and improve the quality of software products. Additionally, Generative AI offers opportunities for enhancing collaboration, facilitating decision-making, and addressing uncertainties inherent in Agile project management. Through an in-depth analysis of the integration of Generative AI within Agile frameworks, this paper provides insights into how organizations can harness the transformative potential of AI to advance Agile software development practices and navigate the complexities of modern software projects more effectively. © (2024), (Science and Information Organization). All Rights Reserved.},
	author_keywords = {Agile software development; Artificial intelligence; software engineering},
	keywords = {Decision making; Iterative methods; Predictive analytics; Project management; Software design; Software testing; Agile Methodologies; Agile software development; Agile software development process; Codegeneration; Common projects; Continuous improvements; Development workflow; Flexible planning; Management challenges; Potential benefits; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20; All Open Access; Gold Open Access}
}

@ARTICLE{Ciancarini202422841,
	author = {Ciancarini, Paolo and Giancarlo, Raffaele and Grimaudo, Gennaro},
	title = {Digital Transformation in the Public Administrations: A Guided Tour for Computer Scientists},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {22841 - 22865},
	doi = {10.1109/ACCESS.2024.3363075},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184804391&doi=10.1109%2FACCESS.2024.3363075&partnerID=40&md5=73385594104e6b892c7b781a3577f1fe},
	abstract = {The goal of Digital Transformation of the Public Sector is the achievement of a better quality of life for citizens, via a more responsive and transparent administration and governance. By now it is clear that technological innovation, both in terms of computer architectures and software systems, is a crucial component of it, yet not sufficient. Indeed, a cultural, organizational and legal shift in how public organizations operate and relate to the citizens is also required. Nevertheless, computer scientists can play a key role in such a transformation and, given its impact on Society, it is essential to achieve a broader level of awareness of it and involvement in it of those scientific and professional figures. To this end, a technical map specifically designed for computer scientists, but properly placed in the context of the cultural, organizational and legal changes mentioned above, would be highly beneficial. To date, such a map is missing, to the best of our knowledge. The main contribution of this Tutorial is to provide it, together with a guided tour describing which key technological aspects enable and drive such a transformation. More specifically, based on a careful analysis of the available scholarly literature, that does not seem to include any Computer Science textbook material, a model of such a transformation is proposed, together with carefully selected examples incarnating it to show its validity: the cities of Barcelona and Chicago. Finally, a look at the future of this area is also provided. © 2013 IEEE.},
	author_keywords = {Agile software development; cloud computing; computer systems organization; computing methodologies; data knowledge and engineering; design and engineering of services for e-citizens; digital government; digital transformation; machine learning},
	keywords = {Computer architecture; Digital storage; Engineering education; Government data processing; Knowledge management; Learning systems; Metadata; Public administration; Software design; Agile software development; Cloud-computing; Computer system organization; Computer system organization cloud computing; Computing methodologies; Computing methodology machine learning; Data engineering; Data knowledge and engineering; Design and engineering of service for e-citizen; Digital government; Digital transformation; E-citizens; Electronicgovernment (e-government); Machine-learning; Open datum; Regulation; Tutorial; Cloud computing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Rahman20231065,
	author = {Rahman, Md Saidur and Khomh, Foutse and Hamidi, Alaleh and Cheng, Jinghui and Antoniol, Giuliano and Washizaki, Hironori},
	title = {Machine learning application development: practitioners’ insights},
	year = {2023},
	journal = {Software Quality Journal},
	volume = {31},
	number = {4},
	pages = {1065 - 1119},
	doi = {10.1007/s11219-023-09621-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151318230&doi=10.1007%2Fs11219-023-09621-9&partnerID=40&md5=5c297b05dc58a6c9962a9b8535c2d4f5},
	abstract = {Nowadays, intelligent systems and services are getting increasingly popular as they provide data-driven solutions to diverse real-world problems, thanks to recent breakthroughs in artificial intelligence (AI) and machine learning (ML). However, machine learning meets software engineering not only with promising potentials but also with some inherent challenges. Despite some recent research efforts, we still do not have a clear understanding of the challenges of developing ML-based applications and the current industry practices. Moreover, it is unclear where software engineering researchers should focus their efforts to better support ML application developers. In this paper, we report about a survey that aimed to understand the challenges and best practices of ML application development. We synthesize the results obtained from 80 practitioners (with diverse skills, experience, and application domains) into 17 findings outlining challenges and best practices for ML application development. Practitioners involved in the development of ML-based software systems can leverage the summarized best practices to improve the quality of their system. We hope that the reported challenges will inform the research community about topics that need to be investigated to improve the engineering process and the quality of ML-based applications. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Machine learning application development; Machine learning best practices; Testing machine learning application},
	keywords = {Application programs; Intelligent systems; Learning systems; Software testing; Application development; Best practices; Data driven; Machine learning application development; Machine learning applications; Machine learning best practice; Machine-learning; Testing machine; Testing machine learning application; Machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access}
}

@ARTICLE{Martins2023,
	author = {Martins, José Luís Bandeira and Branco, Frederico Augusto and Mamede, Henrique São},
	title = {Combining low-code development with ChatGPT to novel no-code approaches: A focus-group study},
	year = {2023},
	journal = {Intelligent Systems with Applications},
	volume = {20},
	pages = {},
	doi = {10.1016/j.iswa.2023.200289},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174408846&doi=10.1016%2Fj.iswa.2023.200289&partnerID=40&md5=d263b1346f2987b443d34315c14159a1},
	abstract = {Low-code tools are a trend in software development for business solutions due to their agility and ease of use. There are a certain number of vendors with such solutions. Still, in most Western countries, there is a clear need for the existence of greater quantities of certified and experienced professionals to work with those tools. This means that companies with more resources can attract and maintain those professionals, whilst other smaller organizations must rely on an endless search for this scarce resource. We will present and validate a model designed to transform ChatGPT into a low-code developer, addressing the demand for a more skilled human resource solution. This innovative tool underwent rigorous validation via a focus group study, engaging a panel of highly experienced experts. Their invaluable insights and feedback on the proposed model were systematically gathered and meticulously analysed. © 2023 The Author(s)},
	author_keywords = {Artificial intelligence; ChatGPT; LLM; Low-code; No-code; Software models},
	keywords = {Business solutions; ChatGPT; Code development; Ease-of-use; Focus group studies; LLM; Low-code; No-code; Software modeling; Western countries; Software design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20; All Open Access; Gold Open Access}
}

@ARTICLE{Obie2023,
	author = {Obie, Humphrey O. and Du, Hung and Madampe, Kashumi and Shahin, Mojtaba and Ilekura, Idowu and Grundy, John C. and Li, Li and Whittle, Jon and Turhan, Burak and Khalajzadeh, Hourieh},
	title = {Automated detection, categorisation and developers’ experience with the violations of honesty in mobile apps},
	year = {2023},
	journal = {Empirical Software Engineering},
	volume = {28},
	number = {6},
	pages = {},
	doi = {10.1007/s10664-023-10361-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173114542&doi=10.1007%2Fs10664-023-10361-4&partnerID=40&md5=f7cdf95b10a4b97f6c1297ffccc286a6},
	abstract = {Human values such as honesty, social responsibility, fairness, privacy, and the like are things considered important by individuals and society. Software systems, including mobile software applications (apps), may ignore or violate such values, leading to negative effects in various ways for individuals and society. While some works have investigated different aspects of human values in software engineering, this mixed-methods study focuses on honesty as a critical human value. In particular, we studied (i) how to detect honesty violations in mobile apps, (ii) the types of honesty violations in mobile apps, and (iii) the perspectives of app developers on these detected honesty violations. We first develop and evaluate 7 machine learning (ML) models to automatically detect violations of the value of honesty in app reviews from an end-user perspective. The most promising was a Deep Neural Network model with F1 score of 0.921. We then conducted a manual analysis of 401 reviews containing honesty violations and characterised honesty violations in mobile apps into 10 categories: unfair cancellation and refund policies; false advertisements; delusive subscriptions; cheating systems; inaccurate information; unfair fees; no service; deletion of reviews; impersonation; and fraudulent-looking apps. A developer survey and interview study with mobile developers then identified 7 key causes behind honesty violations in mobile apps and 8 strategies to avoid or fix such violations. The findings of our developer study also articulate the negative consequences that honesty violations might bring for businesses, developers, and users. Finally, the app developers’ feedback shows that our prototype ML-based models can have promising benefits in practice. © 2023, The Author(s).},
	author_keywords = {App reviews; Developer experience; Honesty; Human values; Machine Learning; Mixed-methods; Mobile apps},
	keywords = {Deep neural networks; E-learning; Learning systems; Neural network models; Social aspects; App review; Application developers; Automated detection; Developer experience; Honesty; Human values; Machine-learning; Mixed method; Mobile applications; Social responsibilities; Application programs},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Sangaroonsilp2023,
	author = {Sangaroonsilp, Pattaraporn and Choetkiertikul, Morakot and Dam, Hoa Khanh and Ghose, Aditya K.},
	title = {An empirical study of automated privacy requirements classification in issue reports},
	year = {2023},
	journal = {Automated Software Engineering},
	volume = {30},
	number = {2},
	pages = {},
	doi = {10.1007/s10515-023-00387-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163735171&doi=10.1007%2Fs10515-023-00387-9&partnerID=40&md5=6b782c718a816e11b2cc1d523d328f6f},
	abstract = {The recent advent of data protection laws and regulations has emerged to protect privacy and personal information of individuals. As the cases of privacy breaches and vulnerabilities are rapidly increasing, people are aware and more concerned about their privacy. These bring a significant attention to software development teams to address privacy concerns in developing software applications. As today’s software development adopts an agile, issue-driven approach, issues in an issue tracking system become a centralised pool that gathers new requirements, requests for modification and all the tasks of the software project. Hence, establishing an alignment between those issues and privacy requirements is an important step in developing privacy-aware software systems. This alignment also facilitates privacy compliance checking which may be required as an underlying part of regulations for organisations. However, manually establishing those alignments is labour intensive and time consuming. In this paper, we explore a wide range of machine learning and natural language processing techniques which can automatically classify privacy requirements in issue reports. We employ six popular techniques namely Bag-of-Words (BoW), N-gram Inverse Document Frequency (N-gram IDF), Term Frequency-Inverse Document Frequency (TF-IDF), Word2Vec, Convolutional Neural Network (CNN) and Bidirectional Encoder Representations from Transformers (BERT) to perform the classification on privacy-related issue reports in Google Chrome and Moodle projects. The evaluation showed that BoW, N-gram IDF, TF-IDF and Word2Vec techniques are suitable for classifying privacy requirements in those issue reports. In addition, N-gram IDF is the best performer in both projects. © 2023, The Author(s).},
	author_keywords = {Deep learning models; Issue reports; Issues classification; Machine learning; Natural language processing; Privacy; Privacy issues classification},
	keywords = {Application programs; Compliance control; Computational linguistics; Data privacy; Deep learning; Laws and legislation; Learning systems; Natural language processing systems; Software design; Text processing; Deep learning model; Issue classification; Issue report; Language processing; Learning models; Machine-learning; Natural language processing; Natural languages; Privacy; Privacy issue; Privacy issue classification; Alignment},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Ballingall2023,
	author = {Ballingall, Stuart and Sarvi, Majid and Sweatman, Peter F.},
	title = {Standards relevant to automated driving system safety: A systematic assessment},
	year = {2023},
	journal = {Transportation Engineering},
	volume = {13},
	pages = {},
	doi = {10.1016/j.treng.2023.100202},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170201795&doi=10.1016%2Fj.treng.2023.100202&partnerID=40&md5=e361c2c2782d70f5023f0e17964aae58},
	abstract = {Automated Driving Systems (ADSs) in road vehicles, which can undertake the dynamic driving task without requiring a human driver, are on the verge of large-scale deployment. Assurance frameworks for vehicle systems have traditionally been underpinned by standards. Some commonly adopted standards will provide barriers to the market deployment of ADSs, particularly the inherent capability of ADSs to have their driving functionality changed using Machine Learning (ML) during in-service operation. New standards specific to ADSs are being adopted, but some of these also present challenges for ML-enabled system changes. Then there are emerging cross-sector AI and ML standards that will have implications for the automotive industry and regulators. This paper summarises an in-depth assessment of standards that are anticipated to play an important role in the safety assurance of ADSs. Summarised findings are provided, including key themes, covering issues such as the use of ADS safety cases that are strengthened by adopting complementary standards, ensuring safety assurance models cover the whole ADS lifecycle, and the use of a systems engineering approach that treats safety as a dynamic control problem and appropriately integrates software engineering standards. Other themes were more specific to ML, including the need for rigorous change processes, horizonal AI and ML standards to be appropriately considered by automotive manufacturers and regulators, and authorities to be clear on whether online changes that can occur autonomously without human oversight are allowed or not. The findings, themes and recommendations in this paper are intended to inform future safety assurance frameworks for ADSs. © 2023},
	author_keywords = {Automated driving systems; Machine learning; Safety assurance; Software engineering; Standards},
	keywords = {Automation; Automotive industry; Life cycle; Safety engineering; Software engineering; Automated driving systems; Driving tasks; Human drivers; Large-scale deployment; Machine-learning; Road vehicles; Safety assurance; System safety; Systematic assessment; Vehicle system; Machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access; Gold Open Access}
}

@ARTICLE{Ahmad2023,
	author = {Ahmad, Khlood and Abdelrazek, Mohamed Almorsy and Arora, Chetan and Bano, Muneera and Grundy, John C.},
	title = {Requirements practices and gaps when engineering human-centered Artificial Intelligence systems},
	year = {2023},
	journal = {Applied Soft Computing},
	volume = {143},
	pages = {},
	doi = {10.1016/j.asoc.2023.110421},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160513131&doi=10.1016%2Fj.asoc.2023.110421&partnerID=40&md5=32db5447291ab4a79e62b88fe7c032b0},
	abstract = {Context: Engineering Artificial Intelligence (AI) software is a relatively new area with many challenges, unknowns, and limited proven best practices. Big companies such as Google, Microsoft, and Apple have provided a suite of recent guidelines to assist engineering teams in building human-centered AI systems. Objective: The practices currently adopted by practitioners for developing such systems, especially during Requirements Engineering (RE), are little studied and reported to date. Method: This paper presents the results of a survey conducted to understand current industry practices in RE for AI (RE4AI) and to determine which key human-centered AI guidelines should be followed. Our survey is based on mapping existing industrial guidelines, best practices, and efforts in the literature. Results: We surveyed 29 professionals and found most participants agreed that all the human-centered aspects we mapped should be addressed in RE. Further, we found that most participants were using UML or Microsoft Office to present requirements. Conclusion: We identify that most of the tools currently used are not equipped to manage AI-based software, and the use of UML and Office may pose issues with the quality of requirements captured for AI. Also, all human-centered practices mapped from the guidelines should be included in RE. © 2023 The Author(s)},
	author_keywords = {Artificial Intelligence; Human-centered; Machine learning; Requirements engineering; Software engineering; Survey research},
	keywords = {Engineering education; Requirements engineering; Software engineering; Artificial intelligence systems; Best practices; Engineering teams; Google+; Human-centered; Intelligence software; Machine-learning; MicroSoft; Requirement engineering; Survey research; Machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Srivastava2023160,
	author = {Srivastava, Prateek and Srivastava, Nidhi and Agrawal, Rashi C. and Singh, Pawan},
	title = {An Intelligent Framework for Estimating Software Development Projects using Machine Learning},
	year = {2023},
	journal = {International Journal on Recent and Innovation Trends in Computing and Communication},
	volume = {11},
	number = {5},
	pages = {160 - 169},
	doi = {10.17762/ijritcc.v11i5.6602},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164576408&doi=10.17762%2Fijritcc.v11i5.6602&partnerID=40&md5=677190567b729402804c8bf9bb4d98f9},
	abstract = {The IT industry has faced many challenges related to software effort and cost estimation. A cost assessment is conducted after software effort estimation, which benefits customers as well as developers. The purpose of this paper is to discuss various methods for the estimation of software effort and cost in the context of software engineering, such as algorithmic methods, expert judgment methods, analogy-based estimation methods, and machine learning methods, as well as their different aspects. In spite of this, estimation of the effort involved in software development are subject to uncertainty. Several methods have been developed in the literature for improving estimation accuracy, many of which involve the use of machine learning techniques. A machine learning framework is proposed in this paper to address this challenging problem. In addition to being completely independent of algorithmic models and estimation problems, this framework also features a modular architecture. It has high interpretability, learning capability, and robustness to imprecise and uncertain inputs. © 2023 Baqiyatallah University of Medical Sciences. All rights reserved.},
	author_keywords = {Cost Estimation; Effort; Machine Learning; Software Engineering; Software Project Estimation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@ARTICLE{Steidl2023,
	author = {Steidl, Monika and Felderer, Michael and Ramler, Rudolf},
	title = {The pipeline for the continuous development of artificial intelligence models—Current state of research and practice},
	year = {2023},
	journal = {Journal of Systems and Software},
	volume = {199},
	pages = {},
	doi = {10.1016/j.jss.2023.111615},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153617843&doi=10.1016%2Fj.jss.2023.111615&partnerID=40&md5=a8a73ec9d81608b363fbbcadc73c31de},
	abstract = {Companies struggle to continuously develop and deploy Artificial Intelligence (AI) models to complex production systems due to AI characteristics while assuring quality. To ease the development process, continuous pipelines for AI have become an active research area where consolidated and in-depth analysis regarding the terminology, triggers, tasks, and challenges is required. This paper includes a Multivocal Literature Review (MLR) where we consolidated 151 relevant formal and informal sources. In addition, nine-semi structured interviews with participants from academia and industry verified and extended the obtained information. Based on these sources, this paper provides and compares terminologies for Development and Operations (DevOps) and Continuous Integration (CI)/Continuous Delivery (CD) for AI, Machine Learning Operations (MLOps), (end-to-end) lifecycle management, and Continuous Delivery for Machine Learning (CD4ML). Furthermore, the paper provides an aggregated list of potential triggers for reiterating the pipeline, such as alert systems or schedules. In addition, this work uses a taxonomy creation strategy to present a consolidated pipeline comprising tasks regarding the continuous development of AI. This pipeline consists of four stages: Data Handling, Model Learning, Software Development and System Operations. Moreover, we map challenges regarding pipeline implementation, adaption, and usage for the continuous development of AI to these four stages. © 2023 The Authors},
	author_keywords = {CI/CD for AI; Continuous (end-to-end) lifecycle pipeline for AI; Continuous development of AI; DevOps for AI; MLOps; Multivocal literature review},
	keywords = {Data handling; Life cycle; Pipelines; Software design; Terminology; Continuous (end-to-end) lifecycle pipeline for artificial intelligence; Continuous development; Continuous development of artificial intelligence; Continuous integration/continuous delivery for artificial intelligence; Continuous integrations; Development and operation for artificial intelligence; Development and operations; End to end; Literature reviews; Machine learning operation; Machine-learning; Multivocal literature review; Machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 63; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Veitía2023,
	author = {Veitía, Francisco Javier Peña and Roldán, María Luciana and Vegetti, Marcela},
	title = {Analysis and comparison of deep learning models for user stories identification; Análisis y comparación de modelos de aprendizaje profundo para la identificación de historias de usuario},
	year = {2023},
	journal = {Ingeniare},
	volume = {31},
	pages = {},
	doi = {10.4067/s0718-33052023000100229},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192990396&doi=10.4067%2Fs0718-33052023000100229&partnerID=40&md5=415625cb000ac9558f1ccd779ef36ef5},
	abstract = {Nowadays, most software companies have adopted agile development methodologies, which suggest the capture of requirements through user stories. Issues Management Systems allow development teams to manage user stories and other issues, such as errors, change requests, and others. Although these systems provide features for categorizing or labeling issue types, the user often needs to include or specify this information correctly. A poor issue categorization causes many user stories to end up buried in a large volume of data, making it difficult to identify them. This article presents and compares three neural network models to classify issues as User Stories. As the ultimate goal of this research is to improve the quality of the software development project documentation, the comparison is practical to select a model to be embedded in an IMS tool for automatically categorizing issues. The compared models are a BRNN-LSTM model, an Elmo-based model, and a BERT-based model. It applied the CRISP-MD methodology to train, validate, and test the three proposed neural network models. Then, a comparison was performed regarding their accuracy and performance. As a result, the article shows that the BERT-based model is the one that best fits the problem posed, managing to classify the issues as user stories with an accuracy of approximately 97%. This model can analyze the text syntactically and semantically with the best accuracy and performance. © 2023, Universidad de Tarapaca. All rights reserved.},
	author_keywords = {machine learning; Natural language processing; recurrent neural networks; software engineering; user story},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Anwar2023143815,
	author = {Anwar, Rahila and Bashir, Muhammad Bilal},
	title = {A Systematic Literature Review of AI-Based Software Requirements Prioritization Techniques},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {143815 - 143860},
	doi = {10.1109/ACCESS.2023.3343252},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180316281&doi=10.1109%2FACCESS.2023.3343252&partnerID=40&md5=4655690431fd057097668a3389cf7e27},
	abstract = {Software requirements show what the customer desires his software to do. They are the first stepping stone towards a successful software development project. With the increasing complexity of the software due to its size and feature base, it is vital to prioritize the requirements for efficient utilization of development resources. To achieve this, industrial organizations are devising new strategies and improved solutions even with the help of artificial intelligence (AI) tool set. Existing requirements prioritization techniques are human-intensive and suffer from several limitations like overlapping outcomes, scalability problems, time consumption, inaccuracy, and so on. Some of the problems can be solved by including artificial intelligence algorithms and strategies. Several AI-based requirements prioritization techniques have been proposed by applying Genetic Algorithms, Fuzzy Logic, Ant Colony Optimization, and Machine Learning. Literature witnesses some good review studies and surveys on conventional prioritization techniques but there exists none for AI-based techniques that identify not only their strengths but also their weaknesses, advantages of machine learning techniques over other AI-based requirements prioritization techniques, and limitations of applying AI-based techniques in requirements prioritization. This study presents a systematic literature review (SLR) of AI-based requirements prioritization approaches covering 46 papers published from 2000 to 2021. We have given this literature review a new dimension by conducting a parametric analysis of AI-based requirements prioritization techniques and we have identified these parameters after a thorough literature study. Some of the chosen parameters are generic (related to the prioritization process) and some are specific (related to AI techniques). This study has greatly helped us draw a clear line among AI-based techniques to show their domain of application to gain maximum advantage. Our findings will assist researchers, requirement analysts, and other stakeholders in making a wise decision to select the best requirements prioritization technique to gain optimal results. © 2013 IEEE.},
	author_keywords = {ant colony; Artificial intelligence; fuzzy logic; generic parameters; genetic algorithm; machine learning; optimization; requirement analysis; requirement engineering; requirement prioritization; specific parameters},
	keywords = {Ant colony optimization; Artificial intelligence; Computer circuits; Fuzzy logic; Learning algorithms; Learning systems; Quality control; Requirements engineering; Software design; Ant colonies; Benchmark testing; Fuzzy-Logic; Generic parameters; Machine-learning; Optimisations; Quality assessment; Requirement analysis; Requirement engineering; Requirements prioritization; Software; Specific parameter; Systematic; Genetic algorithms},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; All Open Access; Gold Open Access}
}

@ARTICLE{Yejian2023142447,
	author = {Yejian, Zhang and Takada, Shingo},
	title = {Review Classification Based on Machine Learning: Classifying Game User Reviews},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {142447 - 142463},
	doi = {10.1109/ACCESS.2023.3342294},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179818388&doi=10.1109%2FACCESS.2023.3342294&partnerID=40&md5=a1d98128ee9269fb2ae0b03920354524},
	abstract = {With the development of the game industry, the maturity of online game sales platforms, and the increasing complexity of game software itself, game companies need to analyze massive amounts of user reviews to understand the hidden defects of the game and the direction of future iterations. Manually reading game reviews is a labor-intensive and time-consuming task, as the number of reviews can go up to several thousand per day. Automatically classifying these game reviews will help alleviate this issue, but traditional classifiers will need a large number of labeled instances for training. In this paper, we propose and implement an approach that combines transfer learning in natural language processing (BERT), unsupervised learning, and active learning to classify game reviews using only a small amount of labeled instances. We found that our approach obtains 88.8% classification accuracy with only 100 labeled training instances. Our implementation can be extended to handle different types of new games by using a small amount of extra labeled instances and manual work. © 2013 IEEE.},
	author_keywords = {Active learning; game review classification; machine learning; natural language processing; software engineering},
	keywords = {Artificial intelligence; Classification (of information); Computer software; Job analysis; Learning algorithms; Learning systems; Natural language processing systems; Active Learning; Game; Game review classification; Game reviews; Labelings; Language processing; Natural language processing; Natural languages; Software; Task analysis; Data mining},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Bocu2023123924,
	author = {Bocu, Rǎzvan and Baicoianu, Alexandra and Kerestely, Arpad},
	title = {An Extended Survey Concerning the Significance of Artificial Intelligence and Machine Learning Techniques for Bug Triage and Management},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {123924 - 123937},
	doi = {10.1109/ACCESS.2023.3329732},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177160916&doi=10.1109%2FACCESS.2023.3329732&partnerID=40&md5=3f548a2bff29797a501ef761af4071c8},
	abstract = {Bug reports are generated in large numbers during the software development processes in the software industry. The manual processing of these issues is usually time consuming and prone to errors, consequently delaying the entire software development process. Thus, a properly designed bug triage and management process implies that essential operations, such as duplicate detection, bug assignments to proper developers, and determination of the importance level, are sustained by efficient algorithmic models and implementation approaches. Designing and implementing a proper bug triage and management process becomes an essential scientific research topic, as it may significantly optimize the software development and business process in the information technology industry. Consequently, this paper thoroughly surveys the most significant related scientific contributions analytically and constructively, distinguishing it from similar survey papers. The paper proposes optimal algorithmic and software solutions for particular real-world use cases that are analyzed. It concludes by presenting the most important open research questions and challenges. Additionally, the paper provides a valuable scientific literature survey for any researcher or practitioner in software bug triage and management systems based on artificial intelligence and machine learning techniques. © 2013 IEEE.},
	author_keywords = {bug assignment; bug prioritization; Bug report; bug triaging; classification; machine learning},
	keywords = {Information management; Learning algorithms; Program debugging; Software design; Artificial intelligence learning; Bug assignment; Bug prioritization; Bug reports; Bug triaging; Machine learning techniques; Machine-learning; Management process; Prioritization; Software development process; Machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access; Gold Open Access}
}

@ARTICLE{Arias-Barahona2023,
	author = {Arias-Barahona, María Ximena and Arteaga-Arteaga, Harold Brayan and Orozco-Arias, Simon and Flórez-Ruíz, Juan Camilo and Valencia Diaz, Mario Andres and Tabares-Soto, Reinel},
	title = {Requests classification in the customer service area for software companies using machine learning and natural language processing},
	year = {2023},
	journal = {PeerJ Computer Science},
	volume = {9},
	pages = {},
	doi = {10.7717/peerj-cs.1016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177054301&doi=10.7717%2Fpeerj-cs.1016&partnerID=40&md5=4b39096ceedc16dfad3bb582ef0591f3},
	abstract = {Artificial intelligence (AI) is one of the components recognized for its potential to transform the way we live today radically. It makes it possible for machines to learn from experience, adjust to new contributions and perform tasks like human beings. The business field is the focus of this research. This article proposes implementing an incident classification model using machine learning (ML) and natural language processing (NLP). The application is for the technical support area in a software development company that currently resolves customer requests manually. Through ML and NLP techniques applied to company data, it is possible to know the category of a request given by the client. It increases customer satisfaction by reviewing historical records to analyze their behavior and correctly provide the expected solution to the incidents presented. Also, this practice would reduce the cost and time spent on relationship management with the potential consumer. This work evaluates different Machine Learning models, such as support vector machine (SVM), Extra Trees, and Random Forest. The SVM algorithm demonstrates the highest accuracy of 98.97% with class balance, hyper-parameter optimization, and pre-processing techniques. Subjects Algorithms and Analysis of Algorithms, Artificial Intelligence, Data Mining and Machine Learning, Data Science, Natural Language and Speech © 2023, Arias-Barahona et al. Distributed under Creative Commons CC-BY 4.0.},
	author_keywords = {Consumer service; Machine learning; Natural language processing; Requests classification; Text classification},
	keywords = {Application programs; Customer satisfaction; Data mining; Forestry; Learning algorithms; Learning systems; Natural language processing systems; Sales; Software design; Support vector machines; Text processing; Consumer services; Customer-service; Language processing; Learning languages; Machine-learning; Natural language processing; Natural languages; Request classification; Service area; Text classification; Classification (of information)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Rana2023,
	author = {Rana, Saadia Afzal and Azizul, Zati Hakim and Awan, Ali Afzal},
	title = {A step toward building a unified framework for managing AI bias},
	year = {2023},
	journal = {PeerJ Computer Science},
	volume = {9},
	pages = {},
	doi = {10.7717/peerj-cs.1630},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175246267&doi=10.7717%2Fpeerj-cs.1630&partnerID=40&md5=a6aa6768d70f6c1147092d9e9389655c},
	abstract = {Integrating artificial intelligence (AI) has transformed living standards. However, AI’s efforts are being thwarted by concerns about the rise of biases and unfairness. The problem advocates strongly for a strategy for tackling potential biases. This article thoroughly evaluates existing knowledge to enhance fairness management, which will serve as a foundation for creating a unified framework to address any bias and its subsequent mitigation method throughout the AI development pipeline. We map the software development life cycle (SDLC), machine learning life cycle (MLLC) and cross industry standard process for data mining (CRISP-DM) together to have a general understanding of how phases in these development processes are related to each other. The map should benefit researchers from multiple technical backgrounds. Biases are categorised into three distinct classes; pre-existing, technical and emergent bias, and subsequently, three mitigation strategies; conceptual, empirical and technical, along with fairness management approaches; fairness sampling, learning and certification. The recommended practices for debias and overcoming challenges encountered further set directions for successfully establishing a unified framework. © Copyright 2023 Rana et al.},
	author_keywords = {Algorithmic bias; Bias mitigation strategy; Data-driven AI system; Fairness in data mining; Fairness management},
	keywords = {Artificial intelligence; Information management; Life cycle; Software design; Algorithmic bias; Algorithmics; Artificial intelligence systems; Bias mitigation strategy; Data driven; Data-driven artificial intelligence system; Fairness in data mining; Fairness management; Mitigation strategy; Unified framework; Data mining},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Jadhav202383523,
	author = {Jadhav, Akshay and Kumar Shandilya, Shishir Kumar and Izonin, Ivan V. and Gregus, Michal Jr},
	title = {Effective Software Effort Estimation Leveraging Machine Learning for Digital Transformation},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {83523 - 83536},
	doi = {10.1109/ACCESS.2023.3293432},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164400531&doi=10.1109%2FACCESS.2023.3293432&partnerID=40&md5=db219b2670708711801a15d1ee18199f},
	abstract = {Software effort estimation is a necessary component of software development projects that belong to industrial software systems and digital transformation initiatives. Digital transformation refers to the process of integrating digital technology into various components of a company or organization in order to improve operations, procedures, customer experiences, and overall performance. Industrial software systems are trained software packages designed for use in industrial and manufacturing processes. The paper deals with the machine learning based effort estimation in order to create an effective and robust model for predicting effort. The paper proposes an Omni-Ensemble Learning (OEL) approach, which is a combination of static ensemble selection along with genetic algorithm and dynamic ensemble selection. The paper identifies the impact of software effort estimation in industrial software system, and works on the these attributes to implement a robust ensemble model. The proposed Omni-Ensemble Selection (OES) provides better overall performance (in terms of evaluation metrics) and on comparing with multiple machine learning models over Finnish and Maxwell datasets. © 2013 IEEE.},
	author_keywords = {Digital transformation; industrial software system; software effort estimation; software engineering},
	keywords = {Artificial intelligence; Learning systems; Software design; Digital transformation; Fourth industrial revolution; Industrial revolutions; Industrial software; Industrial software system; Software; Software effort estimation; Software-systems; Stakeholder; Genetic algorithms},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access; Gold Open Access}
}

@ARTICLE{Olaleye2023,
	author = {Olaleye, Taiwo Olapeju and Arogundade, O. T. and Sanjay, Misra and Abayomi-Alli, Adebayo Adewumi and Kose, Utku},
	title = {Predictive Analytics and Software Defect Severity: A Systematic Review and Future Directions},
	year = {2023},
	journal = {Scientific Programming},
	volume = {2023},
	pages = {},
	doi = {10.1155/2023/6221388},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148108224&doi=10.1155%2F2023%2F6221388&partnerID=40&md5=6725d4fbdffa293099dd3dd3968b5921},
	abstract = {Software testing identifies defects in software products with varying multiplying effects based on their severity levels and sequel to instant rectifications, hence the rate of a research study in the software engineering domain. In this paper, a systematic literature review (SLR) on machine learning-based software defect severity prediction was conducted in the last decade. The SLR was aimed at detecting germane areas central to efficient predictive analytics, which are seldom captured in existing software defect severity prediction reviews. The germane areas include the analysis of techniques or approaches which have a significant influence on the threats to the validity of proposed models, and the bias-variance tradeoff considerations techniques in data science-based approaches. A population, intervention, and outcome model is adopted for better search terms during the literature selection process, and subsequent quality assurance scrutiny yielded fifty-two primary studies. A subsequent thoroughbred systematic review was conducted on the final selected studies to answer eleven main research questions, which uncovers approaches that speak to the aforementioned germane areas of interest. The results indicate that while the machine learning approach is ubiquitous for predicting software defect severity, germane techniques central to better predictive analytics are infrequent in literature. This study is concluded by summarizing prominent study trends in a mind map to stimulate future research in the software engineering industry. © 2023 T. O. Olaleye et al.},
	keywords = {Defects; Machine learning; Quality assurance; Software testing; Germanes; Machine-learning; On-machines; Research studies; Software defects; Software engineering domain; Software products; Software testings; Systematic literature review; Systematic Review; Predictive analytics},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Georgiou2023,
	author = {Georgiou, Konstantinos and Mittas, Nikolaos and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander N. and Angelis, Lefteris},
	title = {Data-Oriented Software Development: The Industrial Landscape through Patent Analysis},
	year = {2023},
	journal = {Information (Switzerland)},
	volume = {14},
	number = {1},
	pages = {},
	doi = {10.3390/info14010004},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146717794&doi=10.3390%2Finfo14010004&partnerID=40&md5=0492cfdfa53cde013326490f6f1e2133},
	abstract = {Τhe large amounts of information produced daily by organizations and enterprises have led to the development of specialized software that can process high volumes of data. Given that the technologies and methodologies used to develop software are constantly changing, offering significant market opportunities, organizations turn to patenting their inventions to secure their ownership as well as their commercial exploitation. In this study, we investigate the landscape of data-oriented software development via the collection and analysis of information extracted from patents. To this regard, we made use of advanced statistical and machine learning approaches, namely Latent Dirichlet Allocation and Brokerage Analysis for the identification of technological trends and thematic axes related to software development patent activity dedicated to data processing and data management processes. Our findings reveal that high-profile countries and organizations are engaging in patent granting, while the main thematic circles found in the retrieved patent data revolve around data updates, integration, version control and software deployment. The results indicate that patent grants in this technological domain are expected to continue their increasing trend in the following years, given that technologies evolve and the need for efficient data processing becomes even more present. © 2022 by the authors.},
	author_keywords = {data management; data processing; patent analysis; software engineering},
	keywords = {Data handling; Engineering education; Patents and inventions; Software design; Statistics; Amount of information; Commercial exploitation; High volumes; Large amounts; Latent Dirichlet allocation; Machine learning approaches; Market opportunities; Patent analysis; Specialized software; Statistical learning; Information management},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Hayashi2022,
	author = {Hayashi, Victor Takashi and Ruggiero, Wilson Vicente and Estrella, Júlio Ćezar and Filho, Artino Quintino and Pita, Matheus Ancelmo Bonfim and Arakaki, Reginaldo and Ribeiro, Cairo Mateus Neves and Trazzi, Bruno Manias and Bulla, Romeo},
	title = {A TDD Framework for Automated Monitoring in Internet of Things with Machine Learning †},
	year = {2022},
	journal = {Sensors},
	volume = {22},
	number = {23},
	pages = {},
	doi = {10.3390/s22239498},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143755750&doi=10.3390%2Fs22239498&partnerID=40&md5=96a5d02dc4a338b9ee5c6a3e5d859f4b},
	abstract = {Robust, fault tolerant, and available systems are fundamental for the adoption of Internet of Things (IoT) in critical domains, such as finance, health, and safety. The IoT infrastructure is often used to collect a large amount of data to meet the business demands of Smart Cities, Industry 4.0, and Smart Home, but there is a opportunity to use these data to intrinsically monitor an IoT system in an autonomous way. A Test Driven Development (TDD) approach for automatic module assessment for ESP32 and ESP8266 IoT development devices based on unsupervised Machine Learning (ML) is proposed to monitor IoT device status. A framework consisting of business drivers, non-functional requirements, engineering view, dynamic system evaluation, and recommendations phases is proposed to be used with the TDD development tool. The proposal is evaluated in academic and smart home study cases with 25 devices, consisting of 15 different firmware versions collected in one week, with a total of over 550,000 IoT status readings. The K-Means algorithm was applied to free memory available, internal temperature, and Wi-Fi level metrics to automatically monitor the IoT devices under development to identify device constraints violation and provide insights for monitoring frequency configuration of different firmware versions. To the best of the authors’ knowledge, it is the first TDD approach for IoT module automatic assessment which uses machine learning based on the real testbed data. The IoT status monitoring and the Python scripts for model training and inference with K-Means algorithm are available under a Creative Commons license. © 2022 by the authors.},
	author_keywords = {IoT; machine learning; software engineering; TDD; testbed},
	keywords = {Automation; Firmware; Inference engines; Internet of things; K-means clustering; Testbeds; Automated monitoring; Critical domain; Development approach; Development frameworks; Fault-tolerant; Health and safety; K-mean algorithms; Machine-learning; Smart homes; Test driven development; Machine learning; algorithm; Internet; machine learning; unsupervised machine learning; Algorithms; Internet of Things; Machine Learning; Unsupervised Machine Learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Mazumdar2022,
	author = {Mazumdar, Somnath},
	title = {Towards a better blockchainification of supply chain applications},
	year = {2022},
	journal = {Systems and Soft Computing},
	volume = {4},
	pages = {},
	doi = {10.1016/j.sasc.2022.200043},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138781102&doi=10.1016%2Fj.sasc.2022.200043&partnerID=40&md5=d3d92cf8bb5dfb6771ec82ff1334643e},
	abstract = {A supply chain ecosystem is a collection of complex asynchronous events. Blockchain has already found commercial applications in the SC domain, particularly in product tracing and verification. However, there is a lack of uniformity in these approaches. Application-generated data cannot be accessed across the supply chain ecosystem, resulting in data silos. Data silos reduce the opportunity for supply chain process optimizations. This paper does not propose any supply chain solution but a generic framework primarily aimed at reducing the communication gaps among the stakeholders and application developer(s) to build quality solutions. The ideal readers are who want to blockchanify their existing supply chain applications. The proposed framework can add real value to the organization by developing effective SC solutions satisfying application requirements. The framework consists of four stages. In the first stage, it extracts the application requirements and then maps on blockchain following an asynchronous mode of communication among the stakeholders and application developer(s). Next, it discusses how it can combine technologies to achieve the requirements stated in the first stage. Later, it discusses how to perform effective data management. Finally, it proposes a four-stage software build method that can lead to an efficient SC solution. The primary aim of this framework is to reduce communication gaps during solution development and ensure smooth operational data movement across the SC ecosystem, thanks to blockchain. The software development process also embeds eight essential features for a quality solution. The paper is concluded by discussing the technical challenges. © 2022 The Author(s)},
	author_keywords = {Blockchain; Data; Framework; IoT; Machine learning; Software; Supply chain},
	keywords = {Application programs; Blockchain; Ecosystems; Information management; Internet of things; Machine learning; Software design; Application developers; Block-chain; Communication gaps; Data; Data silos; Framework; IoT; Machine-learning; Software; Supply chain applications; Supply chains},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Assadi2022,
	author = {Assadi, Azadeh and Laussen, Peter C. and Goodwin, Andrew James and Goodfellow, Sebastian D. and Dixon, William and Greer, Robert W. and Jegatheeswaran, Anusha and Singh, Devin S. and McCradden, Melissa Danielle and Gallant, Sara N. and Goldenberg, Anna and Eytan, Danny and Mazwi, Mjaye L.},
	title = {An integration engineering framework for machine learning in healthcare},
	year = {2022},
	journal = {Frontiers in Digital Health},
	volume = {4},
	pages = {},
	doi = {10.3389/fdgth.2022.932411},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136168660&doi=10.3389%2Ffdgth.2022.932411&partnerID=40&md5=91768f706e4c76f6b57bdd724606bc76},
	abstract = {Background and Objectives: Machine Learning offers opportunities to improve patient outcomes, team performance, and reduce healthcare costs. Yet only a small fraction of all Machine Learning models for health care have been successfully integrated into the clinical space. There are no current guidelines for clinical model integration, leading to waste, unnecessary costs, patient harm, and decreases in efficiency when improperly implemented. Systems engineering is widely used in industry to achieve an integrated system of systems through an interprofessional collaborative approach to system design, development, and integration. We propose a framework based on systems engineering to guide the development and integration of Machine Learning models in healthcare. Methods: Applied systems engineering, software engineering and health care Machine Learning software development practices were reviewed and critically appraised to establish an understanding of limitations and challenges within these domains. Principles of systems engineering were used to develop solutions to address the identified problems. The framework was then harmonized with the Machine Learning software development process to create a systems engineering-based Machine Learning software development approach in the healthcare domain. Results: We present an integration framework for healthcare Artificial Intelligence that considers the entirety of this system of systems. Our proposed framework utilizes a combined software and integration engineering approach and consists of four phases: (1) Inception, (2) Preparation, (3) Development, and (4) Integration. During each phase, we present specific elements for consideration in each of the three domains of integration: The Human, The Technical System, and The Environment. There are also elements that are considered in the interactions between these domains. Conclusion: Clinical models are technical systems that need to be integrated into the existing system of systems in health care. A systems engineering approach to integration ensures appropriate elements are considered at each stage of model design to facilitate model integration. Our proposed framework is based on principles of systems engineering and can serve as a guide for model development, increasing the likelihood of successful Machine Learning translation and integration. 2022 Assadi, Laussen, Goodwin, Goodfellow, Dixon, Greer, Jegatheeswaran, Singh, McCradden, Gallant, Goldenberg, Eytan and Mazwi.},
	author_keywords = {artificial intelligence; digital health; healthcare (MeSH); human factors engineering (HFE); Integration engineering; machine learning; system of systems (SoS)},
	keywords = {Article; artificial intelligence; biomedical engineering; engineering; health care; human; human factors engineering; integration engineering; machine learning; man machine interaction; software design; system of systems; systems engineering; telehealth},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Chahar2022376,
	author = {Chahar, Vikas and Bhatia, Pradeep Kumar},
	title = {Performance Analysis of Software Test Effort Estimation using Genetic Algorithm and Neural Network},
	year = {2022},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {13},
	number = {10},
	pages = {376 - 383},
	doi = {10.14569/IJACSA.2022.0131045},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141796885&doi=10.14569%2FIJACSA.2022.0131045&partnerID=40&md5=e2f0c955f64943ecfae9ec223f76115d},
	abstract = {In present scenario, the software companies are frequently involving software test effort estimation to allocate the resources efficiently during the software development process. Different machine learning models are developed to estimate the total effort that would be required before the software product could be delivered. These computational models are used to use the past data to estimate the efforts. In the current studies, test effort estimation for software is predicted using the Genetic algorithm and Neural Network. The attributes are selected using the Genetic algorithm and similarity measure between the attribute values has been computed using the Cosine Similarity measure. The simulation experiments were done using the PROMISE and Kaggle repository and implementation was done using the MATLAB software. The performance metrics namely, precision, recall, and accuracy are computed to evaluate against the existing techniques. The accuracy of the proposed model is 91.3% and results are improved by 8.9% in comparison to existing technique and comparison has been made for superiority to predict the test effort for software development. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.},
	author_keywords = {Computational intelligence; Machine learning; Neural network; Software testing; Test effort estimation},
	keywords = {Learning algorithms; Machine learning; MATLAB; Neural networks; Software design; Software testing; Testing; Effort Estimation; Machine learning models; Machine-learning; Neural-networks; Performances analysis; Software company; Software development process; Software testings; Test effort estimation; Test efforts; Genetic algorithms},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{Alshammari2022,
	author = {Alshammari, Fahad H.},
	title = {Trends in Intelligent and AI-Based Software Engineering Processes: A Deep Learning-Based Software Process Model Recommendation Method},
	year = {2022},
	journal = {Computational Intelligence and Neuroscience},
	volume = {2022},
	pages = {},
	doi = {10.1155/2022/1960684},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139886449&doi=10.1155%2F2022%2F1960684&partnerID=40&md5=99174796c27bf71bac96e9d3bab76675},
	abstract = {In recent years, numerous studies have successfully implemented machine learning strategies in a wide range of application areas. Therefore, several different deep learning models exist, each one tailored to a certain software task. Using deep learning models provides numerous advantages for the software development industry. Testing and maintaining software is a critical concern today. Software engineers have many responsibilities while developing a software system, including coding, testing, and delivering the software to users via the cloud. From this list, it is easy to see that each task calls for extensive organization and preparation, as well as access to a variety of resources. A developer may consult other code repositories, websites with related programming content, and even colleagues for information before attempting to build and test a solution to the problem at hand. In this investigation, we aim to identify the factors that led to developing the recommender. This system analyzes the recommender's performance and provides suggestions for improving the software based on users' opinions. © 2022 Fahad H. Alshammari.},
	keywords = {Codes (symbols); Deep learning; Learning systems; Recommender systems; Software testing; Application area; Development industry; Learning models; Learning strategy; Machine-learning; Recommendation methods; Software engineering process; Software process models; Software tasks; Software-systems; Software design; machine learning; software; Deep Learning; Machine Learning; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Hassan2022111,
	author = {Hassan, Hossam and Abdel-Fattah, Manal A. and Ghoneim, Amr S.},
	title = {Risk Prediction Applied to Global Software Development using Machine Learning Methods},
	year = {2022},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {13},
	number = {9},
	pages = {111 - 120},
	doi = {10.14569/IJACSA.2022.0130913},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139361681&doi=10.14569%2FIJACSA.2022.0130913&partnerID=40&md5=f58476393bd1b214a45acfcbc410079d},
	abstract = {Software companies aim to develop high-quality software projects with the best global resources at the best cost. To achieve this global software development (GSD), an approach should be used which adopts work on projects across multiple distributed locations, and this is also known as distributed development. When companies attempt to implement GSD, they face numerous challenges owing to the nature of GSD and its differences from traditional methods. The objectives of this study were to identify the top software development factors that affect the overall success or failure of a software project using exploratory data analysis to find relationships between these factors, and to develop and compare risk prediction models that use machine learning classification techniques such as logistic regression, decision tree, random forest, support vector machine, K-nearest neighbors, and Naive Bayes. The findings of this study are as follows: in GSD, the top 18 factors influencing the software project are listed; and experiments show that the logistic regression and random forest models provide the best results, with an accuracy of 89% and 85%, respectively, and an area under the curve of 73% and 71%, respectively. © 2022,International Journal of Advanced Computer Science and Applications. All Rights Reserved.},
	author_keywords = {Distributed development; Global software development; Machine learning; Risk prediction model},
	keywords = {Forecasting; Logistic regression; Nearest neighbor search; Random forests; Risk assessment; Software design; Support vector regression; Distributed development; Global software development; High-quality software; Logistics regressions; Machine learning methods; Machine-learning; Risk prediction models; Risk predictions; Software company; Software project; Decision trees},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Gold Open Access}
}

@ARTICLE{Hai202283249,
	author = {Hai, Vo Van and Thi Kim Nhung, Ho Le and Prokopová, Zdenka and Silhavy, Radek and Šilhavy, Petr},
	title = {Toward Improving the Efficiency of Software Development Effort Estimation via Clustering Analysis},
	year = {2022},
	journal = {IEEE Access},
	volume = {10},
	pages = {83249 - 83264},
	doi = {10.1109/ACCESS.2022.3185393},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133809040&doi=10.1109%2FACCESS.2022.3185393&partnerID=40&md5=9f2c3632dd0e6ebd2adcbed22f2a2431},
	abstract = {Introduction: The precise estimation of software effort is a significant difficulty that project managers encounter during software development. Inaccurate forecasting leads to either overestimating or underestimating software effort, which can be detrimental for stakeholders. The International Function Point Users Group's Function Point Analysis (FPA) method is one of the most critical methods for software effort estimation. However, the practice of using the FPA method in the same fashion across all software areas needs to be reexamined. Aim: We propose a model for evaluating the influence of data clustering on software development effort estimation and then finding the best clustering method. We call this model the effort estimation using machine learning applied to the clusters (EEAC) model. Method: We cluster the dataset according to the clustering method and then apply the FPA and EEAC methods to these clusters for effort estimation. The clustering methods we use in this study include five categorical variable criteria (Development Platform, Industrial Sector, Language Type, Organization Type, and Relative Size) and the k-means clustering algorithm. Results: The experimental results show that the estimation accuracy obtaining with clustering consistently outperforms the accuracy without clustering for both the FPA and EEAC methods. Significantly, using the FPA method, the average improvement rate from using clustering as opposed to non-clustered was highest at 58.06%, according to the RMSE. With the EEAC method, this number reached 65.53%. The Industry Sector categorical variable achieves the best accuracy estimation compared to the other clustering criteria and k-means clustering. The improvement in accuracy in terms of the RMSE when applying this criterion is 63.68% for the FPA method and 72.02% for the EEAC method. Conclusion: Better results are obtained through dataset clustering compared to no clustering for both the FPA and EEAC methods. The Industry Sector is the most suitable clustering method among the tested clustering methods. © 2013 IEEE.},
	author_keywords = {categorical variables; dataset clustering; function point analysis; K-means; machine learning; Software effort estimation},
	keywords = {K-means clustering; Learning systems; Software design; Categorical variables; Clustering methods; Computational modelling; Dataset clustering; Function point analysis; K-means; Machine-learning; Software; Software algorithms; Software effort estimation; Cluster analysis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Sofian202251021,
	author = {Sofian, Hazrina and Md Yunus, Nur Arzilawati and Binti Ahmad, Rodina Binti},
	title = {Systematic Mapping: Artificial Intelligence Techniques in Software Engineering},
	year = {2022},
	journal = {IEEE Access},
	volume = {10},
	pages = {51021 - 51040},
	doi = {10.1109/ACCESS.2022.3174115},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130778732&doi=10.1109%2FACCESS.2022.3174115&partnerID=40&md5=eb06dfe38280332529b1ed6460361661},
	abstract = {Artificial Intelligence (AI) has become a core feature of today's real-world applications, making it a trending topic within the software engineering (SE) community. The rise in the availability of AI techniques encompasses the capability to make rapid, automated, impactful decisions and predictions, leading to the adoption of AI techniques in SE. With industry revolution 4.0, the role of software engineering has become critical for developing productive, efficient, and quality software. Thus, there is a major need for AI techniques to be applied to enhance and improve the critical activities within the software engineering phases. Software is developed through intelligent software engineering phases. This paper concerns a systematic mapping study that aimed to characterize the publication landscape of AI techniques in software engineering. Gaps are identified and discussed by mapping these AI techniques against the SE phases to which they contributed. Many systematic mapping review papers have been produced only for a specific AI technique or a specific SE phase or activity. Hence, to our best of knowledge within the last decade, there is no systematic mapping review that has fully explored the overall trends in AI techniques and their application to all SE phases. © 2013 IEEE.},
	author_keywords = {Analysis and design; Artificial intelligence; Data mining; Deep learning; Machine learning; Requirements engineering; Software deployment; Software development; Software engineering; Software maintenance; Software testing},
	keywords = {Application programs; Computer software maintenance; Deep learning; Mapping; Requirements engineering; Software design; Software testing; Analyse and design; Machine-learning; Predictive models; Requirement engineering; Software; Software deployment; Software development; Software testings; Systematic; Data mining},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 35; All Open Access; Gold Open Access}
}

@ARTICLE{Gupt202238694,
	author = {Gupt, Krishn Kumar and Youssef, Ayman and Murphy, Aidan and Raja, Muhammad Adil and Ryan, Conor M.},
	title = {GELAB - The Cutting Edge of Grammatical Evolution},
	year = {2022},
	journal = {IEEE Access},
	volume = {10},
	pages = {38694 - 38708},
	doi = {10.1109/ACCESS.2022.3166115},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128281046&doi=10.1109%2FACCESS.2022.3166115&partnerID=40&md5=06ac0bc596683260c07b17336f13d893},
	abstract = {The advent of cloud-based super-computing platforms has given rise to a Data Science (DS) boom. Many types of technological problems that were once considered prohibitively expensive to tackle are now candidates for exploration. Machine Learning (ML) tools that were valued only in academic environments are quickly being embraced by industrial giants and tiny startups alike. Coupled with modern-day computing power, ML tools can be looked at as hammers that can deal with even the most stubborn nails. ML tools have become so ubiquitous that the current industrial expectation is that they should not only deliver accurate and intelligent solutions but also do so rapidly. In order to keep pace with these requirements, a new enterprise, referred to as MLOps has blossomed in recent years. MLOps combines the process of ML and DS with an agile software engineering technique to develop and deliver solutions in a fast and iterative way. One of the key challenges to this is that ML and DS tools should be efficient and have better usability characteristics than were traditionally offered. In this paper, we present a novel software for Grammatical Evolution (GE) that meets both of these expectations. Our tool, GELAB, is a toolbox for GE in Matlab which has numerous features that distinguish it from existing contemporary GE software. Firstly, it is user-friendly and its development was aimed for use by non-specialists. Secondly, it is capable of hybrid optimization, in which standard numerical optimization techniques can be added to GE. We have shown experimentally that when hybridized with meta-heuristics GELAB has an overall better performance as compared with standard GE. © 2013 IEEE.},
	author_keywords = {diversity; Grammatical evolution; hybrid optimization},
	keywords = {Computational grammars; High level languages; Iterative methods; MATLAB; Optimization; Diversity; Germaniums (Ge); Grammar; Grammatical evolution; Hybrid optimization; Learning tool; Machine-learning; Matlab; Optimisations; Software; Germanium},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Al-Ahmad2022,
	author = {Al-Ahmad, Bilal I. and Al-Zoubi, Ala’ M. and Kabir, Md Faisal and Al-Tawil, Marwan and Aljarah, Ibrahim},
	title = {Swarm intelligence-based model for improving prediction performance of low-expectation teams in educational software engineering projects},
	year = {2022},
	journal = {PeerJ Computer Science},
	volume = {8},
	pages = {},
	doi = {10.7717/PEERJ-CS.857},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126341741&doi=10.7717%2FPEERJ-CS.857&partnerID=40&md5=d9d8329e1003ee4e1dd5ab805c2c7fe8},
	abstract = {Software engineering is one of the most significant areas, which extensively used in educational and industrial fields. Software engineering education plays an essential role in keeping students up to date with software technologies, products, and processes that are commonly applied in the software industry. The software development project is one of the most important parts of the software engineering course, because it covers the practical side of the course. This type of project helps strengthening students’ skills to collaborate in a team spirit to work on software projects. Software project involves the composition of software product and process parts. Software product part represents software deliverables at each phase of Software Development Life Cycle (SDLC) while software process part captures team activities and behaviors during SDLC. The low-expectation teams face challenges during different stages of software project. Consequently, predicting performance of such teams is one of the most important tasks for learning process in software engineering education. The early prediction of performance for low-expectation teams would help instructors to address difficulties and challenges related to such teams at earliest possible phases of software project to avoid project failure. Several studies attempted to early predict the performance for low-expectation teams at different phases of SDLC. This study introduces swarm intelligence -based model which essentially aims to improve the prediction performance for low-expectation teams at earliest possible phases of SDLC by implementing Particle Swarm Optimization-K Nearest Neighbours (PSO-KNN), and it attempts to reduce the number of selected software product and process features to reach higher accuracy with identifying less than 40 relevant features. Experiments were conducted on the Software Engineering Team Assessment and Prediction (SETAP) project dataset. The proposed model was compared with the related studies and the state-of-the-art Machine Learning (ML) classifiers: Sequential Minimal Optimization (SMO), Simple Linear Regression (SLR), Naïve Bayes (NB), Multilayer Perceptron (MLP), standard KNN, and J48. The proposed model provides superior results compared to the traditional ML classifiers and state-of-the-art studies in the investigated phases of software product and process development. © 2022 Al-Ahmad et al.},
	author_keywords = {Artificial intelligence; Data mining; Machine learning; Optimization; Pso; Software engineering},
	keywords = {Curricula; Education computing; Engineering education; Forecasting; Life cycle; Machine learning; Nearest neighbor search; Particle swarm optimization (PSO); Software design; Students; Swarm intelligence; Optimisations; Performance; Prediction performance; Pso; Software development life-cycle; Software engineering education; Software process; Software products; Software project; State of the art; Data mining},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Linåker202231,
	author = {Linåker, Johan and Runeson, Per and Zuiderwijk, Anneke M.G. and Brock, Amanda},
	title = {Collaborative Aspects of Open Data in Software Engineering},
	year = {2022},
	journal = {IEEE Software},
	volume = {39},
	number = {1},
	pages = {31 - 35},
	doi = {10.1109/MS.2021.3118123},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122445053&doi=10.1109%2FMS.2021.3118123&partnerID=40&md5=7f772c7da1b55ebf4665c7c58222df90},
	abstract = {Engineers require high-quality data for the design and implementation of today's software, especially in the context of machine learning (ML). This puts an emphasis on the need for the publication and sharing of data from and between organizations, public as well as private. Following the paradigm of open innovation, open data provide a mechanism to increase the availability of information, offering utility and improving innovation and user choice through the inevitable interoperability this enables. © 2021 IEEE.},
	keywords = {Software engineering; Design and implementations; High quality data; Open datum; Open innovation; Open Data},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Li2022,
	author = {Li, Bowen and Peng, Xin and Xiang, Qilin and Wang, Hanzhang and Xie, Tao and Sun, Jun and Liu, Xuanzhe},
	title = {Enjoy your observability: an industrial survey of microservice tracing and analysis},
	year = {2022},
	journal = {Empirical Software Engineering},
	volume = {27},
	number = {1},
	pages = {},
	doi = {10.1007/s10664-021-10063-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120162918&doi=10.1007%2Fs10664-021-10063-9&partnerID=40&md5=c6ed8414ddc49c3b182d500d80524db1},
	abstract = {Microservice systems are often deployed in complex cloud-based environments and may involve a large number of service instances being dynamically created and destroyed. It is thus essential to ensure observability to understand these microservice systems’ behaviors and troubleshoot their problems. As an important means to achieve the observability, distributed tracing and analysis is known to be challenging. While many companies have started implementing distributed tracing and analysis for microservice systems, it is not clear whether existing approaches fulfill the required observability. In this article, we present our industrial survey on microservice tracing and analysis through interviewing developers and operation engineers of microservice systems from ten companies. Our survey results offer a number of findings. For example, large microservice systems commonly adopt a tracing and analysis pipeline, and the implementations of the pipeline in different companies reflect different tradeoffs among a variety of concerns. Visualization and statistic-based metrics are the most common means for trace analysis, while more advanced analysis techniques such as machine learning and data mining are seldom used. Microservice tracing and analysis is a new big data problem for software engineering, and its practices breed new challenges and opportunities. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Industrial survey; Logging; Microservice; Tracing},
	keywords = {Data mining; Data visualization; Pipelines; Software engineering; Surveys; Cloud-based; Distributed analysis; Distributed tracing; Industrial surveys; Microservice; Number of services; Service instances; System behaviors; Tracing; Troubleshoots; Observability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 70; All Open Access; Bronze Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Myllyaho2022,
	author = {Myllyaho, Lalli and Raatikainen, Mikko and Männistö, Tomi and Nurminen, Jukka K. and Mikkonen, Tommi},
	title = {On misbehaviour and fault tolerance in machine learning systems},
	year = {2022},
	journal = {Journal of Systems and Software},
	volume = {183},
	pages = {},
	doi = {10.1016/j.jss.2021.111096},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116351574&doi=10.1016%2Fj.jss.2021.111096&partnerID=40&md5=f4afa9392f91910629f11073599ba796},
	abstract = {Machine learning (ML) provides us with numerous opportunities, allowing ML systems to adapt to new situations and contexts. At the same time, this adaptability raises uncertainties concerning the run-time product quality or dependability, such as reliability and security, of these systems. Systems can be tested and monitored, but this does not provide protection against faults and failures in adapted ML systems themselves. We studied software designs that aim at introducing fault tolerance in ML systems so that possible problems in ML components of the systems can be avoided. The research was conducted as a case study, and its data was collected through five semi-structured interviews with experienced software architects. We present a conceptualisation of the misbehaviour of ML systems, the perceived role of fault tolerance, and the designs used. Common patterns to incorporating ML components in design in a fault tolerant fashion have started to emerge. ML models are, for example, guarded by monitoring the inputs and their distribution, and enforcing business rules on acceptable outputs. Multiple, specialised ML models are used to adapt to the variations and changes in the surrounding world, and simpler fall-over techniques like default outputs are put in place to have systems up and running in the face of problems. However, the general role of these patterns is not widely acknowledged. This is mainly due to the relative immaturity of using ML as part of a complete software system: the field still lacks established frameworks and practices beyond training to implement, operate, and maintain the software that utilises ML. ML software engineering needs further analysis and development on all fronts. © 2021 The Author(s)},
	author_keywords = {Case study; Fault tolerance; Machine learning; Software architecture; Software engineering},
	keywords = {Machine learning; Software architecture; Software design; Case-studies; ITS data; Machine learning models; Machine learning systems; Misbehaviour; Product dependability; Products quality; Runtimes; Semi structured interviews; Uncertainty; Fault tolerance},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 24; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Miller2021,
	author = {Miller, Shane and Curran, Kevin J. and Lunney, Tom F.},
	title = {Identifying the use of anonymising proxies to conceal source IP addresses},
	year = {2021},
	journal = {International Journal of Digital Crime and Forensics},
	volume = {13},
	number = {6},
	pages = {},
	doi = {10.4018/IJDCF.20211101.oa8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111461805&doi=10.4018%2FIJDCF.20211101.oa8&partnerID=40&md5=dcfd220b8d28fcd15f69fd5d7fb949bb},
	abstract = {The detection of unauthorised users can be problematic for techniques that are available at present if the nefarious actors are using identity hiding tools such as anonymising proxies or virtual private networks (VPNs). This work presents computational models to address the limitations currently experienced in detecting VPN traffic. The experiments conducted to classify OpenVPN usage found that the neural network was able to correctly identify the VPN traffic with an overall accuracy of 93.71%. These results demonstrate a significant advancement in the detection of unauthorised user access with evidence showing that there could be further advances for research in this field particularly in the application of business security where the detection of VPN usage is important to an organization. © 2021 IGI Global.},
	author_keywords = {Anonymous proxies; Machine learning; Security; Virtual private networks; VPNs},
	keywords = {Computer crime; Software engineering; Business security; Computational model; IP addresss; Overall accuracies; User access; Virtual private networks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Haakman2021,
	author = {Haakman, Mark and Cruz, Luis and Huijgens, Hennie and Van Deursen, Arie Van},
	title = {AI lifecycle models need to be revised: An exploratory study in Fintech},
	year = {2021},
	journal = {Empirical Software Engineering},
	volume = {26},
	number = {5},
	pages = {},
	doi = {10.1007/s10664-021-09993-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109589558&doi=10.1007%2Fs10664-021-09993-1&partnerID=40&md5=081f0e98fb3e7426da22538a4efcbf24},
	abstract = {Tech-leading organizations are embracing the forthcoming artificial intelligence revolution. Intelligent systems are replacing and cooperating with traditional software components. Thus, the same development processes and standards in software engineering ought to be complied in artificial intelligence systems. This study aims to understand the processes by which artificial intelligence-based systems are developed and how state-of-the-art lifecycle models fit the current needs of the industry. We conducted an exploratory case study at ING, a global bank with a strong European base. We interviewed 17 people with different roles and from different departments within the organization. We have found that the following stages have been overlooked by previous lifecycle models: data collection, feasibility study, documentation, model monitoring, and model risk assessment. Our work shows that the real challenges of applying Machine Learning go much beyond sophisticated learning algorithms – more focus is needed on the entire lifecycle. In particular, regardless of the existing development tools for Machine Learning, we observe that they are still not meeting the particularities of this field. © 2021, The Author(s).},
	author_keywords = {AI engineering; AI lifecycle; Case study; Machine learning; SE4AI},
	keywords = {Fintech; Intelligent systems; Life cycle; Machine learning; Risk assessment; Software engineering; Artificial intelligence systems; Development process; Development tools; Entire life cycles; Exploratory case studies; Exploratory studies; Feasibility studies; Software component; Learning algorithms},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 86; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Biesialska2021,
	author = {Biesialska, Katarzyna and Franch, Xavier and Muntés-Mulero, Víctor},
	title = {Big Data analytics in Agile software development: A systematic mapping study},
	year = {2021},
	journal = {Information and Software Technology},
	volume = {132},
	pages = {},
	doi = {10.1016/j.infsof.2020.106448},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095581407&doi=10.1016%2Fj.infsof.2020.106448&partnerID=40&md5=4b04b09e0f1581676e6584a058f9226d},
	abstract = {Context: Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity. Objective: Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA). Method: As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019. Results: In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics. Conclusions: As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives. © 2020 Elsevier B.V.},
	author_keywords = {Agile software development; Artificial intelligence; Data analytics; Literature review; Machine learning; Software analytics},
	keywords = {Advanced Analytics; Application programs; Big data; Data Analytics; Life cycle; Project management; Software testing; Agile software development; Business environments; Development cycle; Literature reviews; Software artifacts; Software development process; Software development teams; Systematic mapping studies; Software design},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 43; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Lekschas2021358,
	author = {Lekschas, Fritz and Zhou, Xinyi and Chen, Wei and Gehlenborg, Nils and Bach, Benjamin and Pfister, Hanspeter},
	title = {A Generic Framework and Library for Exploration of Small Multiples through Interactive Piling},
	year = {2021},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	volume = {27},
	number = {2},
	pages = {358 - 368},
	doi = {10.1109/TVCG.2020.3028948},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100339480&doi=10.1109%2FTVCG.2020.3028948&partnerID=40&md5=360d9569a9ff755e109f02d0ace781e0},
	abstract = {Small multiples are miniature representations of visual information used generically across many domains. Handling large numbers of small multiples imposes challenges on many analytic tasks like inspection, comparison, navigation, or annotation. To address these challenges, we developed a framework and implemented a library called PILlNG.JS for designing interactive piling interfaces. Based on the piling metaphor, such interfaces afford flexible organization, exploration, and comparison of large numbers of small multiples by interactively aggregating visual objects into piles. Based on a systematic analysis of previous work, we present a structured design space to guide the design of visual piling interfaces. To enable designers to efficiently build their own visual piling interfaces, PILlNG.JS provides a declarative interface to avoid having to write low-level code and implements common aspects of the design space. An accompanying GUI additionally supports the dynamic configuration of the piling interface. We demonstrate the expressiveness of PILlNG.JS with examples from machine learning, immunofluorescence microscopy, genomics, and public health. © 1995-2012 IEEE.},
	author_keywords = {Information visualization; interactive piling; small multiples; spatial organization; visual aggregation},
	keywords = {Computer graphics; Software engineering; Dynamic configuration; Generic frameworks; Immunofluorescence microscopy; Small multiples; Structured design; Systematic analysis; Visual information; Visual objects; Firmware; computer graphics; genomics; Computer Graphics; Genomics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Yadav202168,
	author = {Yadav, Vinod and Botchway, Raphael Kwaku and Senkerik, Roman and Komínková Oplatková, Zuzana Komínková},
	title = {Robotic Automation of Software Testing From a Machine Learning Viewpoint},
	year = {2021},
	journal = {Mendel},
	volume = {27},
	number = {2},
	pages = {68 - 73},
	doi = {10.13164/mendel.2021.2.068},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123604331&doi=10.13164%2Fmendel.2021.2.068&partnerID=40&md5=e550195dc48614724748877de3af4144},
	abstract = {The need to scale software test automation while managing the test automation process within a reasonable time frame remains a crucial challenge for software development teams (DevOps). Unlike hardware, the software cannot wear out but can fail to satisfy the functional requirements it is supposed to meet due to the defects observed during system operation. In this era of big data, DevOps teams can deliver better and efficient code by utilizing machine learning (ML) to scan their new codes and identify test coverage gaps. While still in its infancy, the inclusion of ML in software testing is a reality and requirement for coming industry demands. This study introduces the prospects of robot testing and machine learning to manage the test automation process to guarantee software reliability and quality within a reasonable timeframe. Although this paper does not provide any particular demonstration of ML-based technique and numerical results from MLbased algorithms, it describes the motivation, possibilities, tools, components, and examples required for understanding and implementing the robot test automation process approach. © 2021, Brno University of Technology. All rights reserved.},
	author_keywords = {Automation; Big data; Machine learning; Robotic software testing; Software reliability; Test automation},
	keywords = {Big data; Machine learning; Robotics; Software design; Software reliability; Automation process; Machine-learning; Robotic automation; Robotic software testing; Robotic softwares; Software test automation; Software testings; Software-Reliability; Test Automation; Time frame; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Ferreira2021138618,
	author = {Ferreira, Isaac C. and Aragão, Marcelo V.C. and Oliveira, Edvard Martins De and Tardiole Kuehne, Bruno and Moreira, Edmilson M. and Carpinteiro, Otávio Augusto Salgado},
	title = {The Development of the Open Machine-Learning-Based Anti-Spam (Open-MaLBAS)},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {138618 - 138632},
	doi = {10.1109/ACCESS.2021.3118901},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117470392&doi=10.1109%2FACCESS.2021.3118901&partnerID=40&md5=2cf02c938f09885a136314fcf32ee520},
	abstract = {Spam e-mails are unsolicited e-mails received by users of the e-mail service. Spam e-mails cause serious harm to organizations, for they waste, among other things, their computational and networking resources. To reduce the damage caused by them, organizations use anti-spams. Anti-spams are software systems that classify e-mails in order to separate legitimate from spam e-mails. The best current commercial and open-source anti-spams, and in particular the well-known commercial anti-spam CanIt-PRO, make use of various techniques, such as blacklists and/or SMTP extensions, to classify e-mails. Unfortunately, both blacklists and SMTP extensions have serious drawbacks, such as low scalability and high computational and network costs. This paper introduces the Open Machine-Learning-Based Anti-Spam (Open-MaLBAS). Unlike the best current anti-spams, Open-MaLBAS does not make use of blacklists and SMTP extensions, but only of machine learning models for e-mail classification. Open-MaLBAS was compared to CanIt-PRO in a series of experiments on a database composed of 862,227 real e-mails, collected over three months at the Federal University of Itajubá, Brazil. The e-mails were previously classified by CanIt-PRO. From the experiments, it was observed that Open-MaLBAS was able to correctly classify 81.48% and 98.13% of the e-mails in the database, using, respectively, the two models-Multi-Layer Perceptron and Random Forest-evaluated. In addition, it managed to obtain times of up to 88% shorter than those of CanIt-PRO to classify all e-mails in the database. Open-MaLBAS is implemented in Java language, under free software license, for free use. It is available on GitHub. © 2013 IEEE.},
	author_keywords = {Electronic mail (e-mail); internet; machine learning; network security; open source software; simple mail transfer protocol (SMTP); software engineering; unsolicited electronic mail (spam)},
	keywords = {Classification (of information); Database systems; Decision trees; E-learning; Electronic mail; Internet protocols; Machine learning; Open source software; Open systems; Anti-spam; Machine-learning; Networks security; Simple mail transfer protocol; Simple++; Spam e-mails; Transfer protocol; Unsolicited electronic mail (spam); Unsolicited electronic mails; Network security},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Abu-Salih2021208,
	author = {Abu-Salih, Bilal and Alsawalqah, Hamad Iqab and Elshqeirat, Basima and Issa, Tomayess B.T. and Wongthongtham, Pornpit and Premi, Khadija Khalid},
	title = {Toward a knowledge-based personalised recommender system for mobile app development},
	year = {2021},
	journal = {Journal of Universal Computer Science},
	volume = {27},
	number = {2},
	pages = {208 - 229},
	doi = {10.3897/jucs.65096},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103271428&doi=10.3897%2Fjucs.65096&partnerID=40&md5=e7fe2718f907cbae30466407acbc5740},
	abstract = {Over the last few years, the arena of mobile application development has expanded considerably beyond the demand of the world's software markets. With the growing number of mobile software companies and the increasing sophistication of smartphone technology, developers have been establishing several categories of applications on dissimilar platforms. However, developers confront several challenges when undertaking mobile application projects. In particular, there is a lack of consolidated systems that can competently, promptly and efficiently provide developers with personalised services. Hence, it is essential to develop tailored systems that can recommend appropriate tools, IDEs, platforms, software components and other correlated artifacts to mobile application developers. This paper proposes a new recommender system framework comprising a robust set of techniques that are designed to provide mobile app developers with a specific platform where they can browse and search for personalised artifacts. In particular, the new recommender system framework comprises the following functions: (i) domain knowledge inference module: including various semantic web technologies and lightweight ontologies; (ii) profiling and preferencing: a new proposed time-aware multidimensional user modelling; (iii) query expansion: to improve and enhance the retrieved results by semantically augmenting users’ query; and (iv) recommendation and information filtration: to make use of the aforementioned components to provide personalised services to the designated users and to answer a user’s query with the minimum mismatches. © 2021, IICM. All rights reserved.},
	author_keywords = {Machine Learning; Mobile App Development; Recommender Systems; Semantic Analytics; Software Engineering; User Profiling},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Pradhan2021,
	author = {Pradhan, Satya and Nanniyur, Venky},
	title = {Large scale quality transformation in hybrid development organizations – A case study},
	year = {2021},
	journal = {Journal of Systems and Software},
	volume = {171},
	pages = {},
	doi = {10.1016/j.jss.2020.110836},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092034144&doi=10.1016%2Fj.jss.2020.110836&partnerID=40&md5=d3052172701a99a16dedb5fe4235214e},
	abstract = {As the software industry transitions to a subscription-based software-as-a-service (SaaS) model, software development companies are transforming to hybrid development organizations with increased adoption of Agile and Continuous Integration/ Continuous Delivery (CI/CD) development practices for newer products while continuing to use Waterfall methods for older products. This transformation is a huge undertaking impacting all aspects of the software development life cycle (SDLC), including the quality management system. This paper presents a case study of a large-scale transformation of a legacy quality management system to a modern system developed and implemented at Cisco Systems. The framework for this transformation is defined by six distinct areas: metrics, process, measurement, reporting, quality analytics, and culture & leadership. Our implementation leveraged recent advances in Machine Learning (ML), Artificial Intelligence (AI), connected data, integrated operations, and big data technologies to solve the challenges created by a hybrid software development organization. We believe this case study will help researchers and industry leaders understand the benefits and potential challenges of such sizeable transformations. © 2020 The Authors},
	author_keywords = {Agile; Hybrid development organization; Quality management system; Quality transformation; Waterfall},
	keywords = {Artificial intelligence; Computer software; Legacy systems; Life cycle; Quality management; Service industry; Software design; Continuous integrations; Development practices; Integrated Operations; Quality management systems; Scale transformation; Software development life cycle; Software development organizations; Waterfall methods; Software as a service (SaaS)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access; Hybrid Gold Open Access}
}

