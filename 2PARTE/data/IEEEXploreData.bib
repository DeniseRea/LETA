@ARTICLE{10904141,
  author={Haldar, Susmita and Pierce, Mary and Fernando Capretz, Luiz},
  journal={IEEE Access}, 
  title={Exploring the Integration of Generative AI Tools in Software Testing Education: A Case Study on ChatGPT and Copilot for Preparatory Testing Artifacts in Postgraduate Learning}, 
  year={2025},
  volume={13},
  number={},
  pages={46070-46090},
  abstract={Software testing education is important for building qualified testing professionals. To ensure that software testing graduates are ready for real-world challenges, it is necessary to integrate modern tools and technologies into the curriculum. With the emergence of Large Language Models (LLMs), their potential use in software engineering has become a focus, but their application in software testing education remains largely unexplored. This study, conducted in the Capstone Project course of a postgraduate software testing program, was carried out over two semesters with two distinct groups of students. A custom-built Travel Application limited to a web platform was used in the first semester. In the second semester, a new set of students worked with an open-source application, offering a larger-scale, multi-platform experience across web, desktop, and mobile platforms. Students initially created preparatory testing artifacts manually as a group deliverable. Following this, they were assigned an individual assignment to generate the same artifacts using LLM tools such as ChatGPT 3.5 in the first semester and Microsoft Copilot in the second. This process directly compared manually created artifacts and those generated using LLMs, leveraging AI for faster outputs. After completion, they responded to a set of assigned questions. The students’ responses were assessed using an integrated methodology, including quantitative and qualitative assessments, sentiment analysis to understand emotions, and a thematic approach to extract deeper insights. The findings revealed that while LLMs can assist and augment manual testing efforts, they cannot entirely replace the need for manual testing. By incorporating innovative technology into the curriculum, this study highlights how Generative AI can support active learning, connect theoretical concepts with practical applications, and align educational practices with industry needs.},
  keywords={Software testing;Education;Generative AI;Industries;Chatbots;Software engineering;Sentiment analysis;Large language models;Accuracy;Systematic literature review;Capstone project;ChatGPT;generative AI;software testing education;Microsoft Copilot;sentiment analysis},
  doi={10.1109/ACCESS.2025.3545882},
  ISSN={2169-3536},
  month={},}@ARTICLE{10735191,
  author={Baralla, Gavina and Ibba, Giacomo and Tonelli, Roberto},
  journal={IEEE Access}, 
  title={Assessing GitHub Copilot in Solidity Development: Capabilities, Testing, and Bug Fixing}, 
  year={2024},
  volume={12},
  number={},
  pages={164389-164411},
  abstract={In the rapidly evolving landscape of blockchain technology, the development of reliable and secure smart contracts represents one of several crucial challenges. GitHub Copilot, an AI-powered code assistant, aims to enhance developer productivity by generating code snippets, facilitating testing, and assisting in program repair. This research examines Copilot’s proficiency in generating functional and secure smart contracts, including token creation adhering to standards such as ERC20, ERC721, and ERC1155 with various optional features. Additionally, the study assesses its effectiveness in common development tasks, including the implementation of widely employed libraries such as SafeMath. Through controlled experiments, the accuracy, efficiency, and security of the code generated by Copilot are evaluated. This evaluation identifies both its strengths in expediting the development process and its limitations in managing complex blockchain-specific logic and security considerations. The findings contribute to an expanded understanding of the role of AI-assisted programming in blockchain development, offering insights into how developers can best leverage such tools in creating and testing smart contracts. This research aims to guide both practitioners and researchers in the blockchain domain, advancing the discussion on integrating AI into software development workflows in the context of Solidity and smart contract development, underscoring the need for further research to address the challenges and opportunities presented by AI in blockchain technology.},
  keywords={Codes;Software development management;Smart contracts;Programming;Artificial intelligence;Security;Python;Encoding;Computer languages;Complexity theory;GitHub copilot;solidity;smart contract;AI;bug fixing;vulnerability},
  doi={10.1109/ACCESS.2024.3486365},
  ISSN={2169-3536},
  month={},}@ARTICLE{10646341,
  author={Mejía, Jezreel and Terrón-Macias, Victor and Muñoz, Mirna and Terrón-Hernández, Miguel and Canseco-Pérez, Miguel},
  journal={IEEE Access}, 
  title={VSEST 29110 Tool: Using ChatGPT to Evaluate the Implementation of the ISO/IEC 29110 Work Products}, 
  year={2024},
  volume={12},
  number={},
  pages={120935-120948},
  abstract={The global software industry is predominantly composed of micro, small, and medium-sized enterprises (MSMEs), highlighting the need for software quality management to ensure the proper functioning and quality of the software. This research focuses on the evaluation of the implementation of the ISO/IEC 29110 standard work products, which is a standard tailored by the ISO/IEC specifically for MSMEs, which improves the software development process by implementing two processes in its basic profile: Project Management (PM) and Software Implementation (SI). Despite this standard being tailored specifically for this type of enterprise, implementing ISO/IEC 29110 faces several challenges, such as a lack of knowledge and difficulties in adequately implementing the work products regarding the compliance of standard criteria, among others. To address these challenges, we introduce VSEST 29110, a web tool designed to evaluate the ISO/IEC 29110 standard implementation work products by leveraging Artificial Intelligence (AI) technologies, specifically the ChatGPT model, provide detailed feedback on compliance with standard criteria, offer suggestions for improvement based on ChatGPT analysis and streamline the implementation process for MSMEs. To achieve this, our research incorporates a systematic literature review and validation through a case study by document analysis, demonstrating VSEST 29110’s effectiveness in enhancing compliance and providing comprehensive feedback compared to auditor recommendations, which impacts 69.33% on average.},
  keywords={ISO Standards;IEC Standards;Artificial intelligence;Software development management;Chatbots;Text analysis;ISO/IEC 29110 standard;LLMs;ChatGPT;implementation process improvement},
  doi={10.1109/ACCESS.2024.3449252},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10548766,
  author={Dolata, Mateusz and Lange, Norbert and Schwabe, Gerhard},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)}, 
  title={Development in Times of Hype: How Freelancers Explore Generative AI?}, 
  year={2024},
  volume={},
  number={},
  pages={2257-2269},
  abstract={The rise of generative AI has led many companies to hire freelanc-ers to harness its potential. However, this technology presents unique challenges to developers who have not previously engaged with it. Freelancers may find these challenges daunting due to the absence of organizational support and their reliance on positive client feedback. In a study involving 52 freelance developers, we identified multiple challenges associated with developing solutions based on generative AI. Freelancers often struggle with as-pects they perceive as unique to generative AI such as unpredict-ability of its output, the occurrence of hallucinations, and the in-consistent effort required due to trial-and-error prompting cycles. Further, the limitations of specific frameworks, such as token lim-its and long response times, add to the complexity. Hype-related issues, such as inflated client expectations and a rapidly evolving technological ecosystem, further exacerbate the difficulties. To address these issues, we propose Software Engineering for Generative AI (SE4GenAI) and Hype-Induced Software Engineering (HypeSE) as areas where the software engineering community can provide effective guidance. This support is essential for freelancers working with generative AI and other emerging technologies.},
  keywords={Generative AI;Ecosystems;Companies;Complexity theory;Time factors;Software engineering;Generative AI;AI-based Systems;Challenges;Freelancers;Hype;SE for Generative AI;SE4GenAI;Hype-Induced SE;Hype-SE;Fashion;Product;Paradigm;Novelty;Qualitative Research},
  doi={10.1145/3597503.3639111},
  ISSN={1558-1225},
  month={April},}@ARTICLE{10812696,
  author={Nettur, Suresh Babu and Karpurapu, Shanthi and Nettur, Unnati and Gajja, Likhit Sagar},
  journal={IEEE Access}, 
  title={Cypress Copilot: Development of an AI Assistant for Boosting Productivity and Transforming Web Application Testing}, 
  year={2025},
  volume={13},
  number={},
  pages={3215-3229},
  abstract={In today’s fast-paced software development environment, Agile methodologies demand rapid delivery and continuous improvement, making automated testing essential for maintaining quality and accelerating feedback loops. Our study addresses the challenges of developing and maintaining automation code for web-based application testing. In this paper, we propose a novel approach that leverages large language models (LLMs) and a novel prompt technique, few-shot chain, to automate code generation for web application testing. We chose the Behavior-Driven Development (BDD) methodology owing to its advantages and selected the Cypress tool for automating web application testing, as it is one of the most popular and rapidly growing frameworks in this domain. We comprehensively evaluated various OpenAI models, including GPT-4-Turbo, GPT-4o, and GitHub Copilot, using zero-shot and few-shot chain prompt techniques. Furthermore, we extensively validated with a vast set of test cases to identify the optimal approach. Our results indicate that the Cypress automation code generated by GPT-4o using a few-shot chained prompt approach excels in generating complete code for each test case, with fewer empty methods and improved syntactical accuracy and maintainability. Based on these findings, we developed a novel open-source Visual Studio Code (IDE) extension, “Cypress Copilot” utilizing GPT-4o and a few-shot chain prompt technique, which has shown promising results. Finally, we validate the Cypress Copilot tool by generating automation code for end-to-end web tests, demonstrating its effectiveness in testing various web applications and its ability to streamline development processes. More importantly, we are releasing this tool to the open-source community, as it has the potential to be a promising partner in enhancing productivity in web application automation testing.},
  keywords={Automation;Codes;Testing;Graphical user interfaces;Stakeholders;Productivity;Java;Business;Accuracy;Visualization;Agile software development;behavior driven development;large language model;machine learning;prompt engineering;software testing;cypress;selenium;web application;AI assistant tools;GitHub Copilot;code generation;test case generation;test automation;zero-shot;few-shot;OpenAI;GPT-3;GPT3.5;GPT-4;GPT-4o},
  doi={10.1109/ACCESS.2024.3521407},
  ISSN={2169-3536},
  month={},}@ARTICLE{10870152,
  author={Almanasra, Sally and Suwais, Khaled},
  journal={IEEE Access}, 
  title={Analysis of ChatGPT-Generated Codes Across Multiple Programming Languages}, 
  year={2025},
  volume={13},
  number={},
  pages={23580-23596},
  abstract={Our research focuses on the intersection of artificial intelligence (AI) and software development, particularly the role of AI models in automating code generation. With advancements in large language models like ChatGPT, developers can now generate code from natural language prompts, a task that traditionally required significant manual input and expertise. AI-generated code promises to boost productivity by enabling faster prototyping and automating repetitive coding tasks. However, as these models are increasingly adopted in real-world applications, questions surrounding their efficiency and code quality become critical. This research investigates ChatGPT-4o, a state-of-the-art language model, and its ability to generate functional, high-quality code in different programming languages. By comparing performance between Python and Java, the study seeks to shed light on AI’s capabilities and limitations in code generation, addressing not only functional correctness but also broader software engineering concerns such as memory usage, runtime efficiency, and maintainability. The study addresses key questions related to the performance, code quality, and error management of AI-generated code by analyzing solutions for 300 data structure problems and 300 problems from the LeetCode platform. The findings reveal notable performance differences between the two languages: Java demonstrated superior runtime performance, particularly for medium and hard problems, while Python exhibited better memory efficiency across all complexity levels. The research also highlighted significant gaps in code quality, with both languages showing deficiencies in documentation and exception management. This study contributes to the literature by offering a comprehensive cross-language analysis of ChatGPT-4o’s programming capabilities, addressing a gap in the evaluation of AI-generated code performance.},
  keywords={Codes;Programming;Encoding;Chatbots;Runtime;Python;Software development management;Java;Complexity theory;Standards;Artificial intelligence;software development;ChatGPT;large language model;programming},
  doi={10.1109/ACCESS.2025.3538050},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10647145,
  author={Shome, Arumoy and Cruz, Luìs and Van Deursen, Arie},
  booktitle={2024 IEEE/ACM International Workshop on Natural Language-Based Software Engineering (NLBSE)}, 
  title={Towards Automatic Translation of Machine Learning Visual Insights to Analytical Assertions}, 
  year={2024},
  volume={},
  number={},
  pages={29-32},
  abstract={We present our vision for developing an automated tool capable of translating visual properties observed in Machine Learning (ML) visualisations into Python assertions. The tool aims to streamline the process of manually verifying these visualisations in the ML development cycle, which is critical as real-world data and assumptions often change post-deployment. In a prior study, we mined 54, 070 Jupyter notebooks from Github and created a catalogue of 269 semantically related visualisation-assertion (VA) pairs. Building on this catalogue, we propose to build a taxonomy that organises the VA pairs based on ML verification tasks. The input feature space comprises of a rich source of information mined from the Jupyter notebooks-visualisations, Python source code, and associated markdown text. The effectiveness of various AI models, including traditional NLP4Code models and modern Large Language Models, will be compared using established machine translation metrics and evaluated through a qualitative study with human participants. The paper also plans to address the challenge of extending the existing VA pair dataset with additional pairs from Kaggle and to compare the tool’s effectiveness with commercial generative AI models like ChatGPT. This research not only contributes to the field of ML system validation but also explores novel ways to leverage AI for automating and enhancing software engineering practices in ML.},
  keywords={Visualization;Source coding;Taxonomy;Data visualization;Machine learning;Task analysis;System validation;SE4AI;NLP4Code;ML Testing;Visualisations;Assertions;Computational Notebooks;Automated Tool},
  doi={},
  ISSN={},
  month={April},}@ARTICLE{11218044,
  author={Newcomb, Luke and Newcomb, Alexandra and Ochoa, Omar},
  journal={IEEE Access}, 
  title={Preconditions and Postconditions as Design Constraints for LLM Code Generation}, 
  year={2025},
  volume={13},
  number={},
  pages={184259-184274},
  abstract={Large Language Models (LLMs) have significantly advanced automated code generation, but current methods predominantly rely on natural language descriptions during prompting. This approach encounters challenges when handling complex, class-level software generation tasks due to inherent ambiguity and under-specification. On the other hand, few studies have investigated how formal software engineering constraints, such as explicit preconditions and postconditions as part of the Design-by-Contract paradigm, influence class-level generation tasks. This work addresses this gap through a structured evaluation of six state-of-the-art LLMs generating complete software implementations of a medium complexity system from systematically designed class-level specifications. Results demonstrate that incorporating explicit design constraints during prompting significantly boosts initial generation accuracy (measured via the pass@k metric), particularly in Python but also in C++. Models with fewer parameters saw especially pronounced benefits. These findings suggest integrating structured software engineering constraints and design principles into LLM-based code generation workflows to enhance accuracy and maintainability in automated software projects.},
  keywords={Codes;Natural languages;Benchmark testing;Software engineering;Accuracy;Software design;Complexity theory;Translation;Source coding;Software systems;Automated code generation;large language models;software design},
  doi={10.1109/ACCESS.2025.3625819},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10727139,
  author={Mishra, Shyamal and Chatterjee, Preetha},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER)}, 
  title={Exploring ChatGPT for Toxicity Detection in GitHub}, 
  year={2024},
  volume={},
  number={},
  pages={6-10},
  abstract={Fostering a collaborative and inclusive environment is crucial for the sustained progress of open source development. However, the prevalence of negative discourse, often manifested as toxic comments, poses significant challenges to developer well-being and productivity. To identify such negativity in project communications, especially within large projects, automated toxicity detection models are necessary. To train these models effectively, we need large software engineering-specific toxicity datasets. However, such datasets are limited in availability and often exhibit imbalance (e.g., only 6 in 1000 GitHub issues are toxic) [1], posing challenges for training effective toxicity detection models. To address this problem, we explore a zero-shot LLM (ChatGPT) that is pretrained on massive datasets but without being fine-tuned specifically for the task of detecting toxicity in software-related text. Our preliminary evaluation indicates that ChatGPT shows promise in detecting toxicity in GitHub, and warrants further investigation. We experimented with various prompts, including those designed for justifying model outputs, thereby enhancing model interpretability and paving the way for potential integration of ChatGPT-enabled toxicity detection into developer communication channels.},
  keywords={Training;Productivity;Toxicology;Collaboration;Communication channels;Chatbots;Software;Software development management;Software engineering;LLM;software engineering;chatgpt;incivility},
  doi={10.1145/3639476.3639777},
  ISSN={2832-7632},
  month={April},}@INPROCEEDINGS{10549231,
  author={Serafini, Raphael and Otto, Clemens and Horstmann, Stefan Albert and Naiakshina, Alena},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)}, 
  title={ChatGPT-Resistant Screening Instrument for Identifying Non-Programmers}, 
  year={2024},
  volume={},
  number={},
  pages={2231-2243},
  abstract={To ensure the validity of software engineering and IT security studies with professional programmers, it is essential to identify participants without programming skills. Existing screening questions are efficient, cheating robust, and effectively differentiate programmers from non-programmers. However, the release of ChatGPT raises concerns about their continued effectiveness in identifying non-programmers. In a simulated attack, we showed that Chat-GPT can easily solve existing screening questions. Therefore, we designed new ChatGPT-resistant screening questions using visual concepts and code comprehension tasks. We evaluated 28 screening questions in an online study with 121 participants involving programmers and non-programmers. Our results showed that questions using visualizations of well-known programming concepts performed best in differentiating between programmers and non-programmers. Participants prompted to use ChatGPT struggled to solve the tasks. They considered ChatGPT ineffective and changed their strategy after a few screening questions. In total, we present six ChatGPT-resistant screening questions that effectively identify non-programmers. We provide recommendations on setting up a ChatGPT-resistant screening instrument that takes less than three minutes to complete by excluding 99.47% of non-programmers while including 94.83% of programmers.},
  keywords={Visualization;Codes;Instruments;Switches;Chatbots;Security;Task analysis;chatgpt;programmer screening;developer study;study protection},
  doi={10.1145/3597503.3639075},
  ISSN={1558-1225},
  month={April},}@ARTICLE{9568959,
  author={Shafiq, Saad and Mashkoor, Atif and Mayr-Dorn, Christoph and Egyed, Alexander},
  journal={IEEE Access}, 
  title={A Literature Review of Using Machine Learning in Software Development Life Cycle Stages}, 
  year={2021},
  volume={9},
  number={},
  pages={140896-140920},
  abstract={The software engineering community is rapidly adopting machine learning for transitioning modern-day software towards highly intelligent and self-learning systems. However, the software engineering community is still discovering new ways how machine learning can offer help for various software development life cycle stages. In this article, we present a study on the use of machine learning across various software development life cycle stages. The overall aim of this article is to investigate the relationship between software development life cycle stages, and machine learning tools, techniques, and types. We attempt a holistic investigation in part to answer the question of whether machine learning favors certain stages and/or certain techniques.},
  keywords={Machine learning;Data mining;Tools;Support vector machines;Software testing;Software systems;Software engineering;Software engineering;machine learning;literature review},
  doi={10.1109/ACCESS.2021.3119746},
  ISSN={2169-3536},
  month={},}@ARTICLE{11269792,
  author={Krebs, Rasmus and Mazumdar, Somnath},
  journal={IEEE Access}, 
  title={Analyzing LLM-Generated Code According to Four ISO/IEC 5055:2021 Categories}, 
  year={2025},
  volume={13},
  number={},
  pages={202482-202499},
  abstract={The use of large language models (LLMs) for generating code in software development is on the rise. While LLMs demonstrate impressive capabilities, there remains a need to evaluate the quality of generated code beyond functional correctness. This paper addresses a gap in current research by evaluating Python code generated by nine state-of-the-art LLMs according to the four main code quality categories (maintainability, reliability, performance efficiency, and security) defined by the ISO/IEC 5055:2021 standard. The evaluation spans three popular application domains (high-performance computing, machine learning, and data processing). It employs a stratified prompting approach with varying levels (short, medium, and long) of detail. Using widely adopted static code analysis tools, the study identifies which LLMs perform best across these categories. The analysis involved selecting nine algorithms across three application domains, and the generated code was compared against a human developer using four static code analysis tools. Metrics were organized into the four ISO 5055 categories, and composite scores were calculated for each category following preprocessing to ensure accurate evaluation. The results showed that GPT-4-Turbo produced the most reliable, performance-efficient, and secure code, while Gemini excelled in generating maintainable Python code among the evaluated models. The study concludes that properly prompted and configured LLMs can produce code that meets or even exceeds human-developed code in terms of the four ISO categories. Future work will continue to refine and expand upon these methodologies on other programming languages.},
  keywords={Codes;ISO Standards;Reliability;Security;Python;Benchmark testing;Software reliability;IEC Standards;Source coding;Reviews;Analysis;ISO;LLMs;maintainability;performance;Python;quality;reliability;security},
  doi={10.1109/ACCESS.2025.3637569},
  ISSN={2169-3536},
  month={},}@ARTICLE{10179899,
  author={Gjorgjevikj, Ana and Mishev, Kostadin and Antovski, Ljupcho and Trajanov, Dimitar},
  journal={IEEE Access}, 
  title={Requirements Engineering in Machine Learning Projects}, 
  year={2023},
  volume={11},
  number={},
  pages={72186-72208},
  abstract={Over the last decade, machine learning methods have revolutionized a large number of domains and provided solutions to many problems that people could hardly solve in the past. The availability of large amounts of data, powerful processing architectures, and easy-to-use software frameworks have made machine learning a popular, readily available, and affordable option in many different domains and contexts. However, the development and maintenance of production-level machine learning systems have proven to be quite challenging, as these activities require an engineering approach and solid best practices. Software engineering offers a mature development process and best practices for conventional software systems, but some of them are not directly applicable to the new programming paradigm imposed by machine learning. The same applies to the requirements engineering best practices. Therefore, this article provides an overview of the requirements engineering challenges in the development of machine learning systems that have been reported in the research literature, along with their proposed solutions. Furthermore, it presents our approach to overcoming those challenges in the form of a case study. Through this mixed-method study, the article tries to identify the necessary adjustments to (1) the best practices for conventional requirements engineering and (2) the conventional understanding of certain types of requirements to better fit the specifics of machine learning. Moreover, the article tries to emphasize the relevance of properly conducted requirements engineering activities in addressing the complexity of machine learning systems, as well as to motivate further discussion on the requirements engineering best practices in developing such systems.},
  keywords={Data models;Machine learning;Artificial intelligence;Requirements engineering;Software engineering;Best practices;Requirements engineering;Machine learning;requirements engineering;software engineering;software requirements},
  doi={10.1109/ACCESS.2023.3294840},
  ISSN={2169-3536},
  month={},}@ARTICLE{9388660,
  author={Ahmed, Hafiza Anisa and Bawany, Narmeen Zakaria and Shamsi, Jawwad Ahmed},
  journal={IEEE Access}, 
  title={CaPBug-A Framework for Automatic Bug Categorization and Prioritization Using NLP and Machine Learning Algorithms}, 
  year={2021},
  volume={9},
  number={},
  pages={50496-50512},
  abstract={Bug reports facilitate software development teams in improving the quality of software. These reports include significant information related to problems encountered within a software, possible enhancement suggestions, and other potential issues. Bug reports are typically complex and are too detailed; hence a lot of resources are required to analyze and process them manually. Moreover, it leads to delays in the resolution of high priority bugs. Accurate and timely processing of bug reports based on their category and priority plays a significant role in improving the quality of software maintenance. Therefore, an automated process of categorization and prioritization of bug reports is needed to address the aforementioned issues. Automated categorization and prioritization of bug reports have been explored recently by many researchers; however, limited progress has been made in this regard. In this research, we present a novel framework, titled CaPBug, for automated categorization and prioritization of bug reports. The framework is implemented using Natural Language Processing (NLP) and supervised machine learning algorithms. A baseline corpus is built with six categories and five prioritization levels by analyzing more than 2000 bug reports of Mozilla and Eclipse repository. Four classification algorithms i.e., Naive Bayes, Random Forest, Decision Tree, and Logistic Regression have been used to categorize and prioritize bug reports. We demonstrate that the CaPBug framework achieved an accuracy of 88.78% by using a Random Forest classifier with a textual feature for predicting the category. Similarly, using the CaPBug framework, an accuracy of 90.43% was achieved in predicting the priority of bug reports. Synthetic Minority Over-Sampling Technique (SMOTE) has been applied to address the class imbalance issue in priority classes.},
  keywords={Computer bugs;Software;Machine learning algorithms;Feature extraction;Classification algorithms;Prediction algorithms;Location awareness;Bug reports;natural language processing;machine learning;bug report categorization;bug report prioritization},
  doi={10.1109/ACCESS.2021.3069248},
  ISSN={2169-3536},
  month={},}@ARTICLE{10305170,
  author={Bocu, Razvan and Baicoianu, Alexandra and Kerestely, Arpad},
  journal={IEEE Access}, 
  title={An Extended Survey Concerning the Significance of Artificial Intelligence and Machine Learning Techniques for Bug Triage and Management}, 
  year={2023},
  volume={11},
  number={},
  pages={123924-123937},
  abstract={Bug reports are generated in large numbers during the software development processes in the software industry. The manual processing of these issues is usually time consuming and prone to errors, consequently delaying the entire software development process. Thus, a properly designed bug triage and management process implies that essential operations, such as duplicate detection, bug assignments to proper developers, and determination of the importance level, are sustained by efficient algorithmic models and implementation approaches. Designing and implementing a proper bug triage and management process becomes an essential scientific research topic, as it may significantly optimize the software development and business process in the information technology industry. Consequently, this paper thoroughly surveys the most significant related scientific contributions analytically and constructively, distinguishing it from similar survey papers. The paper proposes optimal algorithmic and software solutions for particular real-world use cases that are analyzed. It concludes by presenting the most important open research questions and challenges. Additionally, the paper provides a valuable scientific literature survey for any researcher or practitioner in software bug triage and management systems based on artificial intelligence and machine learning techniques.},
  keywords={Computer bugs;Software engineering;Surveys;Databases;Machine learning;Libraries;Mathematical models;Bug report;bug prioritization;bug assignment;bug triaging;classification;machine learning},
  doi={10.1109/ACCESS.2023.3329732},
  ISSN={2169-3536},
  month={},}@ARTICLE{10870104,
  author={Verma, Amandeep and Saha, Rahul and Kumar, Gulshan and Brighente, Alessandro and Conti, Mauro and Kim, Tai-Hoon},
  journal={IEEE Access}, 
  title={Exploring the Landscape of Programming Language Identification With Machine Learning Approaches}, 
  year={2025},
  volume={13},
  number={},
  pages={23556-23579},
  abstract={The increasing complexity of modern software development necessitates tools and methodologies for code analysis, maintenance, and migration in multi-language Integrated Development Environments (IDEs). The security needs of the software development process also recently led to the introduction of the Software Bill of Materials (SBOM), which vendors must include in their products. Automated tools become imperative to speed up software production and distribution. In the present code development landscape, automated Programming Language Identification (PLI) helps to develop secure software development with its in-depth insights into programming consequences for code maintenance, legacy system management, code analysis, quality assurance, software modernization, migration, and code search. With the increasing success of Machine Learning (ML) models in detection methods, researchers use them for PLI showing superior performance of detection compared to the basic method of identification based on the source file extension. In this paper, we present the first survey for ML-based PLI methods providing insights into the domain’s present status and guiding towards a futuristic tool development for PLI. We evaluate various ML techniques that recognize programming languages. These techniques include conventional, operation-based, and data source-based techniques. Our study examines the advantages and limitations of identifying the programming language in text, images, and videos. Our investigation also emphasizes the capabilities of the existing solutions that improve software development practices. Furthermore, our survey analyzes the strengths and limitations of existing code detection tools such as GitHub’s Linguist, Pygments, highlight.js, Ace, Google’s Code Prettify, SourcererCC, Guesslang, and SonarLint. We also provide some open research problems for future researchers.},
  keywords={Codes;Computer languages;Software;Programming;Feature extraction;Syntactics;Surveys;Source coding;Maintenance;Machine learning;Programming language;detection;machine learning;code;software},
  doi={10.1109/ACCESS.2025.3538108},
  ISSN={2169-3536},
  month={},}@ARTICLE{10538131,
  author={Rahman, Mizanur and Sarwar, Hasan and Kader, MD. Abdul and Gonçalves, Teresa and Tin, Ting Tin},
  journal={IEEE Access}, 
  title={Review and Empirical Analysis of Machine Learning-Based Software Effort Estimation}, 
  year={2024},
  volume={12},
  number={},
  pages={85661-85680},
  abstract={The average software company spends a huge amount of its revenue on Research and Development (R&D) for how to deliver software on time. Accurate software effort estimation is critical for successful project planning, resource allocation, and on-time delivery within budget for sustainable software development. However, both overestimation and underestimation can pose significant challenges, highlighting the need for continuous improvement in estimation techniques. This study reviews recent machine learning approaches employed to enhance the accuracy of software effort estimation (SEE), focusing on research published between 2020 and 2023. The literature review employed a systematic approach to identify relevant research on machine learning techniques for SEE. Additionally, comparative experiments were conducted using five commonly employed Machine Learning (ML) methods: K-Nearest Neighbor, Support Vector Machine, Random Forest, Logistic Regression, and LASSO Regression. The performance of these techniques was evaluated using five widely adopted accuracy metrics: Mean Squared Error (MSE), Mean Magnitude of Relative Error (MMRE), R-squared, Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE). The evaluation was carried out on seven benchmark datasets: Albrecht, Desharnais, China, Kemerer, Mayazaki94, Maxwell, and COCOMO, which are publicly available and extensively used in SEE research. By carefully reviewing study quality, analyzing results across the literature, and rigorously evaluating experimental outcomes, clear conclusions were drawn about the most promising techniques for achieving state-of-the-art accuracy in estimating software effort. This study makes three key contributions to the field: firstly, it furnishes a thorough overview of recent machine learning research in software effort estimation (SEE); secondly, it provides data-driven guidance for researchers and practitioners to select optimal methods for accurate effort estimation; and thirdly, it demonstrates the performance of publicly available datasets through experimental analysis. Enhanced estimation supports the development of better predictive models for software project time, cost, and staffing needs. The findings aim to guide future research directions and tool development toward the most accurate machine learning approaches for modelling software development effort, costs, and delivery schedules, ultimately contributing to more efficient and cost-effective software projects.},
  keywords={Estimation;Machine learning algorithms;Software reliability;Software algorithms;Research and development;Software development management;Linear regression;Support vector machines;Random forests;Software effort estimation;software development efforts estimation;linear regression;support vector machine;random forest;LASSO;KNN;R&D investment},
  doi={10.1109/ACCESS.2024.3404879},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9808768,
  author={Kolltveit, Ask Berstad and Li, Jingyue},
  booktitle={2022 IEEE/ACM 1st International Workshop on Software Engineering for Responsible Artificial Intelligence (SE4RAI)}, 
  title={Operationalizing Machine Learning Models - A Systematic Literature Review}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Deploying machine learning (ML) models to production with the same level of rigor and automation as traditional software systems has shown itself to be a non-trivial task, requiring extra care and infrastructure to deal with the additional challenges. Although many studies focus on adapting ML software engineering (SE) approaches and techniques, few studies have summarized the status and challenges of operationalizing ML models. Model operationalization encompasses all steps after model training and evaluation, including packaging the model in a format appropriate for deployment, publishing to a model registry or storage, integrating the model into a broader software system, serving, and monitoring. This study is the first systematic literature review investigating the techniques, tools, and infrastructures to operationalize ML models. After reviewing 24 primary studies, the results show that there are a number of tools for most use cases to operationalize ML models and cloud deployment in particular. The review also revealed several research opportunities, such as dynamic model-switching, continuous model-monitoring, and efficient edge ML deployments. CCS CONCEPTS • General and reference → Surveys and overviews; • Computing methodologies → Machine learning; • Software and its engineering → Software development techniques.},
  keywords={Training;Adaptation models;Systematics;Publishing;Computational modeling;Bibliographies;Machine learning;Systematic literature review;Machine learning;MLOps;Deployment;Operationalization},
  doi={10.1145/3526073.3527584},
  ISSN={},
  month={May},}@INPROCEEDINGS{9826168,
  author={Gopalakrishna, Nikhil Krishna and Anandayuvaraj, Dharun and Detti, Annan and Bland, Forrest Lee and Rahaman, Sazzadur and Davis, James C.},
  booktitle={2022 IEEE/ACM 4th International Workshop on Software Engineering Research and Practices for the IoT (SERP4IoT)}, 
  title={“If security is required”: Engineering and Security Practices for Machine Learning-based IoT Devices}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={The latest generation of IoT systems incorporate machine learning (ML) technologies on edge devices. This introduces new engineering challenges to bring ML onto resource-constrained hardware, and complications for ensuring system security and privacy. Existing research prescribes iterative processes for machine learning enabled IoT products to ease development and increase product success. However, these processes mostly focus on existing practices used in other generic software development areas and are not specialized for the purpose of machine learning or IoT devices. This research seeks to characterize engineering processes and security practices for ML-enabled IoT systems through the lens of the engineering lifecycle. We collected data from practitioners through a survey (N=25) and interviews (N=4). We found that security processes and engineering methods vary by company. Respondents emphasized the engineering cost of security analysis and threat modeling, and trade-offs with business needs. Engineers reduce their security investment if it is not an explicit requirement. The threats of IP theft and reverse engineering were a consistent concern among practitioners when deploying ML for IoT devices. Based on our findings, we recommend further research into understanding engineering cost, compliance, and security trade-offs.},
  keywords={Costs;Reverse engineering;Prototypes;Machine learning;Internet of Things;Computer security;Interviews;Internet of Things;Machine Learning;Security and Privacy;Cyber-Physical Systems;Embedded Systems;Software Engineering},
  doi={10.1145/3528227.3528565},
  ISSN={},
  month={May},}@ARTICLE{10677006,
  author={Terlapu, Panduranga Vital and Raju, Kalidindi Kishore and Kiran Kumar, G. and Jagadeeswara Rao, G. and Kavitha, K. and Samreen, Shirina},
  journal={IEEE Access}, 
  title={Improved Software Effort Estimation Through Machine Learning: Challenges, Applications, and Feature Importance Analysis}, 
  year={2024},
  volume={12},
  number={},
  pages={138663-138701},
  abstract={Effort estimations are a crucial aspect of software development. The tasks should be completed before the start of any software project. Accurate estimations increase the chances of project success, and inaccurate information can lead to severe issues. This study systematically reviewed the literature on effort-estimating models from 2015-2024, identifying 69 relevant studies from various publications to compile information on various software work estimation models. This review aims to analyze the models proposed in the literature and their classification, the metrics used for accuracy measurement, the leading model that has been chiefly applied for effort estimation, and the benchmark datasets available. The study utilized 542 relevant articles on software development, cost, effort, prediction, estimation, and modelling techniques in the search strategy. After 194 selections, the authors chose 69 articles to understand ML applications in SEE comprehensively. The researchers used a scoring system to assess each study’s responses (from 0 to 5 points) to their research questions. This helped them identify credible studies with higher scores for a comprehensive review aligned with its objectives. The data extraction process identified 91% (63) of 69 studies as either highly or somewhat relevant, demonstrating a successful search strategy for analysis. The literature review on SEE indicates a growing preference for ML-based models in 59% of selected studies. 17% of the studies chosen favor hybrid models to overcome software development challenges. We qualitatively analyzed all the literature on software effort estimation using expert judgment, formal estimation techniques, ML-based techniques, and hybrid techniques. We discovered that researchers have frequently used ML-based models to estimate software effort and are currently in the lead. This study also explores the application of feature importance and selection in machine learning models for Software Effort Estimation (SEE) using popular algorithms like support Vector Machine (SVM), AdaBoost (AB), Gradient Boost (GB), and Random Forest (RF) with six benchmark datasets like CHINA, COCOMO-NASA2, COCOMO, COCOMO81, DESHARNAIS, and KITCHENHAM. We analyze the dataset descriptions and feature importance of the dataset analysis using ML models for choosing crucial play attributes in SEE.},
  keywords={Estimation;Software;Accuracy;Software development management;Analytical models;Measurement;Benchmark testing;Accuracy measure;classification models;feature importance;machine learning;software effort estimation;software metrics},
  doi={10.1109/ACCESS.2024.3457771},
  ISSN={2169-3536},
  month={},}@ARTICLE{10843220,
  author={Izhar, Rahat and Bhatti, Shahid N. and Alharthi, Sultan A.},
  journal={IEEE Access}, 
  title={Bridging Precision and Complexity: A Novel Machine Learning Approach for Ambiguity Detection in Software Requirements}, 
  year={2025},
  volume={13},
  number={},
  pages={12014-12031},
  abstract={Ambiguity in software requirements is a significant challenge as it often leads to misunderstandings, implementation errors, and costly project delays. This research proposes a hybrid framework that combines rule-based techniques with machine learning to identify ambiguity in software requirements with precision and efficiency. The framework begins with a rule-based model that systematically detects ambiguities using a carefully prepared list of ambiguous phrases. The analysis utilizes a dataset of 1,553 software requirements drawn from diverse project domains. To capture more intricate ambiguities that traditional rule-based systems might miss, the framework integrates TF-IDF vectorization and a Random Forest classifier, enhancing the precision and coverage of classification. In addition, clustering analysis identifies patterns to provide deeper insights into ambiguous requirements, while sentiment analysis explores the relationship between ambiguity and the emotional tone of requirements. Together, these analyses offer a broader understanding of ambiguity trends and stakeholder perceptions. The framework’s performance is validated using standard evaluation metrics, achieving an accuracy of 97%, precision of 97%, recall of 89%, and an F1-score of 92%, significantly surpassing traditional rule-based methodologies. This research advances automatic ambiguity detection by delivering a flexible and interpretable solution. The proposed approach enhances the clarity and quality of software requirements, strengthening requirements engineering practices and supporting more effective software development.},
  keywords={Software;Machine learning;Accuracy;Stakeholders;Scalability;Pragmatics;Complexity theory;Support vector machines;Software engineering;Requirements engineering;Software requirements engineering;natural language processing (NLP);machine learning (ML);requirements ambiguity;artificial intelligence (AI);sentiment analysis},
  doi={10.1109/ACCESS.2025.3529943},
  ISSN={2169-3536},
  month={},}@ARTICLE{10741285,
  author={Mehmood, Abid and Ilyas, Qazi Mudassar and Ahmad, Muneer and Shi, Zhongliang},
  journal={IEEE Access}, 
  title={Test Suite Optimization Using Machine Learning Techniques: A Comprehensive Study}, 
  year={2024},
  volume={12},
  number={},
  pages={168645-168671},
  abstract={Software testing is an essential yet costly phase of the software development lifecycle. While machine learning-based test suite optimization techniques have shown promise in reducing testing costs and improving fault detection, a comprehensive evaluation of their effectiveness across different environments is still lacking. This paper reviews 43 studies published between 2018 and 2023, covering various test case selection, prioritization, and reduction techniques using machine learning. The findings reveal that conventional machine learning techniques, particularly supervised learning methods, have been widely adopted for test case prioritization and selection. Recent advancements, such as deep learning and hybrid models, show potential in improving fault detection rates and scalability, though challenges remain in adapting these techniques to large-scale and dynamic environments. Additionally, Generative AI and large language models (LLMs) are emerging as promising tools for automating aspects of test case generation and prioritization, offering new avenues for future research in enhancing test suite optimization. The study identifies recent trends, challenges, and opportunities for further research, with a focus on both conventional and emerging methods, including deep learning, hybrid approaches, and Generative AI models. By systematically analyzing these techniques, this work contributes to the understanding of how machine learning and Generative AI can enhance test suite optimization and highlights future directions for improving the scalability and real-world applicability of these methods.},
  keywords={Optimization;Software testing;Testing;Software quality;Systematics;Deep learning;Reinforcement learning;Market research;Generative AI;Costs;Machine learning;Software quality;software testing;test suite optimization (TSO);machine learning in software testing;test case selection;evaluation metrics for test suite optimization},
  doi={10.1109/ACCESS.2024.3490453},
  ISSN={2169-3536},
  month={},}@ARTICLE{9734272,
  author={Washizaki, Hironori and Khomh, Foutse and Guéhéneuc, Yann-Gaël and Takeuchi, Hironori and Natori, Naotake and Doi, Takuo and Okuda, Satoshi},
  journal={Computer}, 
  title={Software-Engineering Design Patterns for Machine Learning Applications}, 
  year={2022},
  volume={55},
  number={3},
  pages={30-39},
  abstract={In this study, a multivocal literature review identified 15 software-engineering design patterns for machine learning applications. Findings suggest that there are opportunities to increase the patterns’ adoption in practice by raising awareness of such patterns within the community.},
  keywords={Bibliographies;Machine learning;Software engineering;Computer applications},
  doi={10.1109/MC.2021.3137227},
  ISSN={1558-0814},
  month={March},}@ARTICLE{10175526,
  author={Jadhav, Akshay and Shandilya, Shishir Kumar and Izonin, Ivan and Gregus, Michal},
  journal={IEEE Access}, 
  title={Effective Software Effort Estimation Leveraging Machine Learning for Digital Transformation}, 
  year={2023},
  volume={11},
  number={},
  pages={83523-83536},
  abstract={Software effort estimation is a necessary component of software development projects that belong to industrial software systems and digital transformation initiatives. Digital transformation refers to the process of integrating digital technology into various components of a company or organization in order to improve operations, procedures, customer experiences, and overall performance. Industrial software systems are trained software packages designed for use in industrial and manufacturing processes. The paper deals with the machine learning based effort estimation in order to create an effective and robust model for predicting effort. The paper proposes an Omni-Ensemble Learning (OEL) approach, which is a combination of static ensemble selection along with genetic algorithm and dynamic ensemble selection. The paper identifies the impact of software effort estimation in industrial software system, and works on the these attributes to implement a robust ensemble model. The proposed Omni-Ensemble Selection (OES) provides better overall performance (in terms of evaluation metrics) and on comparing with multiple machine learning models over Finnish and Maxwell datasets.},
  keywords={Software engineering;Estimation;Software systems;Stakeholders;Industries;Fourth Industrial Revolution;Industrial engineering;Digital transformation;industrial software system;software effort estimation;software engineering},
  doi={10.1109/ACCESS.2023.3293432},
  ISSN={2169-3536},
  month={},}@ARTICLE{9590539,
  author={Zhou, Yu and Cui, Suxia and Wang, Yonghui},
  journal={IEEE Access}, 
  title={Machine Learning Based Embedded Code Multi-Label Classification}, 
  year={2021},
  volume={9},
  number={},
  pages={150187-150200},
  abstract={With the development of Internet of Things (IoT) technology, embedded based electronic devices have penetrated every corner of our daily lives. As the brain of IoT devices, embedded based micro controller unit (MCU) plays an irreplaceable role. The functions of the MCUs are becoming more and more powerful and complicated, which brings huge challenges to embedded programmers. Embedded code, which is highly related to the hardware resources, differs from other popular programming code. The hardware configuration may be a big challenge to the programmers, who may only be good at software development and algorithm design. Online code searching can be time consuming and cannot guarantee an optimal approach. To solve this problem, in this paper, an embedded code classifier, which is designed to help embedded programmers to search for the most efficient code with precise tags, is demonstrated. A high quality embedded code dataset is built. A tag correlated multi-label machine learning model is developed for the embedded code dataset. The experimental results show that the proposed code dataset structure is proved to be more efficient on embedded code classification. The proposed embedded classifier algorithm shows a promising result on embedded code dataset. And it outperforms the traditional machine learning text classification models.},
  keywords={Codes;Hardware;Support vector machines;Machine learning algorithms;Registers;Prediction algorithms;Logistics;Embedded code classifier;multi-label;tag-correlated;text classification},
  doi={10.1109/ACCESS.2021.3123498},
  ISSN={2169-3536},
  month={},}@ARTICLE{9328431,
  author={Shreda, Qais A. and Hanani, Abualsoud A.},
  journal={IEEE Access}, 
  title={Identifying Non-Functional Requirements From Unconstrained Documents Using Natural Language Processing and Machine Learning Approaches}, 
  year={2025},
  volume={13},
  number={},
  pages={124159-124179},
  abstract={Requirements engineering is the first phase in software development life cycle and it also plays one of the most important and critical roles. Requirement document mainly contains both functional requirements and non-functional requirements. Non-functional requirements are significant to describe the properties and constraints of the system. Early identification of Non-functional requirement has direct impact on the system architecture and initial design decision. Practically, non-functional requirements are extracted manually from the document. This makes it tedious, time-consuming task and prone to various errors. In this paper, we propose an automatic approach to identify and classify non-functional requirements using semantic and syntactic analysis with machine learning approaches from unconstrained documents. We used A dataset of public requirements documents (PURE) that consists of 79 unconstrained requirements documents in different forms. In our approach, features were extracted from the requirement sentences using four different natural language processing methods including statistical and state-of-the-art semantic analysis presented by Google word2vec and bidirectional encoder representations from transformers models. The adopted approach can efficiently classify non-functional requirements with an accuracy between 84% and 87% using statistical vectorization method and 88% to 92% using word embedding semantic methods. Furthermore, by fusing different models trained on different features, the accuracy improves by 2.4% compared with the best individual classifier.},
  keywords={Software;Machine learning;Feature extraction;Semantics;Syntactics;Natural language processing;Support vector machines;Software requirement;machine learning;natural language processing},
  doi={10.1109/ACCESS.2021.3052921},
  ISSN={2169-3536},
  month={},}@ARTICLE{10935309,
  author={Zhang, Zixian and Saber, Takfarinas},
  journal={IEEE Access}, 
  title={Machine Learning Approaches to Code Similarity Measurement: A Systematic Review}, 
  year={2025},
  volume={13},
  number={},
  pages={51729-51764},
  abstract={Source code similarity measurement, which involves assessing the degree of difference between code segments, plays a crucial role in various aspects of the software development cycle. These include but are not limited to code quality assurance, code review processes, code plagiarism detection, security, and vulnerability analysis. Despite the increasing application of ML technique in this domain, a comprehensive synthesis of existing methodologies remains lacking. This paper presents a systematic review of Machine Learning techniques applied to code similarity measurement, aiming to illuminate current methodologies and contribute valuable insights to the research community. Following a rigorous systematic review protocol, we identified and analyzed 84 primary studies on a broad spectrum of dimensions covering application type, devised Machine Learning algorithms, used code representations, datasets, and performance metrics, as well as performance evaluations. A deep investigation reveals that 15 applications for code similarity measurement have utilized 51 different machine learning algorithms. Additionally, the most prevalent code representation is found to be the abstract syntax tree (AST). Furthermore, the most frequently employed dataset across various code similarity research applications is BigCloneBench. Through this comprehensive analysis, the paper not only synthesizes existing research but also identifies prevailing limitations and challenges, shedding light on potential avenues for future work.},
  keywords={Codes;Cloning;Systematic literature review;Syntactics;Plagiarism;Semantics;Machine learning;Unsupervised learning;Source coding;Machine learning algorithms;Code similarity;code clone;machine learning;deep learning;systematic literature review},
  doi={10.1109/ACCESS.2025.3553392},
  ISSN={2169-3536},
  month={},}@ARTICLE{11082155,
  author={Al-Johany, Norah A. and Eassa, Fathy E. and Sharaf, Sanaa A. and Balkhair, Eynas H. and Assiri, Sara M.},
  journal={IEEE Access}, 
  title={Defect Detection and Correction in OpenMP: A Static Analysis and Machine Learning-Based Solution}, 
  year={2025},
  volume={13},
  number={},
  pages={125499-125525},
  abstract={Concurrency defects such as race conditions, deadlocks, and improper synchronization remain a critical challenge in developing reliable OpenMP-based parallel applications. Traditional static analysis tools often focus only on defect detection, offering limited or no automated correction capabilities. This paper presents a novel static analysis tool designed to detect and automatically correct concurrency-related defects in OpenMP programs. The tool performs lexical and syntactic analysis to extract OpenMP constructs, verify directive usage, and identify incorrect synchronization patterns. A rule-based correction engine is employed to repair detected defects through minimally invasive code transformations, such as inserting critical sections, correcting directive placement, and adjusting data-sharing clauses. To enhance predictive accuracy, the tool incorporates machine learning classifiers—Naive Bayes (NB), Decision Tree (DT), Random Forest (RF), and Linear Support Vector Machine (LSVM)—trained on various feature combinations, including Abstract Features (AF), Halstead Features (HF), and Semantic Features (SF). Evaluation results show that NB and LSVM achieved up to 99% accuracy with simple feature sets, while DT and RF exhibited lower performance across all combinations. The tool was validated on a curated dataset of annotated OpenMP programs, achieving a 99.15% correction rate with minimal execution overhead. These results confirm the effectiveness and practicality of the proposed solution in improving the correctness and maintainability of OpenMP applications. This work bridges the gap between static defect detection and automated correction, contributing a scalable and intelligent approach to reliable parallel software development.},
  keywords={Feature extraction;Synchronization;Codes;Instruction sets;Static analysis;Defect detection;Reliability;Concurrent computing;Parallel programming;Accuracy;Static analysis;OpenMP;parallel programming;data races;deadlocks;software defect detection},
  doi={10.1109/ACCESS.2025.3589175},
  ISSN={2169-3536},
  month={},}@ARTICLE{11106424,
  author={Adhikari, Nitanta and Bista, Rabindra and Ferreira, João Carlos},
  journal={IEEE Access}, 
  title={Leveraging Machine Learning for Enhanced Bug Triaging in Open-Source Software Projects}, 
  year={2025},
  volume={13},
  number={},
  pages={136237-136254},
  abstract={Bug triaging–the process of classifying and assigning software issues to appropriate developers–is a critical yet challenging task in large-scale software development. Manual triaging is time-consuming, inconsistent, and prone to human bias, which often delays issue resolution and misallocates developer resources. This study explores the application of machine learning to automate and improve bug triaging efficiency and accuracy. Using a dataset of over 122,000 issues from the microsoft/vscode GitHub repository, we evaluate several machine learning models including Bidirectional LSTM, CNN-LSTM, Random Forest, and Multinomial Naive Bayes. Our primary contribution is the development of an Augmented Bidirectional LSTM model that integrates enriched textual features and contextual metadata. This model, optimized using Optuna, outperforms traditional baselines, achieving a Micro F1-score of 0.6469 and Hamming Loss of 0.0133 for label prediction, and a Micro F1-score of 0.5974 with Hamming Loss of 0.0062 for assignee recommendation. In addition to demonstrating strong predictive performance, we present a robust end-to-end pipeline for data preprocessing, augmentation, model training, and evaluation using multi-label classification techniques. The study highlights how deep learning architectures, in combination with feature engineering and hyperparameter tuning, can provide scalable and generalizable components to support the automation of bug triaging. These findings contribute to the growing field of intelligent software maintenance by offering data-driven approaches that can support developer workflows and improve issue management efficiency in open-source environments.},
  keywords={Computer bugs;Software development management;Machine learning;Accuracy;Open source software;Data models;Predictive models;Machine learning algorithms;Measurement;Manuals;Bug triaging;natural language processing (NLP);multi-label classification;model evaluation metrics},
  doi={10.1109/ACCESS.2025.3595011},
  ISSN={2169-3536},
  month={},}
